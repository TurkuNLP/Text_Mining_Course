{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER exercises\n",
    "\n",
    "This is a modified, more compact version of (one variant of) the CRF tagger code from the NER notebook. You may wish to use this as a starting point for doing some of the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This first bit is Jupyter magic and required imports, feel free to ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%run ../lib/conllu.ipynb\n",
    "%run ../lib/visualization.ipynb\n",
    "\n",
    "import sklearn_crfsuite\n",
    "import eli5\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_f1_score as f1_score\n",
    "from sklearn_crfsuite.metrics import flat_classification_report as classification_report\n",
    "from eli5.sklearn_crfsuite.explain_weights import sorted_for_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "Look here if you want to work on English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnish\n",
    "\n",
    "train_sentences = read_conll('../data/finer-featurized/digitoday.2014.train.conllu')\n",
    "devel_sentences = read_conll('../data/finer-featurized/digitoday.2014.dev.conllu')\n",
    "test_sentences = read_conll('../data/finer-featurized/digitoday.2015.test.conllu')\n",
    "test_sentences_wiki = read_conll('../data/finer-featurized/wikipedia.test.conllu')\n",
    "\n",
    "# English (uncomment if preferred)\n",
    "\n",
    "# train_sentences = read_conll('../data/conll03-featurized/eng.train.conllu')\n",
    "# devel_sentences = read_conll('../data/conll03-featurized/eng.dev.conllu')\n",
    "# test_sentences = read_conll('../data/conll03-featurized/eng.test.conllu')\n",
    "# test_sentences_wiki = []    # Sorry, we don't this dataset for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mangling and featurization\n",
    "\n",
    "You'll want to edit this bit if you want to try different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = namedtuple('Word', [\n",
    "    'form',\n",
    "    'lemma',\n",
    "    'upos',\n",
    "    'xpos',\n",
    "    'morpho',\n",
    "])\n",
    "\n",
    "\n",
    "def reformat_sentences(sentences):\n",
    "    # separate tags, drop parse fields, cast as namedtuples\n",
    "    tags, data = [], []\n",
    "    for sentence in sentences:\n",
    "        tags.append([w[0] for w in sentence])\n",
    "        data.append([Word(*w[1:6]) for w in sentence])\n",
    "    return tags, data\n",
    "\n",
    "\n",
    "def less_minimal_featurizer(sentence, idx):\n",
    "    word = sentence[idx]\n",
    "    features = {\n",
    "        'form': word.form,\n",
    "        'form.lower()': word.form.lower(),\n",
    "        'form[:2]': word.form[:2],\n",
    "        'form[:3]': word.form[:3],\n",
    "        'form[-2:]': word.form[-2:],\n",
    "        'form[-3:]': word.form[-3:],\n",
    "        'lemma': word.lemma,\n",
    "        'lemma.lower()': word.lemma.lower(),\n",
    "        'upos': word.upos,\n",
    "        'xpos': word.xpos,\n",
    "    }\n",
    "    # morpho is either empty (\"_\") or like this: \"Case=Nom|Number=Sing\"\n",
    "    if word.morpho == '_':\n",
    "        features['morpho'] = '_'\n",
    "    else:\n",
    "        for attr, val in [m.split('=') for m in word.morpho.split('|')]:\n",
    "            features['morpho.{}'.format(attr)] = val\n",
    "    return features\n",
    "\n",
    "\n",
    "def featurize_sentence(sentence, featurizer=less_minimal_featurizer):\n",
    "    return [featurizer(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\n",
    "train_y, train_data = reformat_sentences(train_sentences)\n",
    "devel_y, devel_data = reformat_sentences(devel_sentences)\n",
    "test_y, test_data = reformat_sentences(test_sentences)\n",
    "wiki_y, wiki_data = reformat_sentences(test_sentences_wiki)\n",
    "\n",
    "train_x = [featurize_sentence(s) for s in train_data]\n",
    "devel_x = [featurize_sentence(s) for s in devel_data]\n",
    "test_x = [featurize_sentence(s) for s in test_data]\n",
    "wiki_x = [featurize_sentence(s) for s in wiki_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF training and prediction\n",
    "\n",
    "This is the place to try different hyperparameters.\n",
    "\n",
    "Some of the parameters for `CRF()` have been filled in with their default values for reference: see the [API documentation](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite) for explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    min_freq=0,\n",
    "    all_possible_states=False,\n",
    "    all_possible_transitions=True,\n",
    "    c1=0.0,\n",
    "    c2=1.0,\n",
    "    max_iterations=None\n",
    ")\n",
    "crf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_y = crf.predict(devel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_O = [t for t in crf.classes_ if t != 'O']\n",
    "\n",
    "print(classification_report(devel_y, pred_y, labels=exclude_O))\n",
    "eli5.show_weights(crf, targets=sorted_for_ner(crf.classes_), top=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "This may be useful if you're interested in looking at what was tagged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagged(namedtuple('Tagged', 'text tag')):\n",
    "    def __repr__(self):\n",
    "        return '<{0}>{1}</{0}>'.format(self.tag, self.text)\n",
    "\n",
    "\n",
    "def get_tagged(words, tags):\n",
    "    # Get tagged spans from sequences of words and BIO tags\n",
    "    tagged, current, current_type = [], [], None\n",
    "    for word, tag in zip(words, tags):\n",
    "        if tag[0] in 'OB' and current:    # current ends\n",
    "            tagged.append(Tagged(' '.join(current), current_type))\n",
    "            current, current_type = [], None\n",
    "        if tag[0] == 'B':\n",
    "            current, current_type = [word], tag[2:]\n",
    "        elif tag[0] == 'I':\n",
    "            if not current:    # I without B, but nevermind\n",
    "                current, current_type = [word], tag[2:]\n",
    "            else:\n",
    "                current.append(word)\n",
    "                # TODO check that type agrees\n",
    "        else:\n",
    "            assert tag == 'O', 'unexpected tag {}'.format(tag)\n",
    "    if current:    # span open at sentence end\n",
    "        tagged.append(Tagged(' '.join(current), current_type))\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: print most frequently tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for sentence, predictions in zip(devel_data, pred_y):\n",
    "    words = [w.form for w in sentence]    # try w.lemma instead\n",
    "    tagged = get_tagged(words, predictions)\n",
    "    counter.update(tagged)\n",
    "    \n",
    "for pair, count in counter.most_common(50):\n",
    "    print('{}\\t{}'.format(count, pair))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
