{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-bert-for-text-classification-code-only.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LPd4TaxwbMR"
      },
      "source": [
        "# Train BERT text classifier using the Transformers library (code only)\n",
        "\n",
        "This is a version of the notebook [https://github.com/TurkuNLP/Text_Mining_Course/blob/master/train_bert_for_text_classification.ipynb](train_bert_for_text_classification.ipynb) with just the code for training BERT. See that notebook for explanations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1XGi09l-KxP"
      },
      "source": [
        "!pip --quiet install transformers\n",
        "!pip --quiet install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q21P5qTsyyzO"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "MODEL_NAME = 'bert-base-cased'\n",
        "DATASET = ('glue', 'sst2')\n",
        "LEARNING_RATE=2e-5\n",
        "BATCH_SIZE=16\n",
        "TRAIN_EPOCHS=3\n",
        "\n",
        "dataset = load_dataset(*DATASET)\n",
        "num_labels = len(set(dataset['train']['label']))\n",
        "dataset['train'] = dataset['train'].filter(lambda example, idx: idx % 10 == 0, with_indices=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def encode_dataset(d):\n",
        "  return tokenizer(d['sentence'])\n",
        "encoded_dataset = dataset.map(encode_dataset)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    'output_dir',\n",
        "    save_strategy='no',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=TRAIN_EPOCHS,\n",
        ")\n",
        "\n",
        "def compute_accuracy(pred):\n",
        "    y_pred = pred.predictions.argmax(axis=1)\n",
        "    y_true = pred.label_ids\n",
        "    return { 'accuracy': sum(y_pred == y_true) / len(y_true) }\n",
        "\n",
        "trainer = Trainer(\n",
        "      model,\n",
        "      train_args,\n",
        "      train_dataset=encoded_dataset['train'],\n",
        "      eval_dataset=encoded_dataset['validation'],\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_accuracy\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}