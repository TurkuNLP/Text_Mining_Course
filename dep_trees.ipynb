{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple, rule-based relation extraction\n",
    "\n",
    "* Extract simple relations between entities of interest\n",
    "* Can be specified as surface patterns\n",
    "\n",
    "## Hearst patterns\n",
    "\n",
    "<img src=\"figs/fig_hearst_1.png\" />\n",
    "\n",
    "* See [here](https://arxiv.org/pdf/1806.03191.pdf) for more, details\n",
    "* Simple patterns defined on surface, possibly lemmatized text\n",
    "* Surprisingly good at extracting properties and is-a types of relations\n",
    "* Must be combined with large text amounts\n",
    "\n",
    "## Let's try\n",
    "\n",
    "* We can try these quite easily\n",
    "* Extract text from our parser output\n",
    "* Run simple patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ['On', 'Friday', ',', 'Los', 'Angeles', 'County', 'sheriff', \"'s\", 'detectives', 'announced', 'that', 'they', 'had', 'made', 'an', 'arrest', 'in', 'Broudreaux', \"'s\", 'killing', ',', 'the', 'result', 'of', 'a', 'DNA', 'match', '.']\n",
      "LEMMAS ['on', 'Friday', ',', 'Los', 'Angeles', 'County', 'sheriff', \"'s\", 'detective', 'announce', 'that', 'they', 'have', 'make', 'a', 'arrest', 'in', 'Broudreaux', \"'s\", 'killing', ',', 'the', 'result', 'of', 'a', 'dna', 'match', '.']\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "ID,FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC=range(10) #column names\n",
    "\n",
    "def read_conllu(inp):\n",
    "    \"\"\"The simplest conllu reader I can imagine\"\"\"\n",
    "    current_comments=[]\n",
    "    current_tree=[]\n",
    "    for line in inp:\n",
    "        line=line.strip()\n",
    "        if not line: #empty line -> new tree starting, get rid of the old one\n",
    "            yield current_comments, current_tree\n",
    "            current_comments=[]\n",
    "            current_tree=[]\n",
    "        elif line.startswith(\"#\"):\n",
    "            current_comments.append(line) #this is a comment\n",
    "        else:\n",
    "            current_tree.append(line.split(\"\\t\"))\n",
    "    else: #all done\n",
    "        yield current_comments, current_tree\n",
    "\n",
    "with gzip.open(\"/course_data/textmine/parsed-data/english-news-crawl-30M.conllu.gz\",\"rt\",encoding=\"utf-8\") as f:\n",
    "    for comments, tree in read_conllu(f):\n",
    "        words=[cols[FORM] for cols in tree]\n",
    "        lemmas=[cols[LEMMA] for cols in tree]\n",
    "        print(\"WORDS\", words)\n",
    "        print(\"LEMMAS\", lemmas)\n",
    "        break #or else we print waaay too much\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* as a side remark\n",
    "* many of the things in the course are too heavy or clumsy to run in Jupyter\n",
    "* better run on command line and better written in a real editor, but for the sake of simplicity, we use jupyter to edit our code too\n",
    "* Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conllu2text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile conllu2text.py\n",
    "\n",
    "#^^^^ this is how you can edit a file on the drive so you can run it in the terminal\n",
    "\n",
    "import gzip\n",
    "import sys\n",
    "ID,FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC=range(10) #column names\n",
    "\n",
    "def read_conllu(inp):\n",
    "    \"\"\"The simplest conllu reader I can imagine\"\"\"\n",
    "    current_comments=[]\n",
    "    current_tree=[]\n",
    "    for line in inp:\n",
    "        line=line.strip()\n",
    "        if not line: #empty line -> new tree starting, get rid of the old one\n",
    "            yield current_comments, current_tree\n",
    "            current_comments=[]\n",
    "            current_tree=[]\n",
    "        elif line.startswith(\"#\"):\n",
    "            current_comments.append(line) #this is a comment\n",
    "        else:\n",
    "            current_tree.append(line.split(\"\\t\"))\n",
    "    else: #all done\n",
    "        yield current_comments, current_tree\n",
    "\n",
    "for comments, tree in read_conllu(sys.stdin):\n",
    "    lemmas=[cols[LEMMA] for cols in tree]\n",
    "    print(\" \".join(lemmas))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* and now we can run it in the terminal like so\n",
    "* note zipping and unzipping on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "exit #comment this out, it protects me from accidentally running this cell\n",
    "\n",
    "zcat /course_data/textmine/parsed-data/english-news-crawl-30M.conllu.gz | python3 conllu2text.py | gzip > english_news_30M_lemma.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have English lemmas, let's try some simple patterns\n",
    "* Once again, much can be done on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " giant such as Google and\n",
      " event such as heat and\n",
      " starch such as tapioca and\n",
      " brand such as Otrivine and\n",
      " food such as butter and\n",
      " medium such as pornography and\n",
      " region such as Gansu and\n",
      " excuse such as freedom and\n",
      " county such as Hertfordshire and\n",
      " country such as Finland and\n",
      " company such as Penguin and\n",
      " series such as homeland and\n",
      " country such as Germany and\n",
      " source such as wind and\n",
      " invertebrate such as worm and\n",
      " app such as WhatsApp and\n",
      " place such as Iraq and\n",
      " author such as well and\n",
      " symptom such as breathlessness and\n",
      " event such as concert and\n",
      "\n",
      "and in a tab-delimited form\n",
      "\n",
      " giant\tGoogle\n",
      " event\theat\n",
      " starch\ttapioca\n",
      " brand\tOtrivine\n",
      " food\tbutter\n",
      " medium\tpornography\n",
      " region\tGansu\n",
      " excuse\tfreedom\n",
      " county\tHertfordshire\n",
      " country\tFinland\n",
      " company\tPenguin\n",
      " series\thomeland\n",
      " country\tGermany\n",
      " source\twind\n",
      " invertebrate\tworm\n",
      " app\tWhatsApp\n",
      " place\tIraq\n",
      " author\twell\n",
      " symptom\tbreathlessness\n",
      " event\tconcert\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zcat english_news_30M_lemma.txt.gz | grep -Po '\\s\\w+\\ssuch\\sas\\s\\w+\\sand' | head -n 20\n",
    "echo\n",
    "echo \"and in a tab-delimited form\"\n",
    "echo\n",
    "zcat english_news_30M_lemma.txt.gz | grep -Po '\\s\\w+\\ssuch\\sas\\s\\w+\\sand' | perl -pe 's/ such as /\\t/;s/ and$//' | head -n 20\n",
    "zcat english_news_30M_lemma.txt.gz | grep -Po '\\s\\w+\\ssuch\\sas\\s\\w+\\sand' | perl -pe 's/ such as /\\t/;s/ and$//' > simple_patterns.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the patterns are now in the .tsv file, so we could try reading them in, see what we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: country\n",
      "88     China\n",
      "53     Germany\n",
      "39     France\n",
      "36     India\n",
      "26     Spain\n",
      "25     Italy\n",
      "21     Greece\n",
      "21     Brazil\n",
      "20     Canada\n",
      "20     Syria\n",
      "19     Turkey\n",
      "18     Japan\n",
      "17     Russia\n",
      "16     Australia\n",
      "14     Sweden\n",
      "13     Finland\n",
      "13     Britain\n",
      "12     Indonesia\n",
      "12     Vietnam\n",
      "11     Norway\n",
      "\n",
      "\n",
      "CLASS: company\n",
      "71     Google\n",
      "39     Facebook\n",
      "34     Apple\n",
      "30     Uber\n",
      "10     Amazon\n",
      "8     uber\n",
      "7     IBM\n",
      "7     Samsung\n",
      "6     Unilever\n",
      "6     Tesla\n",
      "6     Netflix\n",
      "5     BP\n",
      "5     Comcast\n",
      "5     Twitter\n",
      "5     Boeing\n",
      "4     Huawei\n",
      "4     Microsoft\n",
      "4     Vodafone\n",
      "4     SpaceX\n",
      "4     ExxonMobil\n",
      "\n",
      "\n",
      "CLASS: service\n",
      "57     Netflix\n",
      "37     Spotify\n",
      "32     Uber\n",
      "23     school\n",
      "13     health\n",
      "12     WhatsApp\n",
      "12     education\n",
      "9     healthcare\n",
      "8     netflix\n",
      "7     uber\n",
      "7     water\n",
      "6     Skype\n",
      "6     Twitter\n",
      "5     Facebook\n",
      "5     Snapchat\n",
      "4     Pandora\n",
      "4     electricity\n",
      "4     library\n",
      "4     hospital\n",
      "3     Hulu\n",
      "\n",
      "\n",
      "CLASS: area\n",
      "21     health\n",
      "20     education\n",
      "9     agriculture\n",
      "7     tax\n",
      "6     retail\n",
      "4     defence\n",
      "4     trade\n",
      "4     engineering\n",
      "4     transport\n",
      "4     healthcare\n",
      "4     housing\n",
      "3     infrastructure\n",
      "3     custom\n",
      "3     energy\n",
      "3     mobile\n",
      "3     fishery\n",
      "3     economic\n",
      "3     airport\n",
      "3     banking\n",
      "3     Mayfair\n",
      "\n",
      "\n",
      "CLASS: issue\n",
      "36     immigration\n",
      "29     abortion\n",
      "15     trade\n",
      "11     health\n",
      "8     tax\n",
      "8     obesity\n",
      "7     race\n",
      "5     depression\n",
      "5     security\n",
      "5     privacy\n",
      "5     anxiety\n",
      "5     unemployment\n",
      "5     education\n",
      "5     childcare\n",
      "4     poverty\n",
      "4     energy\n",
      "4     diabete\n",
      "4     racism\n",
      "4     pay\n",
      "4     contraception\n",
      "\n",
      "\n",
      "CLASS: city\n",
      "16     London\n",
      "14     Beijing\n",
      "8     Birmingham\n",
      "7     Chicago\n",
      "7     Sydney\n",
      "6     Melbourne\n",
      "6     Barcelona\n",
      "6     Paris\n",
      "5     Mumbai\n",
      "5     Toronto\n",
      "5     Shanghai\n",
      "4     Bristol\n",
      "4     Brisbane\n",
      "4     Mosul\n",
      "4     Homs\n",
      "4     Philadelphia\n",
      "3     Detroit\n",
      "3     Austin\n",
      "3     Liverpool\n",
      "3     Berlin\n",
      "\n",
      "\n",
      "CLASS: place\n",
      "9     Iraq\n",
      "8     India\n",
      "8     school\n",
      "5     Syria\n",
      "4     Afghanistan\n",
      "4     restaurant\n",
      "4     Dubai\n",
      "4     Iowa\n",
      "3     China\n",
      "3     Australia\n",
      "3     Chicago\n",
      "3     park\n",
      "3     Africa\n",
      "3     Yemen\n",
      "3     Libya\n",
      "3     Birmingham\n",
      "3     hospital\n",
      "3     Germany\n",
      "3     France\n",
      "2     Switzerland\n",
      "\n",
      "\n",
      "CLASS: site\n",
      "53     Facebook\n",
      "14     Twitter\n",
      "6     Instagram\n",
      "5     eBay\n",
      "5     Amazon\n",
      "4     YouTube\n",
      "3     Airbnb\n",
      "3     kickstarter\n",
      "3     indiegogo\n",
      "2     Linkedin\n",
      "2     embassy\n",
      "2     Travelsupermarket\n",
      "2     airbnb\n",
      "2     Stubhub\n",
      "2     health\n",
      "2     gofundme\n",
      "2     Zoopla\n",
      "2     twitter\n",
      "2     Pinterest\n",
      "2     Netflix\n",
      "\n",
      "\n",
      "CLASS: item\n",
      "14     food\n",
      "6     jewellery\n",
      "5     car\n",
      "5     clothes\n",
      "4     fridge\n",
      "3     pension\n",
      "3     television\n",
      "3     furniture\n",
      "3     diaper\n",
      "3     dishwasher\n",
      "3     smartphone\n",
      "2     medicine\n",
      "2     shoe\n",
      "2     bread\n",
      "2     broom\n",
      "2     sock\n",
      "2     fruit\n",
      "2     energy\n",
      "2     battery\n",
      "2     clothing\n",
      "\n",
      "\n",
      "CLASS: brand\n",
      "8     Gucci\n",
      "6     bond\n",
      "5     AAMI\n",
      "5     Chanel\n",
      "4     NRMA\n",
      "4     Apple\n",
      "4     Burberry\n",
      "3     Audi\n",
      "3     Prada\n",
      "3     Penfolds\n",
      "3     Zara\n",
      "3     Lenovo\n",
      "3     tide\n",
      "2     coke\n",
      "2     MAC\n",
      "2     Adidas\n",
      "2     Sony\n",
      "2     Nike\n",
      "2     Smashbox\n",
      "2     coach\n",
      "\n",
      "\n",
      "CLASS: disease\n",
      "32     cancer\n",
      "30     diabete\n",
      "7     measle\n",
      "7     hiv\n",
      "7     tuberculosis\n",
      "7     malaria\n",
      "5     dengue\n",
      "5     dementia\n",
      "5     obesity\n",
      "5     autism\n",
      "5     asthma\n",
      "3     polio\n",
      "3     stroke\n",
      "3     leptospirosis\n",
      "2     cholera\n",
      "2     haemophilia\n",
      "2     mesothelioma\n",
      "2     typhoid\n",
      "2     paralysis\n",
      "2     HIV\n",
      "\n",
      "\n",
      "CLASS: condition\n",
      "32     diabete\n",
      "18     asthma\n",
      "9     depression\n",
      "8     cancer\n",
      "7     eczema\n",
      "7     dementia\n",
      "5     anxiety\n",
      "5     arthritis\n",
      "4     adhd\n",
      "4     stroke\n",
      "3     obesity\n",
      "2     psoriasis\n",
      "2     cataract\n",
      "2     temperature\n",
      "2     epilepsy\n",
      "2     acne\n",
      "2     pneumonia\n",
      "2     breast\n",
      "2     autism\n",
      "2     emphysema\n",
      "\n",
      "\n",
      "CLASS: product\n",
      "7     gasoline\n",
      "5     cheese\n",
      "5     bread\n",
      "4     toothpaste\n",
      "3     steel\n",
      "2     petrol\n",
      "2     sausage\n",
      "2     Januvia\n",
      "2     insecticide\n",
      "2     car\n",
      "2     exfoliant\n",
      "2     death\n",
      "2     drug\n",
      "2     alcohol\n",
      "2     egg\n",
      "2     yoghurt\n",
      "2     fish\n",
      "2     cake\n",
      "2     laptop\n",
      "2     smartphone\n",
      "\n",
      "\n",
      "CLASS: state\n",
      "29     California\n",
      "8     Iowa\n",
      "7     Pennsylvania\n",
      "7     Florida\n",
      "7     Ohio\n",
      "4     Michigan\n",
      "4     Poland\n",
      "4     Texas\n",
      "4     Arizona\n",
      "4     Colorado\n",
      "4     Greece\n",
      "3     Indiana\n",
      "3     Russia\n",
      "3     Jordan\n",
      "3     Nevada\n",
      "3     Victoria\n",
      "3     Queensland\n",
      "3     Tennessee\n",
      "2     Illinois\n",
      "2     Italy\n",
      "\n",
      "\n",
      "CLASS: rival\n",
      "10     Apple\n",
      "9     Netflix\n",
      "5     Amazon\n",
      "5     Tesco\n",
      "4     sky\n",
      "4     Aldi\n",
      "4     Xiaomi\n",
      "4     Google\n",
      "4     Russia\n",
      "3     Samsung\n",
      "2     Barclays\n",
      "2     BMW\n",
      "2     China\n",
      "2     Coles\n",
      "2     Poundworld\n",
      "2     Facebook\n",
      "1     Nokia\n",
      "1     Argentina\n",
      "1     Heineken\n",
      "1     load\n",
      "\n",
      "\n",
      "CLASS: sector\n",
      "7     oil\n",
      "6     energy\n",
      "6     tourism\n",
      "5     manufacturing\n",
      "5     utility\n",
      "5     healthcare\n",
      "5     banking\n",
      "4     financial\n",
      "4     health\n",
      "3     technology\n",
      "3     power\n",
      "3     bank\n",
      "3     mining\n",
      "3     education\n",
      "3     construction\n",
      "3     hospitality\n",
      "3     finance\n",
      "3     food\n",
      "2     retail\n",
      "2     infrastructure\n",
      "\n",
      "\n",
      "CLASS: group\n",
      "22     ISIS\n",
      "12     be\n",
      "8     Hamas\n",
      "6     Hezbollah\n",
      "4     Google\n",
      "3     woman\n",
      "2     FairVote\n",
      "2     Blackstone\n",
      "2     child\n",
      "2     gay\n",
      "2     student\n",
      "1     pac\n",
      "1     Saskwatch\n",
      "1     Tawhid\n",
      "1     FreedomWorks\n",
      "1     school\n",
      "1     frog\n",
      "1     Vodafone\n",
      "1     Christians\n",
      "1     Glaad\n",
      "\n",
      "\n",
      "CLASS: market\n",
      "37     China\n",
      "15     India\n",
      "10     Russia\n",
      "10     Brazil\n",
      "5     Australia\n",
      "4     Melbourne\n",
      "3     Europe\n",
      "2     Turkey\n",
      "2     energy\n",
      "2     Britain\n",
      "2     Japan\n",
      "2     Mexico\n",
      "2     Germany\n",
      "2     Spain\n",
      "1     Algeria\n",
      "1     Argentina\n",
      "1     Columbus\n",
      "1     power\n",
      "1     Sydney\n",
      "1     Mitsuwa\n",
      "\n",
      "\n",
      "CLASS: factor\n",
      "11     age\n",
      "7     obesity\n",
      "6     smoking\n",
      "6     diet\n",
      "4     education\n",
      "3     temperature\n",
      "3     stress\n",
      "3     cholesterol\n",
      "3     diabete\n",
      "2     home\n",
      "2     pollution\n",
      "2     pregnancy\n",
      "2     price\n",
      "2     nutrition\n",
      "2     population\n",
      "2     gender\n",
      "2     political\n",
      "2     inflation\n",
      "2     birth\n",
      "2     income\n",
      "\n",
      "\n",
      "CLASS: giant\n",
      "30     Google\n",
      "15     Facebook\n",
      "10     Apple\n",
      "8     Amazon\n",
      "5     Microsoft\n",
      "4     Comcast\n",
      "2     Tesco\n",
      "2     Neptune\n",
      "2     China\n",
      "2     Netflix\n",
      "2     Twitter\n",
      "2     Shell\n",
      "2     Brazil\n",
      "1     Lehman\n",
      "1     woolworth\n",
      "1     Hulu\n",
      "1     Jupiter\n",
      "1     Infineon\n",
      "1     bank\n",
      "1     GE\n",
      "\n",
      "\n",
      "CLASS: food\n",
      "7     fruit\n",
      "6     nut\n",
      "4     butter\n",
      "4     bread\n",
      "4     seed\n",
      "4     browny\n",
      "4     potatoe\n",
      "3     meat\n",
      "3     egg\n",
      "3     pizza\n",
      "3     cake\n",
      "3     cereal\n",
      "3     salad\n",
      "3     burger\n",
      "3     milk\n",
      "2     dairy\n",
      "2     broccoli\n",
      "2     rice\n",
      "2     bean\n",
      "2     fish\n",
      "\n",
      "\n",
      "CLASS: problem\n",
      "8     diabete\n",
      "8     depression\n",
      "7     obesity\n",
      "6     anxiety\n",
      "5     heart\n",
      "4     poverty\n",
      "4     dementia\n",
      "4     asthma\n",
      "3     unemployment\n",
      "2     prejudice\n",
      "2     fall\n",
      "2     nurse\n",
      "1     overcrowding\n",
      "1     smoking\n",
      "1     delay\n",
      "1     impetigo\n",
      "1     crash\n",
      "1     heatstroke\n",
      "1     Ukraine\n",
      "1     dehydration\n",
      "\n",
      "\n",
      "CLASS: industry\n",
      "10     steel\n",
      "9     oil\n",
      "6     healthcare\n",
      "6     tourism\n",
      "5     mining\n",
      "4     technology\n",
      "4     manufacturing\n",
      "4     fishing\n",
      "3     finance\n",
      "2     wholesale\n",
      "2     engineering\n",
      "2     education\n",
      "2     cement\n",
      "2     science\n",
      "2     telecommunication\n",
      "2     coal\n",
      "2     banking\n",
      "1     fashion\n",
      "1     building\n",
      "1     forestry\n",
      "\n",
      "\n",
      "CLASS: drug\n",
      "13     heroin\n",
      "12     cocaine\n",
      "5     tamoxifen\n",
      "4     ice\n",
      "4     ecstasy\n",
      "4     cannabis\n",
      "3     methamphetamine\n",
      "3     botox\n",
      "3     paracetamol\n",
      "3     ibuprofen\n",
      "3     ritalin\n",
      "2     morphine\n",
      "2     steroid\n",
      "2     amphetamine\n",
      "2     lsd\n",
      "2     methadone\n",
      "2     marijuana\n",
      "2     spice\n",
      "2     alcohol\n",
      "1     antifungal\n",
      "\n",
      "\n",
      "CLASS: platform\n",
      "22     Facebook\n",
      "10     Twitter\n",
      "7     Instagram\n",
      "6     YouTube\n",
      "5     WeChat\n",
      "4     kickstarter\n",
      "4     Google\n",
      "3     Uber\n",
      "3     Bloomberg\n",
      "2     iOS\n",
      "2     twitter\n",
      "2     facebook\n",
      "2     Airbnb\n",
      "2     Alibaba\n",
      "2     indiegogo\n",
      "2     tablet\n",
      "1     Weibo\n",
      "1     building\n",
      "1     Hulu\n",
      "1     Reddit\n",
      "\n",
      "\n",
      "CLASS: activity\n",
      "4     walk\n",
      "3     fishing\n",
      "3     swimming\n",
      "3     drug\n",
      "2     canoeing\n",
      "2     oil\n",
      "2     reading\n",
      "2     skiing\n",
      "2     meditation\n",
      "2     yoga\n",
      "2     bathing\n",
      "2     sport\n",
      "2     mining\n",
      "2     baking\n",
      "2     exercise\n",
      "2     hiking\n",
      "2     golf\n",
      "2     football\n",
      "2     banking\n",
      "1     graduation\n",
      "\n",
      "\n",
      "CLASS: firm\n",
      "11     Google\n",
      "7     Facebook\n",
      "6     Uber\n",
      "5     Apple\n",
      "3     Amazon\n",
      "3     Twitter\n",
      "2     aircraft\n",
      "2     bank\n",
      "2     Boeing\n",
      "2     G4S\n",
      "2     Schlumberger\n",
      "2     Didi\n",
      "2     Citigroup\n",
      "2     Infosys\n",
      "1     FedEx\n",
      "1     Aldi\n",
      "1     ESPN\n",
      "1     nPower\n",
      "1     Huawei\n",
      "1     Sprint\n",
      "\n",
      "\n",
      "CLASS: player\n",
      "5     Google\n",
      "3     Oscar\n",
      "2     Santos\n",
      "2     Russia\n",
      "2     rose\n",
      "2     Huawei\n",
      "2     Amazon\n",
      "2     Netflix\n",
      "2     Facebook\n",
      "1     Kaka\n",
      "1     Michael\n",
      "1     REWE\n",
      "1     Emis\n",
      "1     BetFred\n",
      "1     McIlroy\n",
      "1     Laine\n",
      "1     Sherman\n",
      "1     Sterling\n",
      "1     Cisco\n",
      "1     Djokovic\n",
      "\n",
      "\n",
      "CLASS: source\n",
      "48     wind\n",
      "24     solar\n",
      "1     mining\n",
      "1     cistern\n",
      "1     marsh\n",
      "1     rice\n",
      "1     Russia\n",
      "1     car\n",
      "1     medicaid\n",
      "1     job\n",
      "1     life\n",
      "1     sun\n",
      "1     Landsat\n",
      "1     groundwater\n",
      "1     salmon\n",
      "1     truck\n",
      "1     mills\n",
      "1     venting\n",
      "1     Facebook\n",
      "1     market\n",
      "\n",
      "\n",
      "CLASS: nation\n",
      "11     China\n",
      "7     Germany\n",
      "6     Australia\n",
      "4     Russia\n",
      "4     France\n",
      "4     Vietnam\n",
      "3     Italy\n",
      "3     Japan\n",
      "3     Poland\n",
      "2     Hungary\n",
      "2     Kenya\n",
      "2     Syria\n",
      "2     Mozambique\n",
      "2     Britain\n",
      "2     Greece\n",
      "1     Algeria\n",
      "1     India\n",
      "1     America\n",
      "1     England\n",
      "1     Israel\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "data={} #class_of_things -> list\n",
    "with open(\"simple_patterns.tsv\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        clas,example=line.split(\"\\t\")\n",
    "        data.setdefault(clas,[]).append(example)\n",
    "\n",
    "def sort_key(clas_examples):\n",
    "    clas,examples=clas_examples\n",
    "    return len(examples)\n",
    "        \n",
    "items=sorted(data.items(), key=sort_key, reverse=True)\n",
    "        \n",
    "for clas,examples in items[:30]:\n",
    "    print(\"CLASS:\",clas)\n",
    "    for ex,count in collections.Counter(examples).most_common(20):\n",
    "        print(count,\"   \",ex)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* not bad, for a 10-min job on the command line and mere 30M sentences from news!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency trees\n",
    "\n",
    "* Surface patterns may miss a lot of cases\n",
    "* Why?\n",
    "* Remember dependency trees from the first lecture?\n",
    "\n",
    "<img src=\"figs/giant_company.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The elements of a full parse tree\n",
    "\n",
    "* Words\n",
    "* Lemmas\n",
    "* Tags\n",
    "* Morphological features\n",
    "* Dependency relations\n",
    "\n",
    "* Where are they defined:\n",
    "* [Universal Dependencies](https://universaldependencies.org/)\n",
    "\n",
    "## Search in syntactic trees\n",
    "\n",
    "* Simple relations can be stated in terms of patterns in the syntax trees\n",
    "* In theory, this removes much of the surface variation and increases our recall\n",
    "* In practice, there is a balance between increased recall, and parser-induced noise\n",
    "* One also needs a tool to query parse trees\n",
    "\n",
    "## Dep_search\n",
    "\n",
    "* Querying trees in quantities is quite difficult\n",
    "* Need a specialized tool for that\n",
    "* Many exist, few scale up\n",
    "* Here's one https://fginter.github.io/dep_search/\n",
    "* We can try it at http://edu.turkunlp.org/dep_search with the parsed news datasets we used throughout\n",
    "\n",
    "### Search examples\n",
    "\n",
    "* Lemma equals *problem*: `L=problem` [Link](http://edu.turkunlp.org/dep_search/query?search=L%3Dproblem&db=NEWS_EN_10M&case_sensitive=False&hits_per_page=50)\n",
    "* All posessive modifiers of these problem lemmas (who is having a problem): `_ <nmod:poss L=problem` [Link](http://edu.turkunlp.org/dep_search/query?search=_%20%3Cnmod%3Aposs%20L%3Dproblem&db=NEWS_EN_10M&case_sensitive=True&hits_per_page=10)\n",
    "* Keep only NOUN and PROPN words (we are not interested in *he* or *she*): `NOUN|PROPN <nmod:poss L=problem` [Link](http://edu.turkunlp.org/dep_search/query?search=NOUN%7CPROPN%20%3Cnmod%3Aposs%20L%3Dproblem&db=NEWS_EN_10M&case_sensitive=True&hits_per_page=10)\n",
    "* All adjective modifiers (what kind of a problem): `_ <amod L=problem` [Link](http://edu.turkunlp.org/dep_search/query?search=_%20%3Camod%20L%3Dproblem&db=NEWS_EN_10M&case_sensitive=True&hits_per_page=10)\n",
    "* All nominal modifiers of the *problem* lemma with a *with* adposition (a problem with what): `_  >case with <nmod (L=problem >nmod:poss NOUN|PROPN) ` [Link](http://edu.turkunlp.org/dep_search/query?search=_%20%20%3Ecase%20with%20%3Cnmod%20%28L%3Dproblem%20%3Enmod%3Aposs%20NOUN%7CPROPN%29&db=NEWS_EN_10M&case_sensitive=True&hits_per_page=10) \n",
    "\n",
    "### Top hits\n",
    "\n",
    "```\n",
    "# db-name: /home/ginter/dep_search_py2/en_news/trees_00000.db\n",
    "# graph id: 401\n",
    "# db-name: /home/ginter/dep_search_py2/en_news/trees_00000.db\n",
    "# graph id: 4478\n",
    "# graph id: 401\n",
    "# visual-style  80      bgColor:lightgreen\n",
    "# hittoken:     80      person  person  NOUN    NN      Number=Sing     83      nmod:poss       _       SpaceAfter=No\n",
    "# sent_id = 402\n",
    "# text = But in Australia where 14.6 per cent don't believe depression is a mental illness at all, 21.6 per cent would not hire someone who had been depressed, and 24.7 per cent think a person with depression could \"snap out of it\" if they wanted to, how are we to offer healthcare that truly cares for a person who is overweight if we ignore, or refuse to validate, the cause of this person's health problem?\n",
    "1       But     but     CCONJ   CC      _       35      cc      _       _\n",
    "2       in      in      ADP     IN      _       3       case    _       _\n",
    "3       Australia       Australia       PROPN   NNP     Number=Sing     24      obl     _       _\n",
    "4       where   where   ADV     WRB     PronType=Rel    10      advmod  _       _\n",
    "5       14.6    14.6    NUM     CD      NumType=Card    7       nummod  _       _\n",
    "6       per     per     NOUN    NN      Number=Sing     7       compound        _       _\n",
    "7       cent    cent    NOUN    NN      Number=Sing     10      nsubj   _       _\n",
    "8       do      do      AUX     VBP     Mood=Ind|Tense=Pres|VerbForm=Fin        10      aux     _       SpaceAfter=No\n",
    "9       n't     not     PART    RB      _       10      advmod  _       _\n",
    "10      believe believe VERB    VB      VerbForm=Inf    3       acl:relcl       _       _\n",
    "11      depression      depression      NOUN    NN      Number=Sing     15      nsubj   _       _\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "* The matching word is marked as **# hittoken**\n",
    "* Pick most common matching words or lemmas with grep and sort: `cat results.conllu | grep -P '# hittoken:' | cut -f 4 | sort | uniq -c | sort -nr | head -10`\n",
    "\n",
    "* Possessive modifiers:\n",
    "```\n",
    "     58 country\n",
    "     36 world\n",
    "     31 nation\n",
    "     26 America\n",
    "     23 Britain\n",
    "     20 city\n",
    "     17 Europe\n",
    "     17 company\n",
    "     16 Trump\n",
    "     16 state\n",
    "```\n",
    "\n",
    "* Adjectives:\n",
    "```\n",
    "    468 serious\n",
    "    362 big\n",
    "    329 other\n",
    "    313 real\n",
    "    279 biggest\n",
    "    255 major\n",
    "    180 same\n",
    "    175 many\n",
    "    152 only\n",
    "    143 huge\n",
    "```    \n",
    "\n",
    "* Nominal modifiers of a *problem* with a *with* adposition (a problem with what):\n",
    "```\n",
    "      3 woman\n",
    "      2 photo\n",
    "      2 drug\n",
    "      1 wellbeing\n",
    "      1 we\n",
    "      1 voter\n",
    "      1 violence\n",
    "      1 Union\n",
    "      1 Trump\n",
    "      1 this\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* subject-verb-object is a typical example of extraction targets\n",
    "* let us investigate how that data behaves: [subject_verb_object.ipynb](subject_verb_object.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
