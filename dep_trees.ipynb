{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple, rule-based relation extraction\n",
    "\n",
    "* Extract simple relations between entities of interest\n",
    "* Can be specified as surface patterns\n",
    "\n",
    "## Hearst patterns\n",
    "\n",
    "<img src=\"figs/fig_hearst_1.png\" />\n",
    "\n",
    "* See [here](https://arxiv.org/pdf/1806.03191.pdf) for more, details\n",
    "* Simple patterns defined on surface, possibly lemmatized text\n",
    "* Surprisingly good at extracting properties and is-a types of relations\n",
    "* Must be combined with large text amounts\n",
    "\n",
    "## Let's try\n",
    "\n",
    "* We can try these quite easily\n",
    "* Extract text from our parser output\n",
    "* Run simple patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ['On', 'Friday', ',', 'Los', 'Angeles', 'County', 'sheriff', \"'s\", 'detectives', 'announced', 'that', 'they', 'had', 'made', 'an', 'arrest', 'in', 'Broudreaux', \"'s\", 'killing', ',', 'the', 'result', 'of', 'a', 'DNA', 'match', '.']\n",
      "LEMMAS ['on', 'Friday', ',', 'Los', 'Angeles', 'County', 'sheriff', \"'s\", 'detective', 'announce', 'that', 'they', 'have', 'make', 'a', 'arrest', 'in', 'Broudreaux', \"'s\", 'killing', ',', 'the', 'result', 'of', 'a', 'dna', 'match', '.']\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "ID,FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC=range(10) #column names\n",
    "\n",
    "def read_conllu(inp):\n",
    "    \"\"\"The simplest conllu reader I can imagine\"\"\"\n",
    "    current_comments=[]\n",
    "    current_tree=[]\n",
    "    for line in inp:\n",
    "        line=line.strip()\n",
    "        if not line: #empty line -> new tree starting, get rid of the old one\n",
    "            yield current_comments, current_tree\n",
    "            current_comments=[]\n",
    "            current_tree=[]\n",
    "        elif line.startswith(\"#\"):\n",
    "            current_comments.append(line) #this is a comment\n",
    "        else:\n",
    "            current_tree.append(line.split(\"\\t\"))\n",
    "    else: #all done\n",
    "        yield current_comments, current_tree\n",
    "\n",
    "with gzip.open(\"/home/jmnybl/english-news-crawl-30M.conllu.gz\",\"rt\",encoding=\"utf-8\") as f:\n",
    "    for comments, tree in read_conllu(f):\n",
    "        words=[cols[FORM] for cols in tree]\n",
    "        lemmas=[cols[LEMMA] for cols in tree]\n",
    "        print(\"WORDS\", words)\n",
    "        print(\"LEMMAS\", lemmas)\n",
    "        break #or else we print waaay too much\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* as a side remark\n",
    "* many of the things in the course are too heavy or clumsy to run in Jupyter\n",
    "* better run on command line and better written in a real editor, but for the sake of simplicity, we use jupyter to edit our code too\n",
    "* Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conllu2text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile conllu2text.py\n",
    "\n",
    "import gzip\n",
    "import sys\n",
    "ID,FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC=range(10) #column names\n",
    "\n",
    "def read_conllu(inp):\n",
    "    \"\"\"The simplest conllu reader I can imagine\"\"\"\n",
    "    current_comments=[]\n",
    "    current_tree=[]\n",
    "    for line in inp:\n",
    "        line=line.strip()\n",
    "        if not line: #empty line -> new tree starting, get rid of the old one\n",
    "            yield current_comments, current_tree\n",
    "            current_comments=[]\n",
    "            current_tree=[]\n",
    "        elif line.startswith(\"#\"):\n",
    "            current_comments.append(line) #this is a comment\n",
    "        else:\n",
    "            current_tree.append(line.split(\"\\t\"))\n",
    "    else: #all done\n",
    "        yield current_comments, current_tree\n",
    "\n",
    "for comments, tree in read_conllu(sys.stdin):\n",
    "    words=[cols[LEMMA] for cols in tree]\n",
    "    print(\" \".join(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* and now we can run it in the terminal like so\n",
    "* note zipping and unzipping on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "exit #comment this out, it protects me from accidentally running this cell\n",
    "\n",
    "zcat ~jmnybl/english-news-crawl-30M.conllu.gz | python3 conllu2text.py | gzip > english_news_30M.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have English lemmas, let's try some simple patterns\n",
    "* Once again, much can be done on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " giant such as Google and\n",
      " event such as heat and\n",
      " starch such as tapioca and\n",
      " brand such as Otrivine and\n",
      " food such as butter and\n",
      " medium such as pornography and\n",
      " region such as Gansu and\n",
      " excuse such as freedom and\n",
      " county such as Hertfordshire and\n",
      " country such as Finland and\n",
      " company such as Penguin and\n",
      " series such as homeland and\n",
      " country such as Germany and\n",
      " source such as wind and\n",
      " invertebrate such as worm and\n",
      " app such as WhatsApp and\n",
      " place such as Iraq and\n",
      " author such as well and\n",
      " symptom such as breathlessness and\n",
      " event such as concert and\n",
      "\n",
      "and in a tab-delimited form\n",
      "\n",
      " giant\tGoogle\n",
      " event\theat\n",
      " starch\ttapioca\n",
      " brand\tOtrivine\n",
      " food\tbutter\n",
      " medium\tpornography\n",
      " region\tGansu\n",
      " excuse\tfreedom\n",
      " county\tHertfordshire\n",
      " country\tFinland\n",
      " company\tPenguin\n",
      " series\thomeland\n",
      " country\tGermany\n",
      " source\twind\n",
      " invertebrate\tworm\n",
      " app\tWhatsApp\n",
      " place\tIraq\n",
      " author\twell\n",
      " symptom\tbreathlessness\n",
      " event\tconcert\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zcat english_news_30M.txt.gz | grep -Po '\\s\\w+\\ssuch\\sas\\s\\w+\\sand' | head -n 20\n",
    "echo\n",
    "echo \"and in a tab-delimited form\"\n",
    "echo\n",
    "zcat english_news_30M.txt.gz | grep -Po '\\s\\w+\\ssuch\\sas\\s\\w+\\sand' | perl -pe 's/ such as /\\t/;s/ and$//' | head -n 20\n",
    "zcat english_news_30M.txt.gz | grep -Po '\\s\\w+\\ssuch\\sas\\s\\w+\\sand' | perl -pe 's/ such as /\\t/;s/ and$//' > simple_patterns.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the patterns are now in the .tsv file, so we could try reading them in, see what we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: country\n",
      "88     China\n",
      "53     Germany\n",
      "39     France\n",
      "36     India\n",
      "26     Spain\n",
      "25     Italy\n",
      "21     Brazil\n",
      "21     Greece\n",
      "20     Syria\n",
      "20     Canada\n",
      "19     Turkey\n",
      "18     Japan\n",
      "17     Russia\n",
      "16     Australia\n",
      "14     Sweden\n",
      "13     Finland\n",
      "13     Britain\n",
      "12     Vietnam\n",
      "12     Indonesia\n",
      "11     Norway\n",
      "\n",
      "\n",
      "CLASS: company\n",
      "71     Google\n",
      "39     Facebook\n",
      "34     Apple\n",
      "30     Uber\n",
      "10     Amazon\n",
      "8     uber\n",
      "7     Samsung\n",
      "7     IBM\n",
      "6     Tesla\n",
      "6     Unilever\n",
      "6     Netflix\n",
      "5     Boeing\n",
      "5     Twitter\n",
      "5     BP\n",
      "5     Comcast\n",
      "4     SpaceX\n",
      "4     Microsoft\n",
      "4     Vodafone\n",
      "4     ExxonMobil\n",
      "4     Huawei\n",
      "\n",
      "\n",
      "CLASS: service\n",
      "57     Netflix\n",
      "37     Spotify\n",
      "32     Uber\n",
      "23     school\n",
      "13     health\n",
      "12     WhatsApp\n",
      "12     education\n",
      "9     healthcare\n",
      "8     netflix\n",
      "7     uber\n",
      "7     water\n",
      "6     Skype\n",
      "6     Twitter\n",
      "5     Facebook\n",
      "5     Snapchat\n",
      "4     library\n",
      "4     hospital\n",
      "4     electricity\n",
      "4     Pandora\n",
      "3     insurance\n",
      "\n",
      "\n",
      "CLASS: area\n",
      "21     health\n",
      "20     education\n",
      "9     agriculture\n",
      "7     tax\n",
      "6     retail\n",
      "4     housing\n",
      "4     defence\n",
      "4     healthcare\n",
      "4     engineering\n",
      "4     trade\n",
      "4     transport\n",
      "3     Oxford\n",
      "3     banking\n",
      "3     food\n",
      "3     fishery\n",
      "3     custom\n",
      "3     airport\n",
      "3     Mayfair\n",
      "3     mobile\n",
      "3     finance\n",
      "\n",
      "\n",
      "CLASS: issue\n",
      "36     immigration\n",
      "29     abortion\n",
      "15     trade\n",
      "11     health\n",
      "8     tax\n",
      "8     obesity\n",
      "7     race\n",
      "5     depression\n",
      "5     privacy\n",
      "5     education\n",
      "5     childcare\n",
      "5     anxiety\n",
      "5     security\n",
      "5     unemployment\n",
      "4     pay\n",
      "4     housing\n",
      "4     healthcare\n",
      "4     contraception\n",
      "4     racism\n",
      "4     energy\n",
      "\n",
      "\n",
      "CLASS: city\n",
      "16     London\n",
      "14     Beijing\n",
      "8     Birmingham\n",
      "7     Chicago\n",
      "7     Sydney\n",
      "6     Barcelona\n",
      "6     Paris\n",
      "6     Melbourne\n",
      "5     Mumbai\n",
      "5     Toronto\n",
      "5     Shanghai\n",
      "4     Bristol\n",
      "4     Mosul\n",
      "4     Brisbane\n",
      "4     Homs\n",
      "4     Philadelphia\n",
      "3     Leeds\n",
      "3     Manchester\n",
      "3     Berlin\n",
      "3     Detroit\n",
      "\n",
      "\n",
      "CLASS: place\n",
      "9     Iraq\n",
      "8     school\n",
      "8     India\n",
      "5     Syria\n",
      "4     Afghanistan\n",
      "4     Iowa\n",
      "4     restaurant\n",
      "4     Dubai\n",
      "3     Africa\n",
      "3     Chicago\n",
      "3     Yemen\n",
      "3     Libya\n",
      "3     hospital\n",
      "3     park\n",
      "3     China\n",
      "3     France\n",
      "3     Germany\n",
      "3     Australia\n",
      "3     Birmingham\n",
      "2     Thailand\n",
      "\n",
      "\n",
      "CLASS: site\n",
      "53     Facebook\n",
      "14     Twitter\n",
      "6     Instagram\n",
      "5     Amazon\n",
      "5     eBay\n",
      "4     YouTube\n",
      "3     indiegogo\n",
      "3     Airbnb\n",
      "3     kickstarter\n",
      "2     Stubhub\n",
      "2     Zoopla\n",
      "2     health\n",
      "2     gofundme\n",
      "2     Pinterest\n",
      "2     Netflix\n",
      "2     twitter\n",
      "2     Travelsupermarket\n",
      "2     airbnb\n",
      "2     netflix\n",
      "2     embassy\n",
      "\n",
      "\n",
      "CLASS: item\n",
      "14     food\n",
      "6     jewellery\n",
      "5     clothes\n",
      "5     car\n",
      "4     fridge\n",
      "3     diaper\n",
      "3     pension\n",
      "3     furniture\n",
      "3     television\n",
      "3     dishwasher\n",
      "3     smartphone\n",
      "2     battery\n",
      "2     energy\n",
      "2     water\n",
      "2     medicine\n",
      "2     soap\n",
      "2     holiday\n",
      "2     pen\n",
      "2     book\n",
      "2     shoe\n",
      "\n",
      "\n",
      "CLASS: brand\n",
      "8     Gucci\n",
      "6     bond\n",
      "5     AAMI\n",
      "5     Chanel\n",
      "4     Apple\n",
      "4     NRMA\n",
      "4     Burberry\n",
      "3     Prada\n",
      "3     Penfolds\n",
      "3     tide\n",
      "3     Zara\n",
      "3     Lenovo\n",
      "3     Audi\n",
      "2     Nike\n",
      "2     coke\n",
      "2     SmartWater\n",
      "2     rolex\n",
      "2     Vetements\n",
      "2     Sony\n",
      "2     Adidas\n",
      "\n",
      "\n",
      "CLASS: disease\n",
      "32     cancer\n",
      "30     diabete\n",
      "7     measle\n",
      "7     malaria\n",
      "7     hiv\n",
      "7     tuberculosis\n",
      "5     dementia\n",
      "5     asthma\n",
      "5     autism\n",
      "5     dengue\n",
      "5     obesity\n",
      "3     leptospirosis\n",
      "3     stroke\n",
      "3     polio\n",
      "2     diphtheria\n",
      "2     thalassaemia\n",
      "2     aids\n",
      "2     typhoid\n",
      "2     cholera\n",
      "2     emphysema\n",
      "\n",
      "\n",
      "CLASS: condition\n",
      "32     diabete\n",
      "18     asthma\n",
      "9     depression\n",
      "8     cancer\n",
      "7     eczema\n",
      "7     dementia\n",
      "5     anxiety\n",
      "5     arthritis\n",
      "4     stroke\n",
      "4     adhd\n",
      "3     obesity\n",
      "2     temperature\n",
      "2     breast\n",
      "2     acne\n",
      "2     cataract\n",
      "2     autism\n",
      "2     emphysema\n",
      "2     pneumonia\n",
      "2     epilepsy\n",
      "2     psoriasis\n",
      "\n",
      "\n",
      "CLASS: product\n",
      "7     gasoline\n",
      "5     bread\n",
      "5     cheese\n",
      "4     toothpaste\n",
      "3     steel\n",
      "2     egg\n",
      "2     tobacco\n",
      "2     alcohol\n",
      "2     milk\n",
      "2     cake\n",
      "2     laptop\n",
      "2     drug\n",
      "2     insecticide\n",
      "2     death\n",
      "2     asphalt\n",
      "2     sausage\n",
      "2     smartphone\n",
      "2     car\n",
      "2     Januvia\n",
      "2     mortgage\n",
      "\n",
      "\n",
      "CLASS: state\n",
      "29     California\n",
      "8     Iowa\n",
      "7     Florida\n",
      "7     Ohio\n",
      "7     Pennsylvania\n",
      "4     Greece\n",
      "4     Michigan\n",
      "4     Poland\n",
      "4     Texas\n",
      "4     Colorado\n",
      "4     Arizona\n",
      "3     Jordan\n",
      "3     Queensland\n",
      "3     Indiana\n",
      "3     Nevada\n",
      "3     Russia\n",
      "3     Tennessee\n",
      "3     Victoria\n",
      "2     Wyoming\n",
      "2     Alabama\n",
      "\n",
      "\n",
      "CLASS: rival\n",
      "10     Apple\n",
      "9     Netflix\n",
      "5     Tesco\n",
      "5     Amazon\n",
      "4     Xiaomi\n",
      "4     Google\n",
      "4     Russia\n",
      "4     Aldi\n",
      "4     sky\n",
      "3     Samsung\n",
      "2     Poundworld\n",
      "2     China\n",
      "2     Coles\n",
      "2     Facebook\n",
      "2     Barclays\n",
      "2     BMW\n",
      "1     Porsche\n",
      "1     Dallas\n",
      "1     Ericsson\n",
      "1     Huawei\n",
      "\n",
      "\n",
      "CLASS: sector\n",
      "7     oil\n",
      "6     tourism\n",
      "6     energy\n",
      "5     utility\n",
      "5     healthcare\n",
      "5     manufacturing\n",
      "5     banking\n",
      "4     health\n",
      "4     financial\n",
      "3     construction\n",
      "3     power\n",
      "3     technology\n",
      "3     education\n",
      "3     hospitality\n",
      "3     bank\n",
      "3     food\n",
      "3     mining\n",
      "3     finance\n",
      "2     pharmaceutical\n",
      "2     farming\n",
      "\n",
      "\n",
      "CLASS: group\n",
      "22     ISIS\n",
      "12     be\n",
      "8     Hamas\n",
      "6     Hezbollah\n",
      "4     Google\n",
      "3     woman\n",
      "2     student\n",
      "2     child\n",
      "2     FairVote\n",
      "2     gay\n",
      "2     Blackstone\n",
      "1     codebase\n",
      "1     mammal\n",
      "1     frog\n",
      "1     dropbox\n",
      "1     FreedomWorks\n",
      "1     Wingtech\n",
      "1     Sururiyya\n",
      "1     rotary\n",
      "1     cyclist\n",
      "\n",
      "\n",
      "CLASS: market\n",
      "37     China\n",
      "15     India\n",
      "10     Russia\n",
      "10     Brazil\n",
      "5     Australia\n",
      "4     Melbourne\n",
      "3     Europe\n",
      "2     Turkey\n",
      "2     Spain\n",
      "2     Japan\n",
      "2     Germany\n",
      "2     Britain\n",
      "2     energy\n",
      "2     Mexico\n",
      "1     Africa\n",
      "1     Moncton\n",
      "1     housing\n",
      "1     Ethiopia\n",
      "1     Ukraine\n",
      "1     broadband\n",
      "\n",
      "\n",
      "CLASS: factor\n",
      "11     age\n",
      "7     obesity\n",
      "6     diet\n",
      "6     smoking\n",
      "4     education\n",
      "3     cholesterol\n",
      "3     temperature\n",
      "3     diabete\n",
      "3     stress\n",
      "2     population\n",
      "2     pregnancy\n",
      "2     birth\n",
      "2     home\n",
      "2     nutrition\n",
      "2     political\n",
      "2     alcohol\n",
      "2     gender\n",
      "2     pollution\n",
      "2     price\n",
      "2     inflation\n",
      "\n",
      "\n",
      "CLASS: giant\n",
      "30     Google\n",
      "15     Facebook\n",
      "10     Apple\n",
      "8     Amazon\n",
      "5     Microsoft\n",
      "4     Comcast\n",
      "2     Tesco\n",
      "2     Shell\n",
      "2     China\n",
      "2     Neptune\n",
      "2     Twitter\n",
      "2     Netflix\n",
      "2     Brazil\n",
      "1     Infineon\n",
      "1     Kittel\n",
      "1     Compaq\n",
      "1     Samsung\n",
      "1     amazon\n",
      "1     woolworth\n",
      "1     Siemens\n",
      "\n",
      "\n",
      "CLASS: food\n",
      "7     fruit\n",
      "6     nut\n",
      "4     bread\n",
      "4     potatoe\n",
      "4     browny\n",
      "4     seed\n",
      "4     butter\n",
      "3     egg\n",
      "3     milk\n",
      "3     cereal\n",
      "3     burger\n",
      "3     pizza\n",
      "3     cake\n",
      "3     meat\n",
      "3     salad\n",
      "2     broccoli\n",
      "2     banana\n",
      "2     dairy\n",
      "2     ham\n",
      "2     rice\n",
      "\n",
      "\n",
      "CLASS: problem\n",
      "8     depression\n",
      "8     diabete\n",
      "7     obesity\n",
      "6     anxiety\n",
      "5     heart\n",
      "4     asthma\n",
      "4     poverty\n",
      "4     dementia\n",
      "3     unemployment\n",
      "2     fall\n",
      "2     prejudice\n",
      "2     nurse\n",
      "1     numbness\n",
      "1     housing\n",
      "1     radicalisation\n",
      "1     echoe\n",
      "1     trauma\n",
      "1     crime\n",
      "1     damp\n",
      "1     miscarriage\n",
      "\n",
      "\n",
      "CLASS: industry\n",
      "10     steel\n",
      "9     oil\n",
      "6     healthcare\n",
      "6     tourism\n",
      "5     mining\n",
      "4     manufacturing\n",
      "4     technology\n",
      "4     fishing\n",
      "3     finance\n",
      "2     engineering\n",
      "2     coal\n",
      "2     banking\n",
      "2     education\n",
      "2     telecommunication\n",
      "2     wholesale\n",
      "2     cement\n",
      "2     science\n",
      "1     food\n",
      "1     builder\n",
      "1     energy\n",
      "\n",
      "\n",
      "CLASS: drug\n",
      "13     heroin\n",
      "12     cocaine\n",
      "5     tamoxifen\n",
      "4     ice\n",
      "4     cannabis\n",
      "4     ecstasy\n",
      "3     paracetamol\n",
      "3     methamphetamine\n",
      "3     botox\n",
      "3     ibuprofen\n",
      "3     ritalin\n",
      "2     morphine\n",
      "2     alcohol\n",
      "2     lsd\n",
      "2     methadone\n",
      "2     marijuana\n",
      "2     amphetamine\n",
      "2     steroid\n",
      "2     spice\n",
      "1     Yervoy\n",
      "\n",
      "\n",
      "CLASS: platform\n",
      "22     Facebook\n",
      "10     Twitter\n",
      "7     Instagram\n",
      "6     YouTube\n",
      "5     WeChat\n",
      "4     kickstarter\n",
      "4     Google\n",
      "3     Bloomberg\n",
      "3     Uber\n",
      "2     indiegogo\n",
      "2     Airbnb\n",
      "2     Alibaba\n",
      "2     tablet\n",
      "2     iOS\n",
      "2     facebook\n",
      "2     twitter\n",
      "1     Periscope\n",
      "1     television\n",
      "1     Pave\n",
      "1     gofundme\n",
      "\n",
      "\n",
      "CLASS: activity\n",
      "4     walk\n",
      "3     drug\n",
      "3     swimming\n",
      "3     fishing\n",
      "2     skiing\n",
      "2     hiking\n",
      "2     sport\n",
      "2     yoga\n",
      "2     canoeing\n",
      "2     meditation\n",
      "2     baking\n",
      "2     banking\n",
      "2     golf\n",
      "2     exercise\n",
      "2     bathing\n",
      "2     football\n",
      "2     oil\n",
      "2     mining\n",
      "2     reading\n",
      "1     2d\n",
      "\n",
      "\n",
      "CLASS: firm\n",
      "11     Google\n",
      "7     Facebook\n",
      "6     Uber\n",
      "5     Apple\n",
      "3     Twitter\n",
      "3     Amazon\n",
      "2     Didi\n",
      "2     Schlumberger\n",
      "2     bank\n",
      "2     Infosys\n",
      "2     Boeing\n",
      "2     aircraft\n",
      "2     Citigroup\n",
      "2     G4S\n",
      "1     MRI\n",
      "1     legal\n",
      "1     Barwell\n",
      "1     BHP\n",
      "1     shell\n",
      "1     Michelin\n",
      "\n",
      "\n",
      "CLASS: player\n",
      "5     Google\n",
      "3     Oscar\n",
      "2     Netflix\n",
      "2     Facebook\n",
      "2     rose\n",
      "2     Huawei\n",
      "2     Russia\n",
      "2     Santos\n",
      "2     Amazon\n",
      "1     Burfict\n",
      "1     Cespedes\n",
      "1     Poundland\n",
      "1     Kizer\n",
      "1     Woods\n",
      "1     Youi\n",
      "1     Gonzalez\n",
      "1     Harris\n",
      "1     Cisco\n",
      "1     Kaka\n",
      "1     EDF\n",
      "\n",
      "\n",
      "CLASS: source\n",
      "48     wind\n",
      "24     solar\n",
      "1     cistern\n",
      "1     Qatar\n",
      "1     salmon\n",
      "1     car\n",
      "1     groundwater\n",
      "1     uncorroborated\n",
      "1     Russia\n",
      "1     CNN\n",
      "1     truck\n",
      "1     Facebook\n",
      "1     nuclear\n",
      "1     mills\n",
      "1     life\n",
      "1     sun\n",
      "1     mining\n",
      "1     letter\n",
      "1     coal\n",
      "1     Landsat\n",
      "\n",
      "\n",
      "CLASS: website\n",
      "17     Facebook\n",
      "7     Amazon\n",
      "4     Twitter\n",
      "3     Breitbart\n",
      "3     eBay\n",
      "2     Airbnb\n",
      "2     Google\n",
      "2     mumsnet\n",
      "2     YouTube\n",
      "2     Gumtree\n",
      "1     blood\n",
      "1     CNET\n",
      "1     FLNet\n",
      "1     soberista\n",
      "1     Autotrader\n",
      "1     Energyhelpine\n",
      "1     Rightmove\n",
      "1     porn\n",
      "1     adultfriendfinder\n",
      "1     Craigslist\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "data={} #class_of_things -> list\n",
    "with open(\"simple_patterns.tsv\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        clas,example=line.split(\"\\t\")\n",
    "        data.setdefault(clas,[]).append(example)\n",
    "\n",
    "def sort_key(clas_examples):\n",
    "    clas,examples=clas_examples\n",
    "    return len(examples)\n",
    "        \n",
    "items=sorted(data.items(), key=sort_key, reverse=True)\n",
    "        \n",
    "for clas,examples in items[:30]:\n",
    "    print(\"CLASS:\",clas)\n",
    "    for ex,count in collections.Counter(examples).most_common(20):\n",
    "        print(count,\"   \",ex)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* not bad, for a 10-min job on the command line and mere 30M sentences from news!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency trees\n",
    "\n",
    "* Surface patterns don't always work\n",
    "* Why?\n",
    "* Can get even worse in free word order languages\n",
    "* Remember dependency trees from the first lecture?\n",
    "\n",
    "<img src=\"figs/giant_company.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The elements of a full parse tree\n",
    "\n",
    "* Words\n",
    "* Lemmas\n",
    "* Tags\n",
    "* Morphological features\n",
    "* Dependency relations\n",
    "\n",
    "* Where are they defined?\n",
    "* [Universal Dependencies](https://universaldependencies.org/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
