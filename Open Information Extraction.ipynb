{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Information Extraction\n",
    "* Goal: finding any kind of (not predefined) relations between entities\n",
    "* In practice these relations are usually triples (entity1, relation, entity2)\n",
    "* Existing methods vary on how the relation (or lack of it) is discovered\n",
    "* Many systems rely on finding relation phrases between the entities\n",
    "    * Lets have a look: http://openie.allenai.org/\n",
    "* Simple solution:\n",
    "    * Train a classifier (for instance CRF) to tag relation phrases between entities\n",
    "    * How do we get training data for any type of relation?\n",
    "        * No practical way of manually annotating enough data\n",
    "        * Lets use heuristics:\n",
    "            * Find all co-occuring entity pairs and look at their shortest dependency paths\n",
    "            * If the path matches certain criteria (e.g. not too long) assume the pair to be positive example\n",
    "            * Otherwise assume the pair to be negative, i.e. there is no relation between them\n",
    "    * If we do not use lexical features the classifier will hopefully generalize to any type of relation\n",
    "        * POS tags, chunking, dependencies are still valid features\n",
    "* Issues with OpenIE\n",
    "    * Relations are based on the detected phrases, they have to be identical to aggregate the information (we have learned to measure the similarities between phrases, so this is not strictly true)\n",
    "    * Many of the relations are completely irrelevant:\n",
    "        * [http://openie.allenai.org/search?arg1=Othello&rel=&arg2=Shakespeare&corpora=](http://openie.allenai.org/search?arg1=Othello&rel=&arg2=Shakespeare&corpora=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating information: relational clustering\n",
    "* Sometimes we know that a pair of entities co-occurs multiple times, could we use all this information to decide the relation?\n",
    "* We can surely try...\n",
    "* In the next example we are interested in location entities (countries, cities etc.)\n",
    "* Lets gather all **surface patterns** of the relations for our entity pairs\n",
    "    * In this example we've chosen lexicalized dependency paths, e.g. *&lt;nmod sijaita >nmod* (sijaita = located in)\n",
    "    * Bag-of-words might do the trick just as well.\n",
    "* If we fill a matrix with this data we end up with entity pairs as rows and surface patterns as columns\n",
    "    * If a pair of entities occurs in a given pattern, that cell in the matrix will have value 1 (or frequency etc.)\n",
    "    * What we end up with is basically a feature matrix for each pair -> lets cluster them\n",
    "* Now our relations are based on the aggregated information of all surface patterns, we can't pinpoint a single textual mention explicitly describing the relation (it might not even exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to extract named entities (NER lectures) and we also know how to find the surface patterns (feature generation for supervised relation extraction). Here we have it all pulled together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Unique relations: 2163262\n",
      "Unique pairs: 2681141\n",
      "Pairs total: 3985380\n",
      "Eurooppa Afrikka <nmod:poss maa <dobj muuttaa >xcomp kaltainen >nmod:poss maa >nmod:poss\n",
      "Afrikka Eurooppa <nmod:poss maa <nmod:poss kaltainen <xcomp muuttaa >dobj maa >nmod:poss\n",
      "Tampere Pohjois-Satakunnan <nmod:poss markkina-alue <nmod:poss paikallis#radio >nmod radio >appos oy >compound:nn viestintä >nmod:poss\n",
      "Pohjois-Satakunnan Tampere <nmod:poss viestintä <compound:nn oy <appos radio <nmod paikallis#radio >nmod:poss markkina-alue >nmod:poss\n",
      "Pori Oulu <nmod tulla >nmod\n",
      "Oulu Pori <nmod tulla >nmod\n",
      "Slovakia Puola <nmod pitää >nmod\n",
      "Puola Slovakia <nmod pitää >nmod\n",
      "Suomi Japani <nmod olla >nmod\n",
      "Japani Suomi <nmod olla >nmod\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "print \"Reading data\"\n",
    "pairs = pickle.load(open('pairs.pkl'))\n",
    "\n",
    "rel_counter = Counter([p[2] for p in pairs])\n",
    "pair_counter = Counter([(p[0][3], p[1][3]) for p in pairs])\n",
    "\n",
    "print \"Unique relations: %s\" % len(rel_counter.keys())\n",
    "print \"Unique pairs: %s\" % len(pair_counter.keys())\n",
    "print \"Pairs total: %s\" % len(pairs)\n",
    "\n",
    "loc_pairs = [p for p in pairs if p[0][1] == 'loc' and p[1][1] == 'loc' and rel_counter[p[2]] >= 10\n",
    "             and pair_counter[(p[0][3], p[1][3])] >= 10 and 'conj' not in p[2] and len(p[2].split(' ')) >= 3]\n",
    "\n",
    "for pair in loc_pairs[:10]:\n",
    "    print pair[0][3].decode('utf-8'), pair[1][3].decode('utf-8'), pair[2].decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *loc_pairs* is a filtered list of entity pairs collected from a large text corpus (Thanks Jenna!):\n",
    "    * only location entities are included\n",
    "    * only surface patterns and entity pairs which occur at least 10 times are included\n",
    "    * only surface patterns with one or more words between the entities are included\n",
    "    * No conjunction surface forms are included\n",
    "* These filterings are arbitrary results of trial and error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering...\n",
      "Feature matrix shape: 1268, 764\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Clustering...\"\n",
    "pair_dict = defaultdict(dict)\n",
    "\n",
    "for p in loc_pairs:\n",
    "    pair_dict[(p[0][3], p[1][3])][p[2]]=1\n",
    "\n",
    "pair_list = pair_dict.keys()\n",
    "vectorizer = DictVectorizer()\n",
    "features = normalize(vectorizer.fit_transform(pair_dict.values()))\n",
    "\n",
    "print \"Feature matrix shape: %s, %s\" % features.shape\n",
    "\n",
    "n_clusters = 20\n",
    "c = MiniBatchKMeans(n_clusters=n_clusters)\n",
    "\n",
    "clusters = c.fit_predict(features)\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20)\n",
      "************** 0 ***********\n",
      ">>  413 >>  Pori \tJyväskylä\n",
      ">>  47 >>  Helsinki \tvannas#joki\n",
      ">>  631 >>  Islanti \tGrönlanti\n",
      ">>  1232 >>  Konala \tHelsinki\n",
      ">>  1058 >>  Afganistan \tKiina\n",
      ">>  980 >>  Venäjä \tNorja\n",
      ">>  59 >>  Grönlanti \tIslanti\n",
      ">>  850 >>  Hämeenlinna \tHämeenlinna\n",
      ">>  43 >>  Pohjanmaa \tHelsinki\n",
      ">>  1019 >>  Savonlinna \tPunkaharju\n",
      "************** 1 ***********\n",
      ">>  295 >>  Puola \tNorja\n",
      ">>  225 >>  Norja \tPuola\n",
      ">>  1235 >>  Tanska \tViro\n",
      ">>  121 >>  Viro \tLatvia\n",
      ">>  888 >>  Saksa \tViro\n",
      ">>  886 >>  Latvia \tPuola\n",
      ">>  148 >>  Viro \tSaksa\n",
      ">>  1140 >>  Saksa \tLatvia\n",
      ">>  744 >>  Norja \tLatvia\n",
      ">>  140 >>  Viro \tNorja\n",
      "************** 2 ***********\n",
      ">>  561 >>  Yhdysvalta \tMeksiko\n",
      ">>  1123 >>  Helsinki \tBerliini\n",
      ">>  1042 >>  Vaasa \tKokkola\n",
      ">>  1043 >>  Syyria \tJordania\n",
      ">>  9 >>  suomi#lahti \tHelsinki\n",
      ">>  610 >>  Berliini \tHelsinki\n",
      ">>  697 >>  Yhdysvalta \tIntia\n",
      ">>  494 >>  Kokkola \tVaasa\n",
      ">>  1175 >>  Intia \tYhdysvalta\n",
      ">>  925 >>  Helsinki \tsuomi#lahti\n",
      "************** 3 ***********\n",
      ">>  493 >>  Puola \tEurooppa\n",
      ">>  811 >>  Kroatia \tEurooppa\n",
      ">>  1081 >>  Eurooppa \tKroatia\n",
      ">>  481 >>  Eurooppa \tPuola\n",
      ">>  570 >>  Seinäjoki \tSuomi\n",
      ">>  1238 >>  Suomi \tSeinäjoki\n",
      ">>  695 >>  Rauma \tSuomi\n",
      ">>  624 >>  Suomi \tRauma\n",
      ">>  29 >>  Suomi \tRovaniemellä\n",
      ">>  1236 >>  Rovaniemellä \tSuomi\n",
      "************** 4 ***********\n",
      ">>  1112 >>  keski \tSuomi\n",
      ">>  838 >>  Kanada \tAmerikka\n",
      ">>  113 >>  Tanska \tIslanti\n",
      ">>  1185 >>  Katariina \tSuomi\n",
      ">>  196 >>  Pohjois-Pohjanmaan \tSuomi\n",
      ">>  1104 >>  Suomi \tAfganistan\n",
      ">>  185 >>  Japani \tSaksa\n",
      ">>  593 >>  Irlanti \tUSA\n",
      ">>  166 >>  Ruotsi \tPohjanmaa\n",
      ">>  32 >>  Suomi \tuusi\n",
      "************** 5 ***********\n",
      ">>  899 >>  Laos \tVietnam\n",
      ">>  461 >>  USA \tMeksiko\n",
      ">>  38 >>  Jyväskylä \tSeinäjoki\n",
      ">>  1125 >>  Meksiko \tUSA\n",
      ">>  599 >>  Oulu \tRovaniemi\n",
      ">>  304 >>  Rovaniemi \tOulu\n",
      ">>  1243 >>  Tallinna \tRiika\n",
      ">>  1044 >>  Riika \tTallinna\n",
      ">>  848 >>  Jyväskylä \tOulu\n",
      ">>  1141 >>  Vietnam \tLaos\n",
      "************** 6 ***********\n",
      ">>  256 >>  Taiwan \tYhdysvalta\n",
      ">>  239 >>  Ruotsi \tSingapore\n",
      ">>  18 >>  Yhdysvalta \tSingapore\n",
      ">>  314 >>  Singapore \tTaiwan\n",
      ">>  112 >>  Singapore \tYhdysvalta\n",
      ">>  690 >>  Taiwan \tSingapore\n",
      ">>  824 >>  Singapore \tSuomi\n",
      ">>  1137 >>  Singapore \tRuotsi\n",
      ">>  1226 >>  Suomi \tSingapore\n",
      ">>  710 >>  Yhdysvalta \tTaiwan\n",
      "************** 7 ***********\n",
      ">>  192 >>  Yhdysvalta \tPakistan\n",
      ">>  1241 >>  Maaninka \tKuopio\n",
      ">>  550 >>  Suomi \tIran\n",
      ">>  862 >>  Suomi \tSavo\n",
      ">>  139 >>  Suomi \tNiinistö\n",
      ">>  1261 >>  Suomi \tunioni\n",
      ">>  1183 >>  Suomi \tArgentiina\n",
      ">>  901 >>  Espanja \tEurooppa\n",
      ">>  368 >>  Kauniainen \tEspoo\n",
      ">>  241 >>  Saksa \tKiina\n",
      "************** 8 ***********\n",
      ">>  926 >>  Oulu \tTornio\n",
      ">>  487 >>  Kokkola \tHelsinki\n",
      ">>  453 >>  Helsinki \tKokkola\n",
      ">>  992 >>  Islanti \tJapani\n",
      ">>  133 >>  Pori \tHämeenlinna\n",
      ">>  632 >>  Tornio \tOulu\n",
      ">>  670 >>  Espanja \tRuotsi\n",
      ">>  1113 >>  Hämeenlinna \tKuopio\n",
      ">>  357 >>  Raahe \tOulu\n",
      ">>  283 >>  Kajaani \tRovaniemi\n",
      "************** 9 ***********\n",
      ">>  595 >>  itä#valta \tTanska\n",
      ">>  1034 >>  Helsinki \tPorvoo\n",
      ">>  385 >>  Bryssel \tBryssel\n",
      ">>  736 >>  Tansania \tSuomi\n",
      ">>  522 >>  Suomi \tTammisaari\n",
      ">>  65 >>  Tanska \tRanska\n",
      ">>  875 >>  Helsinki \tLahti\n",
      ">>  1234 >>  Porvoo \tHelsinki\n",
      ">>  628 >>  Ranska \titä#valta\n",
      ">>  820 >>  Tammisaari \tSuomi\n",
      "************** 10 ***********\n",
      ">>  666 >>  Espoo \tJyväskylä\n",
      ">>  729 >>  Hämeenlinna \tTampere\n",
      ">>  842 >>  Helsinki \tArabia\n",
      ">>  375 >>  Syyria \tEgypti\n",
      ">>  163 >>  Savonlinna \tMikkeli\n",
      ">>  1048 >>  Boston \tYhdysvalta\n",
      ">>  197 >>  Tampere \tHämeenlinna\n",
      ">>  180 >>  Jyväskylä \tEspoo\n",
      ">>  468 >>  Yhdysvalta \tBoston\n",
      ">>  887 >>  Mikkeli \tSavonlinna\n",
      "************** 11 ***********\n",
      ">>  1197 >>  Britannia \tYhdysvalta\n",
      ">>  526 >>  Saksa \tBritannia\n",
      ">>  770 >>  Britannia \tIrlanti\n",
      ">>  340 >>  Britannia \tSaksa\n",
      ">>  689 >>  Yhdysvalta \tBritannia\n",
      ">>  870 >>  Irlanti \tBritannia\n",
      ">>  694 >>  Irlanti \tSaksa\n",
      ">>  105 >>  Aasia \tAfrikka\n",
      ">>  486 >>  Saksa \tIrlanti\n",
      ">>  41 >>  Afrikka \tAasia\n",
      "************** 12 ***********\n",
      ">>  52 >>  Puola \tUkraina\n",
      ">>  495 >>  USA \tTanska\n",
      ">>  796 >>  Oulu \tYlivieska\n",
      ">>  893 >>  Britannia \tBritannia\n",
      ">>  1097 >>  Kiina \tTaiwan\n",
      ">>  281 >>  Yhdysvalta \tNorja\n",
      ">>  188 >>  Ylivieska \tOulu\n",
      ">>  769 >>  Australia \tYhdysvalta\n",
      ">>  739 >>  Ukraina \tPuola\n",
      ">>  904 >>  Venäjä \tBritannia\n",
      "************** 13 ***********\n",
      ">>  655 >>  Eurooppa \tEnglanti\n",
      ">>  1199 >>  Eurooppa \tItalia\n",
      ">>  823 >>  Kuopio \tJyväskylä\n",
      ">>  270 >>  Porvoo \tSuomi\n",
      ">>  278 >>  Skotlanti \tIrlanti\n",
      ">>  324 >>  Jyväskylä \tKuopio\n",
      ">>  1146 >>  Suomi \tPorvoo\n",
      ">>  28 >>  Oulu \tVenäjä\n",
      ">>  474 >>  Irlanti \tSkotlanti\n",
      ">>  597 >>  Venäjä \tOulu\n",
      "************** 14 ***********\n",
      ">>  426 >>  USA \tSeelanti\n",
      ">>  1036 >>  Britannia \tRanska\n",
      ">>  268 >>  Ranska \tBritannia\n",
      ">>  855 >>  Pori \tOulu\n",
      ">>  1080 >>  Eurooppa \tTsekki\n",
      ">>  1078 >>  Afrikka \tJapani\n",
      ">>  360 >>  Tsekki \tEurooppa\n",
      ">>  333 >>  Japani \tAfrikka\n",
      ">>  263 >>  Seelanti \tUSA\n",
      ">>  813 >>  Somalia \tKenia\n",
      "************** 15 ***********\n",
      ">>  1086 >>  Yhdysvalta \tArgentiina\n",
      ">>  567 >>  Suomi \tHamina\n",
      ">>  419 >>  Kolumbia \tYhdysvalta\n",
      ">>  0 >>  Hamina \tSuomi\n",
      ">>  177 >>  New York \tManhattan\n",
      ">>  439 >>  Irlanti \tIslanti\n",
      ">>  441 >>  Yhdysvalta \tKolumbia\n",
      ">>  592 >>  Manhattan \tNew York\n",
      ">>  352 >>  Kolumbia \tArgentiina\n",
      ">>  289 >>  Naantali \tRymättylä\n",
      "************** 16 ***********\n",
      ">>  649 >>  Helsinki \tTikkurila\n",
      ">>  884 >>  Oulu \tVaala\n",
      ">>  701 >>  Järvenpää \tTuusula\n",
      ">>  50 >>  Vaala \tOulu\n",
      ">>  58 >>  Seinäjoki \tVaasa\n",
      ">>  82 >>  Oulu \tJoensuu\n",
      ">>  16 >>  Tuusula \tJärvenpää\n",
      ">>  773 >>  Kazakstanin \tVenäjä\n",
      ">>  885 >>  Helsinki \tMoskova\n",
      ">>  1247 >>  Hanko \tHelsinki\n",
      "************** 17 ***********\n",
      ">>  296 >>  Suomi \tTurku\n",
      ">>  20 >>  Turku \tSuomi\n",
      ">>  1079 >>  Vantaa \tSuomi\n",
      ">>  95 >>  Suomi \tVantaa\n",
      ">>  292 >>  Norja \tVenäjä\n",
      ">>  59 >>  Grönlanti \tIslanti\n",
      ">>  1224 >>  Suomi \tLähi-itä\n",
      ">>  850 >>  Hämeenlinna \tHämeenlinna\n",
      ">>  479 >>  Arabia \tHelsinki\n",
      ">>  301 >>  Englanti \tYhdysvalta\n",
      "************** 18 ***********\n",
      ">>  723 >>  Hämeenlinna \tSuomi\n",
      ">>  154 >>  Skanskan \tSuomi\n",
      ">>  615 >>  Hattula \tHämeenlinna\n",
      ">>  734 >>  Joensuu \tVaasa\n",
      ">>  383 >>  Eurooppa \tLontoo\n",
      ">>  996 >>  Hämeenlinna \tHattula\n",
      ">>  1092 >>  Suomi \tHämeenlinna\n",
      ">>  1089 >>  Lontoo \tEurooppa\n",
      ">>  957 >>  Vaasa \tTurku\n",
      ">>  688 >>  Salo \tHalikko\n",
      "************** 19 ***********\n",
      ">>  418 >>  Yhdysvalta \tEnglanti\n",
      ">>  1224 >>  Suomi \tLähi-itä\n",
      ">>  301 >>  Englanti \tYhdysvalta\n",
      ">>  290 >>  Brasilia \tEurooppa\n",
      ">>  918 >>  Iran \tKiina\n",
      ">>  470 >>  Afrikka \tAmerikka\n",
      ">>  31 >>  Kiina \tIran\n",
      ">>  1253 >>  Eurooppa \tBrasilia\n",
      ">>  1049 >>  Lähi-itä \tSuomi\n",
      ">>  891 >>  Amerikka \tAfrikka\n"
     ]
    }
   ],
   "source": [
    "distances=c.transform(features)\n",
    "nearest=np.argpartition(distances,10,axis=0)[:10]\n",
    "\n",
    "print nearest.shape\n",
    "\n",
    "for cluster_num,cluster in enumerate(nearest.T):\n",
    "    print \"**************\", cluster_num,\"***********\"\n",
    "    for pair_id in cluster:\n",
    "        print \">> \",pair_id,\">> \", pair_list[pair_id][0], '\\t', pair_list[pair_id][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we make any sense out of these clusters? Lets look at the feature weights of the cluster centers like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features:\n",
      "************** 0 ***********\n",
      "<nmod olla >nmod\n",
      "<nmod suunnata >nmod\n",
      "<nmod alue >nmod:poss\n",
      "<nmod:poss alue >nmod\n",
      "<nmod sijaita >nmod\n",
      "************** 1 ***********\n",
      "<nmod mukana >nmod\n",
      "<nmod juontaa >nmod\n",
      "<nmod osa >nmod\n",
      "<nsubj miehittää >dobj\n",
      "<appos pää#kaupunki >nmod:poss\n",
      "************** 2 ***********\n",
      "<nmod siirtyä >nmod\n",
      "<nmod sijaita >nmod\n",
      "<nmod:poss raja >nmod\n",
      "<nmod pelata >nmod\n",
      "<appos pää#kaupunki >nmod:poss\n",
      "************** 3 ***********\n",
      "<nmod:poss mestari >nmod:poss\n",
      "<nmod tutkia >nmod\n",
      "<dobj kohdata >nsubj\n",
      "<nmod mennä >nsubj\n",
      "<nmod suuntautua >nmod\n",
      "************** 4 ***********\n",
      "<nsubj tulla >nmod\n",
      "<nmod ilmestyä >nmod\n",
      "<appos maa#kunta >nmod:poss\n",
      ">nsubj:cop kaupunki >nmod:poss\n",
      "<dobj liittää >nmod\n",
      "************** 5 ***********\n",
      "<nmod jatkua >nmod\n",
      "<appos ESB >appos\n",
      "<nmod:poss suur#lähetystö >appos\n",
      "<nmod:poss suur#lähetystö >nmod\n",
      "<nmod:poss suur#lähetystö >nmod:poss\n",
      "************** 6 ***********\n",
      "<name Tanska >name\n",
      "<nmod pitää >dobj\n",
      "<dobj pitää >nmod\n",
      "<nsubj olla >nmod\n",
      "<nsubj tarjota >nmod\n",
      "************** 7 ***********\n",
      "<nsubj olla >nmod\n",
      "<appos maa >nmod:poss\n",
      "<nmod esiintyä >nmod\n",
      "<nmod maa >nmod:poss\n",
      "<nmod pitää >nmod\n",
      "************** 8 ***********\n",
      "<nmod muuttaa >nmod\n",
      "<nmod:poss suur#lähetystö >nmod:poss\n",
      "<nmod tulla >nmod\n",
      "<nmod pitää >nmod\n",
      "<nmod:poss suur#lähettiläs >nmod:poss\n",
      "************** 9 ***********\n",
      "<nmod järjestää >nmod\n",
      "<nmod lähteä >nmod\n",
      "<nmod olla >nsubj\n",
      "<nmod suur#lähetystö >nmod:poss\n",
      "<nmod päästä >nmod\n",
      "************** 10 ***********\n",
      "<nmod toimia >nmod\n",
      "<nmod siirtää >nmod\n",
      "<nmod tuoda >nmod\n",
      "<nmod käydä >nmod\n",
      "<nmod ottaa >nmod\n",
      "************** 11 ***********\n",
      "<nmod prosentti >nmod\n",
      "<nmod voittaa >nmod\n",
      "<nmod kokeilla >nmod\n",
      "<nsubj vaatia >nmod\n",
      "<nmod olla >nmod\n",
      "************** 12 ***********\n",
      "<nmod siirtää >nsubj\n",
      "<nsubj löytyä >nmod\n",
      "<nmod matkata >nmod\n",
      "<dobj laskea >nmod\n",
      "<nmod:poss yli#opisto >nmod\n",
      "************** 13 ***********\n",
      "<nmod ajaa >nmod\n",
      "<nmod:poss maa >appos\n",
      "<nsubj hyökätä >nmod\n",
      "<nmod saapua >nmod\n",
      "<nmod:poss maa >nmod\n",
      "************** 14 ***********\n",
      "<nmod tulla >nmod\n",
      "<nmod lentää >nmod\n",
      "<nmod päästä >nmod\n",
      "<nmod matka >nmod\n",
      "<appos suur#kaupunki >nmod:poss\n",
      "************** 15 ***********\n",
      "<nmod asua >nmod\n",
      "<nmod sijaita >nmod\n",
      "<nmod muuttaa >nmod\n",
      "<nmod:poss pää#konttori <nsubj olla >nmod\n",
      "<nmod:poss kaupunki >appos\n",
      "************** 16 ***********\n",
      "<nmod kulkea >nmod\n",
      "<nmod lähteä >nmod\n",
      "<nmod olla >nmod\n",
      "<nmod laskea >nmod\n",
      "<nmod tulla >nsubj\n",
      "************** 17 ***********\n",
      "<nmod olla >nmod\n",
      "<nmod opiskella >nmod\n",
      "<nmod kokoontua >nmod\n",
      "<nmod työskennellä >nmod\n",
      "<nmod tehdä >nmod\n",
      "************** 18 ***********\n",
      "<nmod alkaa >nmod\n",
      "<nmod:poss taide#museo >nmod\n",
      "<nsubj kehittää >nmod\n",
      ">acl:relcl lähteä >nmod\n",
      "<nmod osa >nsubj:cop\n",
      "************** 19 ***********\n",
      "<nmod tehdä >nmod\n",
      "<nmod ulottua >nmod\n",
      "<nmod saada >nmod\n",
      "<nsubj kilpailla >nmod\n",
      "<nmod tuottaa >nmod\n"
     ]
    }
   ],
   "source": [
    "print \"Important features:\"\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print \"**************\", i,\"***********\"\n",
    "    for ii in np.argsort(-c.cluster_centers_[i])[:5]:\n",
    "        print vectorizer.get_feature_names()[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These surface patterns should then be used to decide a relation type for each cluster\n",
    "\n",
    "Issues:\n",
    "* Like always, we don't really know the optimal number of clusters\n",
    "* Clusters are mutually exclusive, i.e. an entity pair can have only one relation between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A step further: distant supervision\n",
    "* In many cases we have an existing knowledge base of relations:\n",
    "    * Wikipedia/wikidata: https://www.wikidata.org/wiki/Wikidata:Main_Page\n",
    "    * Freebase: https://www.freebase.com/\n",
    "* However, we have no connection between these known relations and their textual mentions\n",
    "* Distant supervision: any attempt of aligning these two sources of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent feature model\n",
    "* Lets consider similar matrix as in the relational cluster\n",
    "* Adding relation data from existing knowledge bases is trivial: we create a new column for each known relation (in addition to the surface pattern columns)\n",
    "* Naturally these knowledge bases are not exhaustive, otherwise there would be no point in trying to find related entity pairs\n",
    "* Thus the goal is to create a model to fill in the blanks in our matrix\n",
    "* One way to achieve this is with a latent feature model where we learn a feature vector for each entity tuple and for each relation. We can then use these feature vectors to model the probability of a single cell in the matrix being equal to 1, i.e. a relation existing for a given entity pair, by taking dot product of these latent feature vectors and using the result as an input for a logistic function.\n",
    "* Side note: the logistic function aside this is actually a matrix factorization M = AV, where M is our original matrix, A is the latent feature matrix for relations and V for entity pairs.\n",
    "* Since the model is forced to use the dot product, a pair having relations X and Y will force these relations to have similar feature vectors and vice versa.\n",
    "* How do we train a model like this then?\n",
    "    * Naive approach is to use the known relations as positives and unknown as negatives\n",
    "        * This is obviously unrealistic as a missing relation doesn't mean it doesn't exist\n",
    "        * Gives us still a working model, but training may be sensitive to the proportion of sampled negatives etc.\n",
    "    * State-of-the-art systems use ranking objectives instead, i.e. a known relation should have a higher predicted probability than an unknown one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Enough theory, this is how it is done in practice\n",
    "* We have again similar data matrix as before\n",
    "* This time we add a new column *located_in*\n",
    "    * We use a small list of countries and their capitals as our distant supervision, i.e. for a set of (country, capital) pairs we set the *located_in* to 1, for others it will remain 0\n",
    "* Lets build the model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import all we need + something we don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "\n",
      "Unique pairs: 7693, unique relations: 3863\n",
      "Distant pairs in final set: 11\n"
     ]
    }
   ],
   "source": [
    "print \"Loading data\"\n",
    "train_data = pickle.load(open('./distant_train_data.pkl', 'rb'))\n",
    "pair_count, rel_count, pair_ids, rel_ids, distant_pairs, train_data = train_data\n",
    "reverse_pair_ids = {i: p for p, i in pair_ids.items()}\n",
    "\n",
    "print ''\n",
    "print \"Unique pairs: %s, unique relations: %s\" % (len(pair_ids), len(rel_ids))\n",
    "print \"Distant pairs in final set: %s\"  % len(set(pair_ids.keys()).intersection(set([(p[0][3], p[1][3] ) for p in distant_pairs])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down below is our model implemented in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = Graph()\n",
    "        \n",
    "model.add_input(name='e1', input_shape=(1, ), dtype='int')\n",
    "model.add_input(name='e2', input_shape=(1, ), dtype='int')\n",
    "\n",
    "model.add_node(Embedding(pair_count, 50, input_length=1, trainable=True), input='e1', name='e1e')\n",
    "model.add_node(Embedding(rel_count, 50, input_length=1, trainable=True), input='e2', name='e2e')\n",
    "\n",
    "model.add_node(TimeDistributedDense(1, activation='sigmoid'), name='outd', inputs=['e1e', 'e2e'], merge_mode='dot', concat_axis=-1)\n",
    "\n",
    "model.add_output(name='out', input='outd')\n",
    "\n",
    "print \"Building model\"\n",
    "model.compile(loss={'out':'binary_crossentropy',},\n",
    "              optimizer='adam')\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train on 800160 samples, validate on 200040 samples\n",
      "Epoch 1/6\n",
      "800160/800160 [==============================] - 8s - loss: 0.4432 - val_loss: 0.3778\n",
      "Epoch 2/6\n",
      "800160/800160 [==============================] - 8s - loss: 0.1473 - val_loss: 0.1893\n",
      "Epoch 3/6\n",
      "800160/800160 [==============================] - 7s - loss: 0.0626 - val_loss: 0.1225\n",
      "Epoch 4/6\n",
      "800160/800160 [==============================] - 7s - loss: 0.0287 - val_loss: 0.0964\n",
      "Epoch 5/6\n",
      "800160/800160 [==============================] - 7s - loss: 0.0143 - val_loss: 0.0863\n",
      "Epoch 6/6\n",
      "800160/800160 [==============================] - 7s - loss: 0.0074 - val_loss: 0.0829\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Training\"\n",
    "model.fit(train_data, verbose=1, nb_epoch=6, batch_size=1024, validation_split=0.2)\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we actually learn something? Lets get some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting\n",
      "Done!\n",
      "Top hits with training pairs included (***):\n",
      "Kuuba Havanna ***\n",
      "Intia New Delhi \n",
      "Israel Jerusalem \n",
      "Puola Varsova ***\n",
      "Afganistan Kabul \n",
      "Italia Rooma \n",
      "Norja Oslo ***\n",
      "Latvia Riika ***\n",
      "Irlanti Dublin \n",
      "Kiina Peking ***\n",
      "Bulgaria Sofia \n",
      "Belgia Bryssel \n",
      "Saksa Berliini ***\n",
      "Ranska Pariisi \n",
      "Itä-Suomen Pohjois-Karjalan \n",
      "Portugali Lissabon ***\n",
      "Oulu Kannus \n",
      "Ruotsi Tukholma ***\n",
      "Viro Tallinna ***\n",
      "Kannus Oulu \n"
     ]
    }
   ],
   "source": [
    "print \"Predicting\"\n",
    "test_pairs = np.asarray([[i] for p, i in pair_ids.items()])\n",
    "test_rels = np.asarray([[rel_ids['LOCATED_IN']]*test_pairs.shape[0]]).T\n",
    "\n",
    "pred = model.predict({'e1': test_pairs, 'e2':test_rels})\n",
    "pred_list = pred['out'][:,0,0]\n",
    "\n",
    "dis_pairs = set([(d[0][3], d[1][3]) for d in distant_pairs])\n",
    "res = [reverse_pair_ids[test_pairs[i][0]] for i in np.argsort(-pred_list)]\n",
    "print \"Done!\"\n",
    "\n",
    "print \"Top hits with training pairs included (***):\"\n",
    "\n",
    "for r in res[:20]:\n",
    "    if r in dis_pairs:\n",
    "        in_train = '***'\n",
    "    else:\n",
    "        in_train = ''\n",
    "    print r[0], r[1].decode('utf-8'), in_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aaaaand we are done!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
