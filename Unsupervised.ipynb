{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised methods\n",
    "\n",
    "Most of the methods we've looked at so far have applied *supervised* machine learning, where the learning methods require examples of text **input** (documents, words, etc.) and the **correct outputs** (classes, tags, etc.)\n",
    "\n",
    "By contrast, *unsupervised* methods do not expect outputs, but instead aim to identify commonalities or structure in their input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: language models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical language models estimate the \"probability\" of text, i.e. $P(w_1, w_2, \\ldots, w_n)$\n",
    "\n",
    "Basic N-gram approach:\n",
    "\n",
    "* Assume (incorrectly) that $P(w_i)$ only depends on the N-1 previous words\n",
    "    * e.g. for bigrams $P(w_i|w_{i-1},w_{i-2}, \\ldots) = P(w_i|w_{i-1})$\n",
    "* Count occurrences of N-grams in a large amount of (unlabelled) text\n",
    "    * e.g. [Google Web 5-grams](https://catalog.ldc.upenn.edu/LDC2006T13): 1,024,908,267,229 words (~1 trillion)\n",
    "* Estimate probabilities from counts and apply smoothing\n",
    "\n",
    "---\n",
    "\n",
    "A model that can estimate $P(w_i|w_{i-1},w_{i-2}, \\ldots)$ can be used to generate text: given a \"prompt\" of words, pick the most likely next word, and repeat.\n",
    "\n",
    "Recent example from [Open AI GPT-2](https://openai.com/blog/better-language-models/) neural language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:1em\">\n",
    "<table style=\"font-size:100%; text-align:left\">\n",
    "  <tr>\n",
    "    <td><b>Prompt (human input)</b>:</td>\n",
    "    <td><i>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</i></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>System output</b>:</td>\n",
    "    <td><p>The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.</p>\n",
    "\n",
    "<p>Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.</p>\n",
    "\n",
    "<p>Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.</p>\n",
    "\n",
    "<p>Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.</p>\n",
    "\n",
    "<p>Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.</p>\n",
    "\n",
    "<p>While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”</p>\n",
    "\n",
    "<p>[...]</p>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics\n",
    "\n",
    "Methods that aim to derive (representations of) meaning from text statistics. Based on a comparatively old idea (mandatory quotes):\n",
    "\n",
    "<div style=\"text-align:center; font-size:120%; margin-top:1em\"><i>If A and B have almost identical environments we say that they are synonyms</i> (Harris 1954)</div>\n",
    "\n",
    "<div style=\"text-align:center; font-size:120%; margin-top:1em\"><i>You shall know a word by the company it keeps</i> (Firth 1957)</div>\n",
    "\n",
    "<div style=\"text-align:center; font-size:120%; margin-top:1em\"><i>Words which are similar in meaning occur in similar contexts</i> (Rubenstein and Goodenough, 1965)</div>\n",
    "\n",
    "---\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:1em\">\n",
    "<table style=\"font-size:100%; text-align:left\">\n",
    "    <tr style=\"background-color:lightgray\"><td>A large</td><td><b>___</b></td><td>runs in the yard</td></tr>\n",
    "    <tr style=\"background-color:lightgreen\"><td>A large</td><td><b>dog</b></td><td>runs in the yard</td></tr>\n",
    "    <tr style=\"background-color:lightgreen\"><td>A large</td><td><b>cat</b></td><td>runs in the yard</td></tr>\n",
    "    <tr style=\"background-color:yellow\"><td>A large</td><td><b>hat</b></td><td>runs in the yard</td></tr>\n",
    "    <tr style=\"background-color:orangered\"><td>A large</td><td><b>the</b></td><td>runs in the yard</td></tr>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic approach:\n",
    "\n",
    "* Create a **word-context matrix** that records occurrences of words in context (for various definitions of context)\n",
    "    * Most typically **word-word matrix**\n",
    "* Process a large corpus of text, summing up (word, context) counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/word_context_example.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply smoothing and/or statistics measuring association strength (e.g. PMI, TF-IDF)\n",
    "* Perform dimensionality reduction\n",
    "\n",
    "The process creates **word vectors** that aim to reflect word relationships: e.g. `similarity(cat,dog)` > `similarity(cat,hat)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothetical example with *very* small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['a', 'cat', 'dog', 'eats', 'runs', 'the', 'yard']\n",
    "\n",
    "word_word_matrix = [\n",
    "    [  0, 43, 57,  0,  0,  0, 12 ],\n",
    "    [  0,  0,  0, 13,  9,  0,  0 ],\n",
    "    [  0,  0,  0,  9, 11,  0,  0 ],\n",
    "    [ 21,  0,  0,  0,  0, 16,  0 ],\n",
    "    [  5,  0,  0,  0,  0,  2,  0 ],\n",
    "    [  0, 33, 38,  0,  0,  0,  7 ],\n",
    "    [  0,  0,  0,  0,  0,  0,  0 ],    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>eats</th>\n",
       "      <th>runs</th>\n",
       "      <th>the</th>\n",
       "      <th>yard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eats</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>runs</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yard</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a  cat  dog  eats  runs  the  yard\n",
       "a      0   43   57     0     0    0    12\n",
       "cat    0    0    0    13     9    0     0\n",
       "dog    0    0    0     9    11    0     0\n",
       "eats  21    0    0     0     0   16     0\n",
       "runs   5    0    0     0     0    2     0\n",
       "the    0   33   38     0     0    0     7\n",
       "yard   0    0    0     0     0    0     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "DataFrame(word_word_matrix, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar words should have similar vectors (rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog \t [ 0  0  0  9 11  0  0]\n",
      "cat \t [ 0  0  0 13  9  0  0]\n",
      "eats \t [21  0  0  0  0 16  0]\n",
      "runs \t [5 0 0 0 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "vec = dict((words[i], np.array(word_word_matrix[i])) for i in range(len(words)))\n",
    "\n",
    "for w in ['dog', 'cat', 'eats', 'runs']:\n",
    "    print(w, '\\t', vec[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector similarity is typically measured using *cosine similarity*, the dot product of normalized (length 1) vectors:\n",
    "\n",
    "$$cos(a,b) = \\frac{a \\cdot b}{\\|a\\|\\|b\\|} = \\frac{\\sum_i{a_i b_i}}{\\sqrt{\\sum_i{a_i^2}}\\sqrt{\\sum_i{b_i^2}}}$$\n",
    "\n",
    "(i.e. for normalized vectors, this is just the sum of the elementwise products)\n",
    "\n",
    "As the name suggests, this corresponds to the cosine of the angle between the vectors:\n",
    "\n",
    "* if $a$ and $b$ pointing in the same direction, $cos(a,b) = 1$\n",
    "* if $a$ and $b$ have a near 90 degree angle, $cos(a,b) = 0$\n",
    "* if $a$ and $b$ point in opposite directions, $cos(a,b) = -1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(dog ,cat ) = 0.96\n",
      "sim(dog ,eats) = 0.00\n",
      "sim(eats,runs) = 0.96\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "def cos(a, b):\n",
    "    return np.dot(a,b)/(sqrt(np.dot(a,a))*sqrt(np.dot(b,b)))\n",
    "\n",
    "\n",
    "for a, b in [['dog', 'cat'], ['dog', 'eats'], ['eats', 'runs']]:\n",
    "    print('sim({:4s},{:4s}) = {:.2f}'.format(a, b, cos(vec[a], vec[b])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very surprising that an artificially constructed example can be made to work. Can we do this with real data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-based word vectors are easy and effective, but recent results (e.g. [Baroni et al. 2014](http://www.aclweb.org/anthology/P/P14/P14-1023.pdf)) indicate that *prediction-based* methods offer better performance at many tasks.\n",
    "\n",
    "In prediction-based models, the task is to predict a word given its context (or vice versa):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/cbow_and_skipgram.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:80%\">(figure adapted from <a href=\"https://arxiv.org/pdf/1301.3781.pdf\">Mikolov et al. 2013</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors are learned as a \"side effect\" of performing the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We'll be working with the [word2vec](https://github.com/tmikolov/word2vec) implementation of the skip-gram model. Before going into details, let's give this a quick try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from lib import wvlib\n",
    "\n",
    "wv = wvlib.load(\"/course_data/textmine/GoogleNews-vectors-negative300.100K.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.0512695  -0.0223389  -0.172852    0.161133   -0.0844727   0.057373\n",
      "  0.0585938  -0.0825195  -0.0153809  -0.0634766   0.179688   -0.423828\n",
      " -0.022583   -0.166016   -0.0251465   0.107422   -0.199219    0.15918\n",
      " -0.1875     -0.120117    0.155273   -0.0991211   0.142578   -0.164062\n",
      " -0.0893555   0.200195   -0.149414    0.320312    0.328125    0.0244141\n",
      " -0.097168   -0.0820312  -0.036377   -0.0859375  -0.0986328   0.00778198\n",
      " -0.0134277   0.0527344   0.148438    0.333984    0.0166016  -0.212891\n",
      " -0.0150757   0.0524902  -0.107422   -0.0888672   0.249023   -0.0703125\n",
      " -0.0159912   0.0756836  -0.0703125   0.119141    0.229492    0.0141602\n",
      "  0.115234    0.00750732  0.275391   -0.244141    0.296875    0.0349121\n",
      "  0.242188    0.135742    0.142578    0.0175781   0.0292969  -0.121582\n",
      "  0.0228271  -0.0476074  -0.155273    0.00314331  0.345703    0.122559\n",
      " -0.195312    0.0810547  -0.0683594  -0.0147095   0.214844   -0.121094\n",
      "  0.157227   -0.207031    0.136719   -0.129883    0.0529785  -0.271484\n",
      " -0.298828   -0.18457    -0.229492    0.119141    0.0153198  -0.261719\n",
      " -0.123047   -0.0186768  -0.0649414  -0.081543    0.0786133  -0.353516\n",
      "  0.0524902  -0.0245361  -0.00543213 -0.208984  ]\n"
     ]
    }
   ],
   "source": [
    "print(wv['dog'].shape)\n",
    "print(wv['dog'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(dog ,cat ) = 0.76\n",
      "sim(dog ,eats) = 0.22\n",
      "sim(eats,runs) = 0.26\n"
     ]
    }
   ],
   "source": [
    "for a, b in [['dog', 'cat'], ['dog', 'eats'], ['eats', 'runs']]:\n",
    "    print('sim({:4s},{:4s}) = {:.2f}'.format(a, b, cos(wv[a], wv[b])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems OK, if not quite ideal. What are the words with the largest similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.86804897),\n",
       " ('puppy', 0.8106429),\n",
       " ('pit_bull', 0.7803961),\n",
       " ('pooch', 0.7627376),\n",
       " ('cat', 0.7609456),\n",
       " ('golden_retriever', 0.75009006),\n",
       " ('German_shepherd', 0.74651736),\n",
       " ('Rottweiler', 0.7437614),\n",
       " ('beagle', 0.7418621),\n",
       " ('pup', 0.7406911)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.nearest('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(To be continued ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
