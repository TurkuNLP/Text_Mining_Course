{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "In a nutshell (details will be explained during the lecture)\n",
    "\n",
    "- Assign input text into categories, either predefined (supervised) or not (unsupervised/clustering)\n",
    "  - Spam / not spam\n",
    "  - One of several topics\n",
    "  - Who is the author?\n",
    "  - ...\n",
    "- Done with machine learning\n",
    "  - We covered clustering last week, so now we look into **supervised** classification\n",
    "  - Main difference: unsupervised = no training data, supervised = training data\n",
    "- Training data:\n",
    "  - Ready examples of documents and their classes\n",
    "  - Learn the task from these examples\n",
    "  - Unsupervised = we don't know the classes, supervised = we know the classes\n",
    "- Training: text features + model induction algorithm -> model\n",
    "- Classification: text features + model -> predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "- Represent each document for the classifier\n",
    "- E.g.\n",
    "  - Bag of Words (BoW)\n",
    "  - Character N-Grams\n",
    "  - Document metadata\n",
    "  - PoS tags\n",
    "  - ...you name it, someone tried it...\n",
    "  \n",
    "Let's try on Suomi24 VRT data\n",
    "\n",
    "```\n",
    "<text discussionarea=\"Suhteet\" subsections=\"Sinkut\" title=\"Jos ATM hypp√§isi benjihypyn\" views=\"0\" cid=\"unspecified\" anonnick=\"√§tmink√§inen\" comms=\"9\" year=\"2015\" date=\"12.05.2015\" dateto=\"20150512\" tid=\"13592337\" datefrom=\"20150512\" time=\"22:50\" sect=\"Suhteet\" subsect=\"Sinkut\" ssubsect=\"\" sssubsect=\"\" ssssubsect=\"\" sssssubsect=\"\" ssssssubsect=\"\" urlboard=\"http://keskustelu.suomi24.fi/t/13592337\" urlmsg=\"http://keskustelu.suomi24.fi/t/13592337\">\n",
    "<paragraph>\n",
    "<sentence>\n",
    "Niin    1       niin    Adv     CASECHANGE_Up   2       advmod\n",
    "parantaisiko    2       parantaa        V       PRS_Sg3|VOICE_Act|MOOD_Cond|CLIT_Qst    0       ROOT\n",
    "se      3       se      Pron    SUBCAT_Dem|NUM_Sg|CASE_Nom      2       nsubj\n",
    "h√§nen   4       h√§n     Pron    SUBCAT_Pers|NUM_Sg|CASE_Gen     5       poss\n",
    "markkina-arvoaan        5       markkina-arvo   N       NUM_Sg|CASE_Par|POSS_Px3        2       dobj\n",
    "naisten 6       nainen  N       NUM_Pl|CASE_Gen 7       poss\n",
    "silmiss√§        7       silm√§   N       NUM_Pl|CASE_Ine 2       nommod\n",
    "?       8       ?       Punct   _       2       punct\n",
    "</sentence>\n",
    "</paragraph>\n",
    "</text>\n",
    "<text discussionarea=\"Suhteet\" subsections=\"Sinkut\" title=\"Jos ATM hypp√§isi benjihypyn\" cid=\"79614512\" anonnick=\"NaisetOvatElukoita\" comms=\"9\" views=\"\" date=\"20.06.2015\" dateto=\"20150620\" year=\"2015\" tid=\"13592337\" datefrom=\"20150620\" time=\"20:34\" sect=\"Suhteet\" subsect=\"Sinkut\" ssubsect=\"\" sssubsect=\"\" ssssubsect=\"\" sssssubsect=\"\" ssssssubsect=\"\" urlboard=\"http://keskustelu.suomi24.fi/t/13592337\" urlmsg=\"http://keskustelu.suomi24.fi/t/13592337#comment-79614512\">\n",
    "<paragraph>\n",
    "<sentence>\n",
    "No      1       no      Interj  CASECHANGE_Up   3       intj\n",
    "jos     2       jos     Adv     _       3       advmod\n",
    "teet    3       tehd√§   V       PRS_Sg2|VOICE_Act|TENSE_Prs|MOOD_Ind    0       ROOT\n",
    "sen     4       se      Pron    SUBCAT_Dem|NUM_Sg|CASE_Gen      5       poss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count: 12453\n",
      "Distinct topics: Paikkakunnat, Tori, Koti ja rakentaminen, Ty√∂ ja opiskelu, Ajanviete, Nuoret, Ruoka ja juoma, MainPage, Suhteet, Lemmikit, Matkailu, Suomi24, Perhe, Ajoneuvot ja liikenne, Yhteiskunta, Tiede ja teknologia, Harrastukset, Viihde ja kulttuuri, Muoti ja kauneus, Ryhm√§t, Urheilu ja kuntoilu, Talous, Terveys\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import codecs\n",
    "txt_re=re.compile(ur'^<text discussionarea=\"(.*?)\".*tid=\"([0-9]+?)\"',re.U)\n",
    "ignore_re=re.compile(ur'^</?(text|sentence|paragraph)')\n",
    "\n",
    "\n",
    "def read_vrt(inp):\n",
    "    \"\"\"Function to read the Suomi24 VRT format\"\"\"\n",
    "    current_topic=None #topic name\n",
    "    current_tid=None #discussion thread number\n",
    "    words=[] #words in the discussion\n",
    "    for line in inp:\n",
    "        line=line.strip()\n",
    "        match=txt_re.match(line)\n",
    "        if match: #we have a new post\n",
    "            if match.group(2)!=current_tid and words:#...and it is not part of the current thread\n",
    "                yield current_topic, words\n",
    "                words=[]\n",
    "            current_topic=match.group(1) #Pick groups out of the regular expression\n",
    "            current_tid=match.group(2)\n",
    "        if ignore_re.match(line):\n",
    "            continue\n",
    "        columns=line.split(u\"\\t\")\n",
    "        if not columns[1].isdigit(): #there seem to be few broken ones, skip\n",
    "            continue\n",
    "        words.append(columns[2].lower())\n",
    "    else: #for loop ran out of items\n",
    "        if words:\n",
    "            yield current_topic, words\n",
    "\n",
    "topics=[] #list of strings\n",
    "texts=[] #list of strings\n",
    "with codecs.open(\"s24.vrt\",\"r\",\"utf-8\") as f:\n",
    "    for topic, words in read_vrt(f):\n",
    "        topics.append(topic)\n",
    "        texts.append(u\" \".join(words))\n",
    "\n",
    "print \"Document count:\", len(topics)\n",
    "print \"Distinct topics:\", u\", \".join(set(topics))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.IDF weights\n",
    "\n",
    "$$ TF\\cdot\\frac{N}{DF} $$\n",
    "\n",
    "* TF - term frequency - count of term in current document\n",
    "* N - number of documents in the data\n",
    "* DF - number of documents with the term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents x features (12453, 229764)\n",
      "feature matrix\n",
      "  (0, 227112)\t0.13654656756\n",
      "  (0, 227095)\t0.109876693955\n",
      "  (0, 226981)\t0.112921934495\n",
      "  (0, 223285)\t0.0671284409095\n",
      "  (0, 219663)\t0.0644993532335\n",
      "  (0, 218297)\t0.0261740937501\n",
      "  (0, 217821)\t0.0602237358203\n",
      "  (0, 215121)\t0.0799111500286\n",
      "  (0, 208541)\t0.0322894333732\n",
      "  (0, 208466)\t0.104748001507\n",
      "  (0, 203132)\t0.0731111585761\n",
      "  (0, 202008)\t0.0455023327263\n",
      "  (0, 200001)\t0.150731517533\n",
      "  (0, 199437)\t0.0306226943858\n",
      "  (0, 199156)\t0.0911279995445\n",
      "  (0, 198843)\t0.100900616753\n",
      "  (0, 198766)\t0.0270201961422\n",
      "  (0, 190661)\t0.0596916611611\n",
      "  (0, 187069)\t0.0299879590476\n",
      "  (0, 180836)\t0.0968389976216\n",
      "  (0, 176730)\t0.0316865123023\n",
      "  (0, 176630)\t0.0637229924694\n",
      "  (0, 175613)\t0.0543242115156\n",
      "  (0, 175480)\t0.112138936284\n",
      "  (0, 171781)\t0.0798338024262\n",
      "  :\t:\n",
      "  (12452, 51024)\t0.0471395227882\n",
      "  (12452, 49511)\t0.0523624941598\n",
      "  (12452, 47227)\t0.0938046288728\n",
      "  (12452, 39342)\t0.0198309929611\n",
      "  (12452, 38446)\t0.0428498935496\n",
      "  (12452, 36926)\t0.0342077174773\n",
      "  (12452, 35690)\t0.0402627124092\n",
      "  (12452, 33898)\t0.063963055679\n",
      "  (12452, 22045)\t0.221856562337\n",
      "  (12452, 22028)\t0.110637013986\n",
      "  (12452, 21767)\t0.0563268241309\n",
      "  (12452, 18529)\t0.0501545164628\n",
      "  (12452, 18505)\t0.0745904885567\n",
      "  (12452, 17933)\t0.0327734929389\n",
      "  (12452, 16719)\t0.0671784333884\n",
      "  (12452, 16541)\t0.0404879839675\n",
      "  (12452, 15929)\t0.059314092321\n",
      "  (12452, 15615)\t0.025171807376\n",
      "  (12452, 15520)\t0.0538450978686\n",
      "  (12452, 13416)\t0.0543064632033\n",
      "  (12452, 1439)\t0.163055672364\n",
      "  (12452, 1283)\t0.0283312901216\n",
      "  (12452, 1267)\t0.0325255517423\n",
      "  (12452, 1246)\t0.151895838882\n",
      "  (12452, 973)\t0.0452302655056\n",
      "features\n",
      "1 !\n",
      "5001 16:12\n",
      "10001 4770r-kone\n",
      "15001 aerosoli|pakkaus\n",
      "20001 ansaitsekkaan\n",
      "25001 bailu\n",
      "30001 counter\n",
      "35001 ellinooraaa\n",
      "40001 eu|ajo|kortti\n",
      "45001 grupperna\n",
      "50001 henkil√∂|tunniste\n",
      "55001 http://jukkawallin.puheenvuoro.uusisuomi.fi/197028-toistaako-historia-jalleen-itseaan\n",
      "60001 http://yle.fi/uutiset/kiihottavia_pornoloytoja_ei_juuri_somepaivityksissa_jaeta/7851960\n",
      "65001 hyys√§√§m√§√§√§n\n",
      "70001 inverkar\n",
      "75001 joukko|vaino\n",
      "80001 kaksois|kytkin\n",
      "85001 keksi\n",
      "90001 kissanraksua\n",
      "95001 koukero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2641: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction\n",
    "\n",
    "def tokenizer(txt):\n",
    "    \"\"\"Simple whitespace tokenizer\"\"\"\n",
    "    return txt.split()\n",
    "\n",
    "#Extract the features\n",
    "tfidf_v=sklearn.feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer) #,max_df=0.9\n",
    "d=tfidf_v.fit_transform(texts)\n",
    "print \"documents x features\", d.shape\n",
    "print \"feature matrix\"\n",
    "print d\n",
    "print \"features\"\n",
    "fnames=tfidf_v.get_feature_names()\n",
    "for feature_id in range(1,100000,5000):\n",
    "    print feature_id,fnames[feature_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "* Will be explained during the lecture, Google if you couldn't attend\n",
    "* Key concepts:\n",
    "  - Separating hyperplane\n",
    "  - Margin\n",
    "  - Errors and slack variables\n",
    "  - The parameter C\n",
    "  - Regularization\n",
    "  \n",
    "<img src=\"http://docs.opencv.org/2.4/_images/sample-errors-dist.png\"/>\n",
    "\n",
    "* Multiclass classification = number of classes > 2\n",
    "* One vs all = train a classifier for each class, pick the max score\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "* Will be explained during the lecture, Google key concepts if you couldn't attend\n",
    "* Key concepts:\n",
    "  - Accuracy, Precision, Recall, F-score\n",
    "  - Train / Development / Test Data\n",
    "  - Crossvalidation\n",
    "  - Overfitting\n",
    "  - Parameter optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.010  Accuracy=37.42%\n",
      "C=0.100  Accuracy=59.82%\n",
      "C=1.000  Accuracy=66.68%\n",
      "C=10.000  Accuracy=66.62%\n",
      "C=100.000  Accuracy=65.95%\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "import sklearn.cross_validation\n",
    "X_train,X_test,Y_train,Y_test=sklearn.cross_validation.train_test_split(d, topics, test_size=0.3, random_state=0)\n",
    "\n",
    "for C in (0.01,0.1,1,10,100):\n",
    "    lin_clf = sklearn.svm.LinearSVC(C=C)\n",
    "    lin_clf.fit(X_train,Y_train)\n",
    "    print \"C=%.3f  Accuracy=%.2f%%\"%(C,lin_clf.score(X_test,Y_test)*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...66% is not bad, keeping in mind we have 23 classes to choose from.\n",
    "\n",
    "# Random baseline\n",
    "\n",
    "* That we have 23 classes doesn't mean our baseline is 1/23!\n",
    "* Class imbalance\n",
    "* Accuracy susceptible to this!\n",
    "\n",
    "How do we fare compared to making random choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier predicting most frequent class: 28.88%\n",
      "Dummy classifier predicting at random by class dist.: 13.01%\n"
     ]
    }
   ],
   "source": [
    "import sklearn.dummy\n",
    "dummy=sklearn.dummy.DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train,Y_train)\n",
    "print \"Dummy classifier predicting most frequent class: %.2f%%\"%(dummy.score(X_test,Y_test)*100.0)\n",
    "dummy=sklearn.dummy.DummyClassifier(strategy=\"stratified\")\n",
    "dummy.fit(X_train,Y_train)\n",
    "print \"Dummy classifier predicting at random by class dist.: %.2f%%\"%(dummy.score(X_test,Y_test)*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if you predict the most frequent class, you get to 28% accuracy and with the simple SVM we get 66% accuracy. I.e we can safely say the classifier is learning something. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-grams\n",
    "\n",
    "* Quite popular choice\n",
    "* Does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents x features (12453, 229764)\n",
      "feature matrix\n",
      "  (0, 327495)\t0.0279364084575\n",
      "  (0, 327494)\t0.0246940006294\n",
      "  (0, 326082)\t0.0284459123917\n",
      "  (0, 326081)\t0.0195666672323\n",
      "  (0, 325561)\t0.0570182735877\n",
      "  (0, 325556)\t0.0397059026255\n",
      "  (0, 323870)\t0.0145962063742\n",
      "  (0, 323857)\t0.0141140700881\n",
      "  (0, 323698)\t0.0329225192112\n",
      "  (0, 323691)\t0.0325440317323\n",
      "  (0, 323579)\t0.013446493561\n",
      "  (0, 323569)\t0.00752836225688\n",
      "  (0, 323164)\t0.0143551104277\n",
      "  (0, 323159)\t0.0139049754487\n",
      "  (0, 322949)\t0.0260674398704\n",
      "  (0, 322948)\t0.0252456237298\n",
      "  (0, 322845)\t0.0755417813006\n",
      "  (0, 322843)\t0.0727451050801\n",
      "  (0, 322632)\t0.0144044808613\n",
      "  (0, 322621)\t0.0124514648769\n",
      "  (0, 322401)\t0.0172161118601\n",
      "  (0, 322400)\t0.0151691640639\n",
      "  (0, 322151)\t0.0413451316873\n",
      "  (0, 322141)\t0.0229695854203\n",
      "  (0, 321922)\t0.0415038550878\n",
      "  :\t:\n",
      "  (12452, 7121)\t0.012006175183\n",
      "  (12452, 7119)\t0.0228802062083\n",
      "  (12452, 7107)\t0.0191241708685\n",
      "  (12452, 7104)\t0.00912417192638\n",
      "  (12452, 7100)\t0.0192372220089\n",
      "  (12452, 7093)\t0.0294206477113\n",
      "  (12452, 6326)\t0.0169746067645\n",
      "  (12452, 6325)\t0.0132810244705\n",
      "  (12452, 6323)\t0.0135998608957\n",
      "  (12452, 6281)\t0.0216733813397\n",
      "  (12452, 1349)\t0.0633947077443\n",
      "  (12452, 1348)\t0.0606419300251\n",
      "  (12452, 1307)\t0.0101769197325\n",
      "  (12452, 1304)\t0.0127530473273\n",
      "  (12452, 1303)\t0.0182593358082\n",
      "  (12452, 1216)\t0.0191201747253\n",
      "  (12452, 1215)\t0.0154287139419\n",
      "  (12452, 1214)\t0.0241872736056\n",
      "  (12452, 1207)\t0.0239623912573\n",
      "  (12452, 1174)\t0.0168917636929\n",
      "  (12452, 1163)\t0.0550608416555\n",
      "  (12452, 899)\t0.00946902812298\n",
      "  (12452, 893)\t0.00828576678397\n",
      "  (12452, 881)\t0.0133338344521\n",
      "  (12452, 850)\t0.0174998320168\n",
      "features\n",
      "1 \u0005 ( \n",
      "5001  70e\n",
      "10001  hah\n",
      "15001  sil\n",
      "20001  ‚ñ™ h\n",
      "25001 *k1\n",
      "30001 -31_\n",
      "35001 -zi-\n",
      "40001 .vul\n",
      "45001 /non\n",
      "50001 05/\n",
      "55001 1093\n",
      "60001 1‚Äì45\n",
      "65001 2cb\n",
      "70001 39-\n",
      "75001 4844\n",
      "80001 564b\n",
      "85001 6767\n",
      "90001 7_r\n",
      "95001 8p v\n"
     ]
    }
   ],
   "source": [
    "tfidf_v_char=sklearn.feature_extraction.text.TfidfVectorizer(analyzer='char',ngram_range=(3,4)) #,max_df=0.9\n",
    "d_char=tfidf_v_char.fit_transform(texts)\n",
    "print \"documents x features\", d.shape\n",
    "print \"feature matrix\"\n",
    "print d_char\n",
    "print \"features\"\n",
    "fnames=tfidf_v_char.get_feature_names()\n",
    "for feature_id in range(1,100000,5000):\n",
    "    print feature_id,fnames[feature_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.010  Accuracy=43.98%\n",
      "C=0.100  Accuracy=63.30%\n",
      "C=1.000  Accuracy=68.55%\n"
     ]
    }
   ],
   "source": [
    "X_train_char,X_test_char,Y_train_char,Y_test_char=\\\n",
    "    sklearn.cross_validation.train_test_split(d_char, topics, test_size=0.3, random_state=0)\n",
    "\n",
    "for C in (0.01,0.1,1):\n",
    "    lin_clf_char = sklearn.svm.LinearSVC(C=C)\n",
    "    lin_clf_char.fit(X_train_char,Y_train_char)\n",
    "    print \"C=%.3f  Accuracy=%.2f%%\"%(C,lin_clf_char.score(X_test_char,Y_test_char)*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget about words and you'll get better numbers! Cool, eh? :)\n",
    "\n",
    "Does this generalize? Let's run on Finnish tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I gathered a bunch of totally random Finnish tweets, will my model work?\n",
    "import json\n",
    "\n",
    "tweets=[]\n",
    "with open(\"fin_tweets.json\",\"r\") as f:\n",
    "    for lineno,line in enumerate(f):\n",
    "        line=line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            tweet=json.loads(line)\n",
    "        except ValueError: #some of these are broken\n",
    "            continue\n",
    "        tweets.append(tweet[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(886, 336566)\n",
      "Yhteiskunta  ---  RT @zeekends: wcw babe ; isha asli sofia rye vanessa üëÖ\n",
      "Yhteiskunta  ---  DaanLuyten #TilItHappensToYou #BestMovieSong #iHeartAwards\n",
      "Yhteiskunta  ---  Mulla on kangasv√§ri√§ farkuissa rip üò¢üò¢ https://t.co/67iudMBfuj\n",
      "Yhteiskunta  ---  @MaayronFerreira IJAEJIOEJIOEAJOIEAJI\n",
      "Yhteiskunta  ---  #NowPlaying BFC-radio (@BFC_radio) https://t.co/xYZVlndWyG ‚Ä¶ #Erdioo\n",
      "Yhteiskunta  ---  omfg esQUEJ MEESTOYJ\n",
      "Yhteiskunta  ---  Meik√§ oli jo hetken pitk√§perjantaissa. Ja nyt on vasta kiiraskeskiviikko. P√§iv√§t sekoo kun on n√§it√§ pyhi√§.\n",
      "Yhteiskunta  ---  [22:59:10] 118.113.52.162:4384 &gt;&gt; :1433 (TCP:SYN)\n",
      "Ty√∂ ja opiskelu  ---  Oho tukkani on ekaa kertaa vuosiin mitassa jossa se alkaa aaltoilla ellen kampaa sit√§ suoraksi suihkun j√§lkeen. Hassua.\n",
      "Yhteiskunta  ---  #vibrator adulttoys #sextoys https://t.co/dk83khR6lo\n",
      "Yhteiskunta  ---  T√§n√§√§n osui Hip Hop ja Rap YouTube Video Suomessa.„ÄåCheek„Äç's „ÄéKuka Muu Muka„Äè https://t.co/pQQ8kJTWiL\n",
      "Yhteiskunta  ---  [23:00:26] 125.123.234.198:2980 &gt;&gt; :1433 (TCP:SYN)\n",
      "Yhteiskunta  ---  #fucking adulttoys #vibrator https://t.co/fixAjHsUTT\n",
      "Viihde ja kulttuuri  ---  #Lowongan kerja Sales Finance Business Partner - Mondelƒìz International Jakarta https://t.co/VBxzdYnJCN #loker lowker\n",
      "Yhteiskunta  ---  Tykk√§sin @YouTube-videosta, jonka teki @pixeldan https://t.co/IIJcOiWOpu Batman v Superman Massive Mystery Box Unboxing! - Lego,\n",
      "Yhteiskunta  ---  @nicolettching @Mdcarigaba @Espanto2001 zxcvbnm kyaaaah üòçüòÇ\n",
      "Ajanviete  ---  Uva passa https://t.co/vbn2dUmGw1\n",
      "Yhteiskunta  ---  #fucking adulttoys #sextoys https://t.co/W2O7prQfli\n",
      "Yhteiskunta  ---  RT @imazlum: Dr. Itƒ±r Toks√∂z presenting us on Peace Studies, Syria and Refugees #wednesdaytalks @itirtoksoz @MURCIR https://t.co/VZem98vrwo\n",
      "Yhteiskunta  ---  #sextoys adulttoys #masturbating https://t.co/DDPzcocs8P\n",
      "Yhteiskunta  ---  #fucking adulttoys #analdildo https://t.co/59lnPa0FDF\n",
      "Paikkakunnat  ---  Suomessa henkil√∂st√∂johtajat junnaavat yh√§ tehokkuudessa, kun muualla sitouttaminen on t√§rke√§√§\n",
      "https://t.co/TF0OSuLaGD\n",
      "Yhteiskunta  ---  Seiy≈´ : Kanako Mitsuhashi (1999)\n",
      "Yhteiskunta  ---  RT @Partiokuuluu: Monikulttuurisen rekrytoinnin t√§rkein sana on tervetuloa! https://t.co/OsDf9IhaUG #partioscout #rasisminvastainenviikko #‚Ä¶\n",
      "Yhteiskunta  ---  TPS:n p√§√§valmentaja Mika Laurikainen on nimennyt kokoonpanon illan FC Honka-TPS-otteluun #FCTPS #SuomenCup #Ykk√∂nen https://t.co/hACuGsdWh2\n",
      "Yhteiskunta  ---  RT @KiipulaAO: Ajankohtainen julkaisu, ammattitaitomaajoukkueemme juuri kisojen kynnyksell√§! #Abilympics #ammatillinenkoulutus https://t.co‚Ä¶\n",
      "Yhteiskunta  ---  @AjatustenVanki en oo vege. Kanasalaatti on hyv√§√§ x) enk√§ sy√∂ muutenkaan kuin kanahamppareita xD\n",
      "Yhteiskunta  ---  RT @prillvers_kudus: @Prillverskubdg_ amiin YRA\n",
      "#CantWaitAnnivPV5th\n",
      "Yhteiskunta  ---  RT @KouvolanSanomat: Suomen ensimm√§inen Youtube -opintojakso Kaakkois-Suomen ammattikorkeakouluun‚Äâ‚Äî‚Äâopettajina maan suosituimmat‚Ä¶ https://t‚Ä¶\n",
      "Yhteiskunta  ---  Mikkeli: pahoinpitely L√§nsi-Savo 23.3.2016 15:54 18-vuotias nainen puukotti samanik√§is.. #Mikkeli https://t.co/Op9cRbdPCd\n",
      "Yhteiskunta  ---  @kenekk0317 lhmtalrtjstytyftyudrstysryfyjdsfgudtgyhimfti,yhimjnfyundrtmdrtujndrtrdtmutymdrthnrtyndrturydtydhkgfyildtysrtsyssryisrys\n",
      "Yhteiskunta  ---  Kirjoitus antoi kuvan, ett√§ miehille keskivartalolihavuus on ep√§terveellist√§, naisille \"ei kivan n√§k√∂ist√§\". Yll√§ttik√∂ neg. palaute? @ksmlfi\n",
      "Yhteiskunta  ---  LIVE-l√§hetys #Periscope-sovelluksessa: jooo laulua niin o https://t.co/WXgGHgrAX3\n",
      "Yhteiskunta  ---  11:11 wigetta\n",
      "Yhteiskunta  ---  @JuusoQ Ranskanbulldoggi-mittelspitz, on s√∂p√∂!\n",
      "Suhteet  ---  RT @Nduweybadass: Which Rihanna? Rihanna Rihanna or Rihanna Mkhwanazi https://t.co/XghUBGt007\n",
      "Yhteiskunta  ---  @BTS_twt jin~san kakkoi desu  ^^\n",
      "Yhteiskunta  ---  RT @MitasanSharp: MX-3070N / MX-3570N / MX-4070N (Phoenix)\n",
      " \n",
      "Geli≈ümi≈ü CR4 teknolojisiyle Yeni Sharp renkli √ºr√ºnleri‚Ä¶ https://t.co/XwgZz32DY1\n",
      "Yhteiskunta  ---  RT @lehtinen_esa: Suomessa HR-johto keskittyy HR-j√§rjestelmiin, muualla yrityskulttuurin vahvistamiseen ja ihmisten sitouttamiseen.https://‚Ä¶\n",
      "Yhteiskunta  ---  Ruuvi s√§ve! Karjala kadulla Alppilassa. Mit√§ vaa ruuvei! Sheiiiiiiiit! #studio #construction #prolevel #t√∂rkeduni https://t.co/cyXIFSthbC\n",
      "Yhteiskunta  ---  @skewnger hello joo oppa!\n",
      "Yhteiskunta  ---  @TopiYrjl Ei varmasti. Ai ett√§ sit√§ paskamyrskyn m√§√§r√§√§. :D\n",
      "Yhteiskunta  ---  RT @kskokoomus: @alexstubb vastaanottaa Saarij√§rven #kokoomuksen terveiset Harri Lehtiselt√§ ja Esa J√§rviselt√§. https://t.co/5ir4WfTxjy\n",
      "Yhteiskunta  ---  huurhun shuu https://t.co/xyQU8fmtmM\n",
      "Yhteiskunta  ---  #sextoys adulttoys #cockring https://t.co/6Cgg1eiVMd\n",
      "Yhteiskunta  ---  @kuningaskulutta Olisi ollut turhauttavaa joutua siirt√§m√§√§n p√§ivitt√§iset raha-asiat lainan vuoksi. Jotenkin tykk√§√§n, kun ovat erill√§√§n.\n",
      "Yhteiskunta  ---  RT @OutwardBoundFIN: Satavuotiaan Mallan luonnonpuiston uusi tunnus kunnioittaa alueen ainutlaatuista luontoa https://t.co/jT5YMi5VoB https‚Ä¶\n",
      "Yhteiskunta  ---  RT @GreenpeaceSuomi: My√∂s YK:n ihmisoikeusvaltuutettu liittyi mets√§hallituslain arvostelijoihin! #mets√§hallituslaki @UNHumanRights https://‚Ä¶\n",
      "Yhteiskunta  ---  Kuva: Meteoriitti sy√∂ksyi Mikkeliin ‚Äì tulipallo n√§kyi satojen kilometrien p√§√§h√§n {iltasanomat} https://t.co/ML75s9Uvnx\n",
      "Yhteiskunta  ---  RT @catwomaine: slvsufudjbshuygxv https://t.co/aHkyyGFpSX\n",
      "Yhteiskunta  ---  ZXCVBNM\n"
     ]
    }
   ],
   "source": [
    "d_tweet_char=tfidf_v_char.transform(tweets)\n",
    "print d_tweet_char.shape\n",
    "for counter,(tweet, cls) in enumerate(zip(tweets,lin_clf_char.predict(d_tweet_char))):\n",
    "    print cls, \" --- \", tweet\n",
    "    if counter==50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-O\n",
    "\n",
    "Oh good lord - twitter is such crap! [pulling hair 1AM the night before the lecture] Let's try to apply some of our newly acquired skills to recover. :| How about we try run the tweets through the parser and check the words against the top-most Finnish vocabulary and only keep tweets of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oho tukka olla eka kerta vuosi mitta joka se alkaa aaltoilla josei kammata se suora suihku j√§lkeen . hassu .\n",
      "@kuningaskulutta olla olla turhauttaa joutua siirt√§√§ p√§ivitt√§inen raha-asia laina vuoksi . jotenkin tyk√§t√§ , kun olla erill√§√§n .\n",
      "@maijalarmo tuoda Felix uusi korkki olla ihan ykk√∂nen\n",
      "ei haluu liikkua , pit√§√§ menn√§ kauppa mut ulkona sata lumi ja m√§ olla ruokakooma p√§√§ll√§ ugh\n",
      "@BornForFiNRS m√§ ei √§rsytt√§√§ viel√§ koska vet√§√§ just pussi fanipaloi ja nyt sattua maha lol\n",
      "@RenneKorppila vai sellanen kaveri . m√§ ei toisaalta mik√§√§n ihme ett√§ei olla koskaan kuulla ko . tyyppi .\n",
      "@Kinukki sanoma olla selv√§ , ett√§ ei uskoa olla v√§√§r√§ kun arvella sin√§ kertoa t√§m√§ itse ,olethan aikuinen ..\n",
      "@Nysses ei . olla ilo huomata ett√§ min√§ @jysk_fi t√§m√§ tapahtua p√§ivitt√§in ja asiakaspalvelu olla kunnia-asia .\n",
      "paitsi ain olla kiva n√§h√§ √§mmii tappelees mut veikka t√§m√§ menoo vappun sata lumi\n",
      "RT @SaaraHuttunen : m√§ haluta olla terve ja onnellinen . muu prioriteetti m√§ ei nyt olla . toki koulu ois kiva joskus valmistua\n",
      "\n",
      "\n",
      "Oho tukkani on ekaa kertaa vuosiin mitassa jossa se alkaa aaltoilla ellen kampaa sit√§ suoraksi suihkun j√§lkeen . Hassua .\n",
      "@kuningaskulutta Olisi ollut turhauttavaa joutua siirt√§m√§√§n p√§ivitt√§iset raha-asiat lainan vuoksi . Jotenkin tykk√§√§n , kun ovat erill√§√§n .\n",
      "@maijalarmo toi Felixin uusi korkki on ihan ykk√∂nen\n",
      "En haluu liikkuu , pit√§is menn√§ kauppaan mut ulkona sataa lunta ja mulla on ruokakooma p√§√§ll√§ ugh\n",
      "@BornForFiNRS mua ei √§rsyt√§ viel√§ koska vedin just pussin fanipaloi ja nyt sattuu mahaan lol\n",
      "@RenneKorppila Vai sellanen kaveri . Mut eip√§ toisaalta mik√§√§n ihme etten ole koskaan kuullutkaan ko . tyypist√§ .\n",
      "@Kinukki Sanomattakin on selv√§√§ , ett√§ en usko olevani v√§√§r√§ss√§ kun arvelen sinun kertovan t√§ss√§ itsest√§si ,olethan aikuinen ..\n",
      "@Nysses Eik√∂ . On ilo huomata ett√§ meill√§ @jysk_fi t√§t√§ tapahtuu p√§ivitt√§in ja asiakaspalvelu on kunnia-asia .\n",
      "paitsi ain on kiva n√§h√§ √§mmii tappelees mut veikkaan t√§t√§ menoo vappun sataa lunta\n",
      "RT @SaaraHuttunen : M√§ haluan olla terve ja onnellinen . muita prioriteetteja mulla ei nyt ole . Toki koulusta ois kiva joskus valmistua\n"
     ]
    }
   ],
   "source": [
    "import lwvlib\n",
    "wv=lwvlib.load(\"pb34_lemma_200_v2.bin\",70000,70000)\n",
    "\n",
    "def read_conllu(inp):\n",
    "    tweet=[] #list of lemmas\n",
    "    tweet_words=[] #list of words\n",
    "    for line in inp:\n",
    "        line=line.strip().replace(u\"#\",u\"\")\n",
    "        if not line:\n",
    "            yield tweet, tweet_words\n",
    "            tweet=[]\n",
    "            tweet_words=[]\n",
    "        else:\n",
    "            tweet.append(line.split(u\"\\t\")[2])\n",
    "            tweet_words.append(line.split(u\"\\t\")[1])\n",
    "            \n",
    "            \n",
    "import re\n",
    "wrdre=re.compile(u\"^[a-z√§√∂√•-]+$\")\n",
    "def known_words(tweet):\n",
    "    return sum(1 for word in tweet if word in wv.words and wrdre.match(word))\n",
    "\n",
    "tweets=[]\n",
    "tweets_words=[]\n",
    "with codecs.open(\"fin_tweets.conllu\",\"r\",\"utf-8\") as f:\n",
    "    for tweet,tweet_words in read_conllu(f):\n",
    "        if float(known_words(tweet))/len(tweet)>0.7:\n",
    "            tweets.append(u\" \".join(tweet).replace(u\"#\",u\"|\"))\n",
    "            tweets_words.append(u\" \".join(tweet_words))\n",
    "            \n",
    "for t in tweets[:10]:\n",
    "    print t\n",
    "print\n",
    "print\n",
    "for t in tweets_words[:10]:\n",
    "    print t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 336566)\n",
      "Koti ja rakentaminen  ---  Oho tukkani on ekaa kertaa vuosiin mitassa jossa se alkaa aaltoilla ellen kampaa sit√§ suoraksi suihkun j√§lkeen . Hassua .\n",
      "\n",
      "Yhteiskunta  ---  @kuningaskulutta Olisi ollut turhauttavaa joutua siirt√§m√§√§n p√§ivitt√§iset raha-asiat lainan vuoksi . Jotenkin tykk√§√§n , kun ovat erill√§√§n .\n",
      "\n",
      "Paikkakunnat  ---  @maijalarmo toi Felixin uusi korkki on ihan ykk√∂nen\n",
      "\n",
      "Suhteet  ---  En haluu liikkuu , pit√§is menn√§ kauppaan mut ulkona sataa lunta ja mulla on ruokakooma p√§√§ll√§ ugh\n",
      "\n",
      "Suhteet  ---  @BornForFiNRS mua ei √§rsyt√§ viel√§ koska vedin just pussin fanipaloi ja nyt sattuu mahaan lol\n",
      "\n",
      "Yhteiskunta  ---  @RenneKorppila Vai sellanen kaveri . Mut eip√§ toisaalta mik√§√§n ihme etten ole koskaan kuullutkaan ko . tyypist√§ .\n",
      "\n",
      "Suhteet  ---  @Kinukki Sanomattakin on selv√§√§ , ett√§ en usko olevani v√§√§r√§ss√§ kun arvelen sinun kertovan t√§ss√§ itsest√§si ,olethan aikuinen ..\n",
      "\n",
      "Yhteiskunta  ---  @Nysses Eik√∂ . On ilo huomata ett√§ meill√§ @jysk_fi t√§t√§ tapahtuu p√§ivitt√§in ja asiakaspalvelu on kunnia-asia .\n",
      "\n",
      "Yhteiskunta  ---  paitsi ain on kiva n√§h√§ √§mmii tappelees mut veikkaan t√§t√§ menoo vappun sataa lunta\n",
      "\n",
      "Nuoret  ---  RT @SaaraHuttunen : M√§ haluan olla terve ja onnellinen . muita prioriteetteja mulla ei nyt ole . Toki koulusta ois kiva joskus valmistua\n",
      "\n",
      "Yhteiskunta  ---  @gynsy @MikiHoijer Tarkistin viel√§ , ett√§ kuudelta sen piti alkaa . Kerrankos on kytti√§ enemm√§n kuin osallistujia .\n",
      "\n",
      "Paikkakunnat  ---  Nonii , nyt paikka v√§h√§n enemm√§n t√§ynn√§ . H√§m√§r√§sti t√§yttyy t√§√§ n√§in pienes ajas\n",
      "\n",
      "Yhteiskunta  ---  @MaipuMaire @Mallas6 Hyv√§ niin . Saaristossa se h√§ijympi kaveri joka levitt√§√§ h√§ijymp√§√§ tautia .\n",
      "\n",
      "Urheilu ja kuntoilu  ---  Ja niinh√§n se on , ett√§ kova treeni jo itsess√§√§n on stressi ja vie energiaa palautumiseen ja henkinen stressi viel√§ . Hy√∂tyk√§ytt√∂√∂n siis !\n",
      "\n",
      "Lemmikit  ---  @maailmanvaltias enkkuu ! jos lykky siis k√§y ja p√§√§sen ... suuntaan ehk√§ opettajaks mut en sulje muita vaihtoehtoi pois jos mieli muuttuu !\n",
      "\n",
      "Yhteiskunta  ---  @ManninenJoonas katsottavahko ? varmaan siin√§ vaiheessa kun ei en√§√§ pysy hereill√§ tai muuten keskittym√§√§n elokuvaan mitenk√§√§n\n",
      "\n",
      "Paikkakunnat  ---  @yleuutiset Ei ole kun merkki mik√§ viittaa suomalaisesta astia teollisuudesta . T√§mm√∂ist√§ joskus tehty suomessa\n",
      "\n",
      "Ty√∂ ja opiskelu  ---  @emohilkka mut vois my√∂s kirjottaa kaks esseet√§ ennen ku syke alkaa\n",
      "\n",
      "Yhteiskunta  ---  @Mirppu @Elmatule Eik√∂h√§n se tosta aika pian lopu . Vartijat viimeist√§√§n puhaltaa pelin poikki .\n",
      "\n",
      "Suhteet  ---  psykologia on niin mielenkiintosta TYKK√Ñ√ÑN\n",
      "\n",
      "Yhteiskunta  ---  @BLAKKIIIIISSSSS Jos suoraan sanon , on s√§√§litt√§v√§√§ jos rupeaa tubettajaksi vain rahan ja maineen takia . Tubenomithautaan\n",
      "\n",
      "Suhteet  ---  en juurikaan kuuntele david bowien musaa mut t√§lleen pinnallisesti voin sanoa ett√§ oli se kyll√§ perkeleen kaunis √§ij√§\n",
      "\n",
      "Yhteiskunta  ---  Kuka helvetti toi t√§nne k√§mpp√§√§n suklaata ? Terveisin possu .\n",
      "\n",
      "Yhteiskunta  ---  Oon melko varma , ett√§ joku mun naapuri ulkoilutti kissaa , kun tulin t√∂ist√§ kotiin . H√§mmennyin . Ulkona on pakkasta .\n",
      "\n",
      "Paikkakunnat  ---  nonii nyt on eve aino ja elina tehny jo konseptit kai sit√§ itekki piakkoin vois tied√§n ett√§ ne kyll√§ √§rsytt√§√§ monia Anteeksi etuk√§teen : )\n",
      "\n",
      "Yhteiskunta  ---  @heikki_hakala Olen edelleen h√§mill√§ni ja kummissani Moskovan matkan tuloksista . Ett√§ mit√§ ne oikein ovat siis .\n",
      "\n",
      "Paikkakunnat  ---  @wwimpula vois vaan alkaa hoitaa omii asioit puhelimessa tolleen üòÇ? ?\n",
      "\n",
      "Paikkakunnat  ---  Se on siin√§ vaiheessa menetetty tapaus ku pizzeriassa osataan tilauksesi ..\n",
      "\n",
      "Suhteet  ---  @montuttaja @mitavittualehti Nyt on viety valkoinen itseinho ihan uudelle tasolle WhiteGenocideIsReal\n",
      "\n",
      "Urheilu ja kuntoilu  ---  @LottaEmpi mun salisuunnitelmat meni muiden suunnitelmien kanssa ketjussa pl√∂rin√§ksi , huomenna treeni√§ taas salilla !\n",
      "\n",
      "Paikkakunnat  ---  T√§√§ on sellanen cannot unsee tilanne itq\n",
      "\n",
      "Yhteiskunta  ---  Onneksi on internet ja urpoja joihin purkaa oma vitutuksensa .\n",
      "\n",
      "Yhteiskunta  ---  On tuo syke vaan niin mukiinmenev√§ sarja n√§in keskiviikkoiltana üëå\n",
      "\n",
      "Suhteet  ---  @dragmenialI √ÑITI JA ISK√Ñ M√Ñ EL√ÑN OOTTE NIIN IHANIA\n",
      "\n",
      "Suhteet  ---  Nyt kun oon t√§ysik√§nen ni mut pistet√§√§n aina maistelee alkoholijuomii . T√§n√§√§n testiss√§ vadelmaskumppa - kaikki nauro ku irvistelin vaan :(\n",
      "\n",
      "Paikkakunnat  ---  Voi kyl olla et ite j√§nnit√§n enemm√§n ku poikkis mut onhan se iso juttu esitt√§ytyy koko suvulle ! Etenkin ku ollaan muuttamas yhteen ihan pian\n",
      "\n",
      "Yhteiskunta  ---  T√§√§ TIsuomi olis pit√§ny tulla l√§√§ppij√§ kohun aikaan . üòÉ siin√§ olis ollu helvetti irti\n",
      "\n",
      "Yhteiskunta  ---  Haluun ton nyt pelk√§st√§√§ jo esteettisist√§ syist√§ , herra varjele\n",
      "\n",
      "Yhteiskunta  ---  @maaaw Joku vanki on varmaan taas unohtunut palata lomiltansa\n",
      "\n",
      "Paikkakunnat  ---  @dragmenialI S√Ñ OOT JA KATO NYT TOTA T√ÑYDELLINEN KONSEPTI\n",
      "\n",
      "Yhteiskunta  ---  @suski_kaukinen @laurahaimila Tuo oli h√∂lm√∂√§ jopa sinun n√§pp√§imist√∂lt√§si . Rasismi t√§ss√§ ketjussa on kaikki puoleltasi\n",
      "\n",
      "Yhteiskunta  ---  Hetken ihmettelin ettei yksik√§√§n ottelu ole mennyt viel√§ jatkorei 'ille , mut t√§n uuden formaatin mukaan voikin p√§√§tty√§ tasan . Nj√§h..\n",
      "\n",
      "Yhteiskunta  ---  VAARALLISIN\n",
      "\n",
      "Suhteet  ---  Ai vittu . Tajusin just et m√§ en voi jatkaa ku tajusin saatan spoilaa . Sori .\n",
      "\n",
      "Viihde ja kulttuuri  ---  Pitk√§ kausi takana . Upeita hetki√§ ja my√∂s pieni pettymyksen tunne . T√§st√§ kaivetaan voimaa ja palataan vahvempana ensi kauteen ! Ketter√§\n",
      "\n",
      "Suhteet  ---  tai no , ymm√§rr√§n jos siin√§ on kuvia jotka viev√§t koko aukeaman ... mutta kuitenkin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_tweet_char=tfidf_v_char.transform(tweets)\n",
    "print d_tweet_char.shape\n",
    "for counter,(tweet, tweet_words, cls) in enumerate(zip(tweets,tweets_words,lin_clf_char.predict(d_tweet_char))):\n",
    "    print cls, \" --- \", tweet_words\n",
    "    print\n",
    "    if counter==50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And how about the vectors, do they help any?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10225586,  0.08287574,  0.04128639, ...,  0.01366414,\n",
       "         0.14373324,  0.17274647],\n",
       "       [-0.05364433, -0.01276515,  0.0603284 , ...,  0.00620707,\n",
       "         0.19762445,  0.13600904],\n",
       "       [-0.06920946,  0.01818488,  0.03807921, ...,  0.02176174,\n",
       "         0.13039736,  0.17019013],\n",
       "       ..., \n",
       "       [-0.07167699,  0.06385985,  0.05177857, ..., -0.00953654,\n",
       "         0.17387475,  0.13991461],\n",
       "       [-0.06337436,  0.04784775,  0.04624651, ...,  0.03932344,\n",
       "         0.09021433,  0.12855842],\n",
       "       [-0.10096667,  0.06961406,  0.01622496, ..., -0.01524248,\n",
       "         0.14144494,  0.164709  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lwvlib\n",
    "import numpy\n",
    "wv=lwvlib.load(\"pb34_lemma_200_v2.bin\",50000,50000)\n",
    "\n",
    "def doc2vec(txt,wv,i,data_matrix):\n",
    "    \"\"\"Text with whitespace tokenization\n",
    "    wv\n",
    "    i - which row are we filling\n",
    "    data_matrix - and to where?\"\"\"\n",
    "    for w in txt.split():\n",
    "        w=w.lower()\n",
    "        dim=wv.get(w)\n",
    "        if dim==None:\n",
    "            continue\n",
    "        data_matrix[i]+=wv.vectors[dim]\n",
    "\n",
    "#topics,texts\n",
    "data_matrix=numpy.zeros((len(texts),wv.vectors.shape[1]))\n",
    "for i,txt in enumerate(texts):\n",
    "    doc2vec(txt,wv,i,data_matrix)\n",
    "sklearn.preprocessing.normalize(data_matrix,copy=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls=sklearn.svm.LinearSVC(C=1.0)\n",
    "cls.fit(data_matrix[:10000],topics[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59315124337545866"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.score(data_matrix[10000:],topics[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done.\n",
    "\n",
    "Below is a try with multilayer neural network. I won't go into this during the lecture, but leave it here for those who might be interested to check out the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 23\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.6256 - acc: 0.2446 - val_loss: 2.3121 - val_acc: 0.3460\n",
      "Epoch 2/100\n",
      "0s - loss: 2.3534 - acc: 0.3229 - val_loss: 2.1510 - val_acc: 0.3623\n",
      "Epoch 3/100\n",
      "0s - loss: 2.1948 - acc: 0.3580 - val_loss: 2.0203 - val_acc: 0.4263\n",
      "Epoch 4/100\n",
      "0s - loss: 2.0739 - acc: 0.3984 - val_loss: 1.9285 - val_acc: 0.4443\n",
      "Epoch 5/100\n",
      "0s - loss: 1.9835 - acc: 0.4297 - val_loss: 1.8644 - val_acc: 0.4783\n",
      "Epoch 6/100\n",
      "0s - loss: 1.9087 - acc: 0.4584 - val_loss: 1.8500 - val_acc: 0.4673\n",
      "Epoch 7/100\n",
      "0s - loss: 1.8540 - acc: 0.4679 - val_loss: 1.7724 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "0s - loss: 1.8137 - acc: 0.4786 - val_loss: 1.7483 - val_acc: 0.5110\n",
      "Epoch 9/100\n",
      "0s - loss: 1.7763 - acc: 0.4904 - val_loss: 1.7529 - val_acc: 0.5050\n",
      "Epoch 10/100\n",
      "0s - loss: 1.7434 - acc: 0.4894 - val_loss: 1.7137 - val_acc: 0.5137\n",
      "Epoch 11/100\n",
      "0s - loss: 1.7174 - acc: 0.5007 - val_loss: 1.6900 - val_acc: 0.5157\n",
      "Epoch 12/100\n",
      "0s - loss: 1.6938 - acc: 0.5061 - val_loss: 1.7008 - val_acc: 0.5173\n",
      "Epoch 13/100\n",
      "0s - loss: 1.6705 - acc: 0.5099 - val_loss: 1.6553 - val_acc: 0.5313\n",
      "Epoch 14/100\n",
      "0s - loss: 1.6471 - acc: 0.5193 - val_loss: 1.6372 - val_acc: 0.5360\n",
      "Epoch 15/100\n",
      "0s - loss: 1.6302 - acc: 0.5266 - val_loss: 1.6178 - val_acc: 0.5367\n",
      "Epoch 16/100\n",
      "0s - loss: 1.6196 - acc: 0.5250 - val_loss: 1.6064 - val_acc: 0.5510\n",
      "Epoch 17/100\n",
      "0s - loss: 1.5959 - acc: 0.5347 - val_loss: 1.6080 - val_acc: 0.5433\n",
      "Epoch 18/100\n",
      "0s - loss: 1.5795 - acc: 0.5420 - val_loss: 1.5812 - val_acc: 0.5580\n",
      "Epoch 19/100\n",
      "0s - loss: 1.5523 - acc: 0.5494 - val_loss: 1.6282 - val_acc: 0.5287\n",
      "Epoch 20/100\n",
      "0s - loss: 1.5447 - acc: 0.5520 - val_loss: 1.5653 - val_acc: 0.5590\n",
      "Epoch 21/100\n",
      "0s - loss: 1.5260 - acc: 0.5570 - val_loss: 1.5726 - val_acc: 0.5600\n",
      "Epoch 22/100\n",
      "0s - loss: 1.5186 - acc: 0.5589 - val_loss: 1.5600 - val_acc: 0.5627\n",
      "Epoch 23/100\n",
      "0s - loss: 1.5133 - acc: 0.5626 - val_loss: 1.5596 - val_acc: 0.5613\n",
      "Epoch 24/100\n",
      "0s - loss: 1.4998 - acc: 0.5656 - val_loss: 1.5351 - val_acc: 0.5737\n",
      "Epoch 25/100\n",
      "0s - loss: 1.5028 - acc: 0.5626 - val_loss: 1.5214 - val_acc: 0.5737\n",
      "Epoch 26/100\n",
      "0s - loss: 1.4859 - acc: 0.5670 - val_loss: 1.5369 - val_acc: 0.5730\n",
      "Epoch 27/100\n",
      "0s - loss: 1.4837 - acc: 0.5700 - val_loss: 1.5138 - val_acc: 0.5783\n",
      "Epoch 28/100\n",
      "0s - loss: 1.4720 - acc: 0.5757 - val_loss: 1.5225 - val_acc: 0.5773\n",
      "Epoch 29/100\n",
      "0s - loss: 1.4598 - acc: 0.5756 - val_loss: 1.5399 - val_acc: 0.5690\n",
      "Epoch 30/100\n",
      "0s - loss: 1.4522 - acc: 0.5726 - val_loss: 1.5094 - val_acc: 0.5803\n",
      "Epoch 31/100\n",
      "0s - loss: 1.4427 - acc: 0.5837 - val_loss: 1.5136 - val_acc: 0.5753\n",
      "Epoch 32/100\n",
      "0s - loss: 1.4389 - acc: 0.5800 - val_loss: 1.5140 - val_acc: 0.5887\n",
      "Epoch 33/100\n",
      "0s - loss: 1.4246 - acc: 0.5837 - val_loss: 1.5581 - val_acc: 0.5553\n",
      "Epoch 34/100\n",
      "0s - loss: 1.4419 - acc: 0.5771 - val_loss: 1.5260 - val_acc: 0.5783\n",
      "Epoch 35/100\n",
      "0s - loss: 1.4303 - acc: 0.5816 - val_loss: 1.5276 - val_acc: 0.5767\n",
      "Epoch 36/100\n",
      "0s - loss: 1.4193 - acc: 0.5816 - val_loss: 1.5069 - val_acc: 0.5767\n",
      "Epoch 37/100\n",
      "0s - loss: 1.4106 - acc: 0.5903 - val_loss: 1.4945 - val_acc: 0.5897\n",
      "Epoch 38/100\n",
      "0s - loss: 1.4114 - acc: 0.5821 - val_loss: 1.5115 - val_acc: 0.5810\n",
      "Epoch 39/100\n",
      "0s - loss: 1.4094 - acc: 0.5890 - val_loss: 1.5004 - val_acc: 0.5860\n",
      "Epoch 40/100\n",
      "0s - loss: 1.4117 - acc: 0.5850 - val_loss: 1.5551 - val_acc: 0.5747\n",
      "Epoch 41/100\n",
      "0s - loss: 1.4001 - acc: 0.5897 - val_loss: 1.5033 - val_acc: 0.5843\n",
      "Epoch 42/100\n",
      "0s - loss: 1.3960 - acc: 0.5857 - val_loss: 1.5185 - val_acc: 0.5813\n",
      "Epoch 43/100\n",
      "0s - loss: 1.3919 - acc: 0.5907 - val_loss: 1.5479 - val_acc: 0.5547\n",
      "Epoch 44/100\n",
      "0s - loss: 1.3833 - acc: 0.5939 - val_loss: 1.5531 - val_acc: 0.5693\n",
      "Epoch 45/100\n",
      "0s - loss: 1.3740 - acc: 0.5930 - val_loss: 1.4925 - val_acc: 0.5900\n",
      "Epoch 46/100\n",
      "0s - loss: 1.3813 - acc: 0.5960 - val_loss: 1.4962 - val_acc: 0.5873\n",
      "Epoch 47/100\n",
      "0s - loss: 1.3788 - acc: 0.5997 - val_loss: 1.5154 - val_acc: 0.5790\n",
      "Epoch 48/100\n",
      "0s - loss: 1.3751 - acc: 0.5984 - val_loss: 1.4983 - val_acc: 0.5850\n",
      "Epoch 49/100\n",
      "0s - loss: 1.3801 - acc: 0.5959 - val_loss: 1.5082 - val_acc: 0.5857\n",
      "Epoch 50/100\n",
      "0s - loss: 1.3614 - acc: 0.5966 - val_loss: 1.5036 - val_acc: 0.5820\n",
      "Epoch 51/100\n",
      "0s - loss: 1.3596 - acc: 0.6017 - val_loss: 1.4915 - val_acc: 0.5913\n",
      "Epoch 52/100\n",
      "0s - loss: 1.3609 - acc: 0.5971 - val_loss: 1.5064 - val_acc: 0.5867\n",
      "Epoch 53/100\n",
      "0s - loss: 1.3573 - acc: 0.5999 - val_loss: 1.5090 - val_acc: 0.5770\n",
      "Epoch 54/100\n",
      "0s - loss: 1.3509 - acc: 0.6011 - val_loss: 1.4949 - val_acc: 0.5927\n",
      "Epoch 55/100\n",
      "0s - loss: 1.3537 - acc: 0.6047 - val_loss: 1.4949 - val_acc: 0.5887\n",
      "Epoch 56/100\n",
      "0s - loss: 1.3396 - acc: 0.6091 - val_loss: 1.4953 - val_acc: 0.5843\n",
      "Epoch 57/100\n",
      "0s - loss: 1.3388 - acc: 0.6077 - val_loss: 1.5272 - val_acc: 0.5733\n",
      "Epoch 58/100\n",
      "0s - loss: 1.3475 - acc: 0.6036 - val_loss: 1.5136 - val_acc: 0.5813\n",
      "Epoch 59/100\n",
      "0s - loss: 1.3439 - acc: 0.6094 - val_loss: 1.4985 - val_acc: 0.5877\n",
      "Epoch 60/100\n",
      "0s - loss: 1.3462 - acc: 0.6064 - val_loss: 1.5260 - val_acc: 0.5757\n",
      "Epoch 61/100\n",
      "0s - loss: 1.3311 - acc: 0.6051 - val_loss: 1.5134 - val_acc: 0.5780\n",
      "Epoch 62/100\n",
      "0s - loss: 1.3251 - acc: 0.6080 - val_loss: 1.5172 - val_acc: 0.5777\n",
      "Epoch 63/100\n",
      "0s - loss: 1.3261 - acc: 0.6096 - val_loss: 1.5419 - val_acc: 0.5750\n",
      "Epoch 64/100\n",
      "0s - loss: 1.3260 - acc: 0.6133 - val_loss: 1.5002 - val_acc: 0.5850\n",
      "Epoch 65/100\n",
      "0s - loss: 1.3256 - acc: 0.6034 - val_loss: 1.5071 - val_acc: 0.5813\n",
      "Epoch 66/100\n",
      "0s - loss: 1.3245 - acc: 0.6114 - val_loss: 1.5165 - val_acc: 0.5830\n",
      "Epoch 67/100\n",
      "0s - loss: 1.3220 - acc: 0.6114 - val_loss: 1.5059 - val_acc: 0.5803\n",
      "Epoch 68/100\n",
      "0s - loss: 1.3140 - acc: 0.6071 - val_loss: 1.5203 - val_acc: 0.5837\n",
      "Epoch 69/100\n",
      "0s - loss: 1.3061 - acc: 0.6161 - val_loss: 1.4995 - val_acc: 0.5877\n",
      "Epoch 70/100\n",
      "0s - loss: 1.3021 - acc: 0.6146 - val_loss: 1.5347 - val_acc: 0.5750\n",
      "Epoch 71/100\n",
      "0s - loss: 1.3144 - acc: 0.6101 - val_loss: 1.4958 - val_acc: 0.5957\n",
      "Epoch 72/100\n",
      "0s - loss: 1.3095 - acc: 0.6107 - val_loss: 1.5053 - val_acc: 0.5843\n",
      "Epoch 73/100\n",
      "0s - loss: 1.3104 - acc: 0.6166 - val_loss: 1.5255 - val_acc: 0.5780\n",
      "Epoch 74/100\n",
      "0s - loss: 1.3130 - acc: 0.6147 - val_loss: 1.5061 - val_acc: 0.5910\n",
      "Epoch 75/100\n",
      "0s - loss: 1.2970 - acc: 0.6219 - val_loss: 1.5097 - val_acc: 0.5837\n",
      "Epoch 76/100\n",
      "0s - loss: 1.2990 - acc: 0.6134 - val_loss: 1.5172 - val_acc: 0.5773\n",
      "Epoch 77/100\n",
      "0s - loss: 1.3083 - acc: 0.6119 - val_loss: 1.5035 - val_acc: 0.5873\n",
      "Epoch 78/100\n",
      "0s - loss: 1.3085 - acc: 0.6150 - val_loss: 1.5205 - val_acc: 0.5720\n",
      "Epoch 79/100\n",
      "0s - loss: 1.2990 - acc: 0.6140 - val_loss: 1.5246 - val_acc: 0.5797\n",
      "Epoch 80/100\n",
      "0s - loss: 1.2943 - acc: 0.6191 - val_loss: 1.5022 - val_acc: 0.5883\n",
      "Epoch 81/100\n",
      "0s - loss: 1.3001 - acc: 0.6121 - val_loss: 1.5018 - val_acc: 0.5873\n",
      "Epoch 82/100\n",
      "0s - loss: 1.2965 - acc: 0.6160 - val_loss: 1.5085 - val_acc: 0.5883\n",
      "Epoch 83/100\n",
      "0s - loss: 1.2973 - acc: 0.6133 - val_loss: 1.5124 - val_acc: 0.5907\n",
      "Epoch 84/100\n",
      "0s - loss: 1.3013 - acc: 0.6111 - val_loss: 1.5084 - val_acc: 0.5930\n",
      "Epoch 85/100\n",
      "0s - loss: 1.2881 - acc: 0.6253 - val_loss: 1.5353 - val_acc: 0.5817\n",
      "Epoch 86/100\n",
      "0s - loss: 1.2960 - acc: 0.6163 - val_loss: 1.5156 - val_acc: 0.5863\n",
      "Epoch 87/100\n",
      "0s - loss: 1.2851 - acc: 0.6206 - val_loss: 1.5178 - val_acc: 0.5890\n",
      "Epoch 88/100\n",
      "0s - loss: 1.2791 - acc: 0.6204 - val_loss: 1.5148 - val_acc: 0.5850\n",
      "Epoch 89/100\n",
      "0s - loss: 1.2832 - acc: 0.6189 - val_loss: 1.5549 - val_acc: 0.5640\n",
      "Epoch 90/100\n",
      "0s - loss: 1.2866 - acc: 0.6204 - val_loss: 1.5199 - val_acc: 0.5870\n",
      "Epoch 91/100\n",
      "0s - loss: 1.2853 - acc: 0.6216 - val_loss: 1.5179 - val_acc: 0.5790\n",
      "Epoch 92/100\n",
      "0s - loss: 1.2812 - acc: 0.6173 - val_loss: 1.5298 - val_acc: 0.5817\n",
      "Epoch 93/100\n",
      "0s - loss: 1.2878 - acc: 0.6180 - val_loss: 1.5179 - val_acc: 0.5823\n",
      "Epoch 94/100\n",
      "0s - loss: 1.2836 - acc: 0.6109 - val_loss: 1.5149 - val_acc: 0.5870\n",
      "Epoch 95/100\n",
      "0s - loss: 1.2744 - acc: 0.6217 - val_loss: 1.5460 - val_acc: 0.5743\n",
      "Epoch 96/100\n",
      "0s - loss: 1.2722 - acc: 0.6193 - val_loss: 1.5140 - val_acc: 0.5887\n",
      "Epoch 97/100\n",
      "0s - loss: 1.2828 - acc: 0.6164 - val_loss: 1.4998 - val_acc: 0.5927\n",
      "Epoch 98/100\n",
      "0s - loss: 1.2652 - acc: 0.6200 - val_loss: 1.5213 - val_acc: 0.5897\n",
      "Epoch 99/100\n",
      "0s - loss: 1.2837 - acc: 0.6197 - val_loss: 1.5165 - val_acc: 0.5853\n",
      "Epoch 100/100\n",
      "0s - loss: 1.2677 - acc: 0.6239 - val_loss: 1.5176 - val_acc: 0.5873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ce9d10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe we could try with some nonlinear stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import keras.optimizers\n",
    "import keras.utils.np_utils\n",
    "\n",
    "def class2id(topics):\n",
    "    d={}\n",
    "    nums=[]\n",
    "    for t in topics:\n",
    "        nums.append(d.setdefault(t,len(d)))\n",
    "    return nums,d\n",
    "\n",
    "topic_numbers,class_dict=class2id(topics)\n",
    "topic_numbers_matrix=keras.utils.np_utils.to_categorical(topic_numbers)\n",
    "dim_in,dim_internal,dim_out=data_matrix.shape[1],200,len(class_dict)\n",
    "\n",
    "print dim_in, dim_internal,dim_out\n",
    "\n",
    "#Neural network:\n",
    "model = Sequential()\n",
    "#Non-linear layer #1\n",
    "model.add(Dense(dim_internal, input_dim=dim_in))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(dim_internal))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "#Linear projection at the end\n",
    "model.add(Dense(dim_out))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,class_mode='categorical')\n",
    "#Learn!\n",
    "model.fit(data_matrix[:10000],topic_numbers_matrix[:10000],verbose=2,batch_size=200,show_accuracy=True,validation_split=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2453/2453 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58499796167957607"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \n",
    "import sklearn.metrics\n",
    "\n",
    "sklearn.metrics.accuracy_score(topic_numbers[10000:],model.predict_classes(data_matrix[10000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...no luck. :) This time the good old bag of character n-grams beats us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
