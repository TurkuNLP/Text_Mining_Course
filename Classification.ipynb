{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "In a nutshell (details will be explained during the lecture)\n",
    "\n",
    "- Assign input text into categories, either predefined (supervised) or not (unsupervised/clustering)\n",
    "  - Spam / not spam\n",
    "  - One of several topics\n",
    "  - Who is the author?\n",
    "  - ...\n",
    "- Done with machine learning\n",
    "  - We covered clustering last week, so now we look into **supervised** classification\n",
    "  - Main difference: unsupervised = no training data, supervised = training data\n",
    "- Training data:\n",
    "  - Ready examples of documents and their classes\n",
    "  - Learn the task from these examples\n",
    "  - Unsupervised = we don't know the classes, supervised = we know the classes\n",
    "- Training: text features + model induction algorithm -> model\n",
    "- Classification: text features + model -> predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "- Represent each document for the classifier\n",
    "- E.g.\n",
    "  - Bag of Words (BoW)\n",
    "  - Character N-Grams\n",
    "  - Document metadata\n",
    "  - PoS tags\n",
    "  - ...you name it, someone tried it...\n",
    "  \n",
    "Let's try on Suomi24 VRT data\n",
    "\n",
    "```\n",
    "<text discussionarea=\"Suhteet\" subsections=\"Sinkut\" title=\"Jos ATM hyppäisi benjihypyn\" views=\"0\" cid=\"unspecified\" anonnick=\"ätminkäinen\" comms=\"9\" year=\"2015\" date=\"12.05.2015\" dateto=\"20150512\" tid=\"13592337\" datefrom=\"20150512\" time=\"22:50\" sect=\"Suhteet\" subsect=\"Sinkut\" ssubsect=\"\" sssubsect=\"\" ssssubsect=\"\" sssssubsect=\"\" ssssssubsect=\"\" urlboard=\"http://keskustelu.suomi24.fi/t/13592337\" urlmsg=\"http://keskustelu.suomi24.fi/t/13592337\">\n",
    "<paragraph>\n",
    "<sentence>\n",
    "Niin    1       niin    Adv     CASECHANGE_Up   2       advmod\n",
    "parantaisiko    2       parantaa        V       PRS_Sg3|VOICE_Act|MOOD_Cond|CLIT_Qst    0       ROOT\n",
    "se      3       se      Pron    SUBCAT_Dem|NUM_Sg|CASE_Nom      2       nsubj\n",
    "hänen   4       hän     Pron    SUBCAT_Pers|NUM_Sg|CASE_Gen     5       poss\n",
    "markkina-arvoaan        5       markkina-arvo   N       NUM_Sg|CASE_Par|POSS_Px3        2       dobj\n",
    "naisten 6       nainen  N       NUM_Pl|CASE_Gen 7       poss\n",
    "silmissä        7       silmä   N       NUM_Pl|CASE_Ine 2       nommod\n",
    "?       8       ?       Punct   _       2       punct\n",
    "</sentence>\n",
    "</paragraph>\n",
    "</text>\n",
    "<text discussionarea=\"Suhteet\" subsections=\"Sinkut\" title=\"Jos ATM hyppäisi benjihypyn\" cid=\"79614512\" anonnick=\"NaisetOvatElukoita\" comms=\"9\" views=\"\" date=\"20.06.2015\" dateto=\"20150620\" year=\"2015\" tid=\"13592337\" datefrom=\"20150620\" time=\"20:34\" sect=\"Suhteet\" subsect=\"Sinkut\" ssubsect=\"\" sssubsect=\"\" ssssubsect=\"\" sssssubsect=\"\" ssssssubsect=\"\" urlboard=\"http://keskustelu.suomi24.fi/t/13592337\" urlmsg=\"http://keskustelu.suomi24.fi/t/13592337#comment-79614512\">\n",
    "<paragraph>\n",
    "<sentence>\n",
    "No      1       no      Interj  CASECHANGE_Up   3       intj\n",
    "jos     2       jos     Adv     _       3       advmod\n",
    "teet    3       tehdä   V       PRS_Sg2|VOICE_Act|TENSE_Prs|MOOD_Ind    0       ROOT\n",
    "sen     4       se      Pron    SUBCAT_Dem|NUM_Sg|CASE_Gen      5       poss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count: 12453\n",
      "Distinct topics: Paikkakunnat, Tori, Koti ja rakentaminen, Työ ja opiskelu, Ajanviete, Nuoret, Ruoka ja juoma, MainPage, Suhteet, Lemmikit, Matkailu, Suomi24, Perhe, Ajoneuvot ja liikenne, Yhteiskunta, Tiede ja teknologia, Harrastukset, Viihde ja kulttuuri, Muoti ja kauneus, Ryhmät, Urheilu ja kuntoilu, Talous, Terveys\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import codecs\n",
    "txt_re=re.compile(ur'^<text discussionarea=\"(.*?)\".*tid=\"([0-9]+?)\"',re.U)\n",
    "ignore_re=re.compile(ur'^</?(text|sentence|paragraph)')\n",
    "\n",
    "\n",
    "def read_vrt(inp):\n",
    "    \"\"\"Function to read the Suomi24 VRT format\"\"\"\n",
    "    current_topic=None #topic name\n",
    "    current_tid=None #discussion thread number\n",
    "    words=[] #words in the discussion\n",
    "    for line in inp:\n",
    "        line=line.strip()\n",
    "        match=txt_re.match(line)\n",
    "        if match: #we have a new post\n",
    "            if match.group(2)!=current_tid and words:#...and it is not part of the current thread\n",
    "                yield current_topic, words\n",
    "                words=[]\n",
    "            current_topic=match.group(1) #Pick groups out of the regular expression\n",
    "            current_tid=match.group(2)\n",
    "        if ignore_re.match(line):\n",
    "            continue\n",
    "        columns=line.split(u\"\\t\")\n",
    "        if not columns[1].isdigit(): #there seem to be few broken ones, skip\n",
    "            continue\n",
    "        words.append(columns[2].lower())\n",
    "    else: #for loop ran out of items\n",
    "        if words:\n",
    "            yield current_topic, words\n",
    "\n",
    "topics=[] #list of strings\n",
    "texts=[] #list of strings\n",
    "with codecs.open(\"s24.vrt\",\"r\",\"utf-8\") as f:\n",
    "    for topic, words in read_vrt(f):\n",
    "        topics.append(topic)\n",
    "        texts.append(u\" \".join(words))\n",
    "\n",
    "print \"Document count:\", len(topics)\n",
    "print \"Distinct topics:\", u\", \".join(set(topics))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.IDF weights\n",
    "\n",
    "$$ TF\\cdot\\frac{N}{DF} $$\n",
    "\n",
    "* TF - term frequency - count of term in current document\n",
    "* N - number of documents in the data\n",
    "* DF - number of documents with the term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents x features (12453, 229764)\n",
      "feature matrix\n",
      "  (0, 227112)\t0.13654656756\n",
      "  (0, 227095)\t0.109876693955\n",
      "  (0, 226981)\t0.112921934495\n",
      "  (0, 223285)\t0.0671284409095\n",
      "  (0, 219663)\t0.0644993532335\n",
      "  (0, 218297)\t0.0261740937501\n",
      "  (0, 217821)\t0.0602237358203\n",
      "  (0, 215121)\t0.0799111500286\n",
      "  (0, 208541)\t0.0322894333732\n",
      "  (0, 208466)\t0.104748001507\n",
      "  (0, 203132)\t0.0731111585761\n",
      "  (0, 202008)\t0.0455023327263\n",
      "  (0, 200001)\t0.150731517533\n",
      "  (0, 199437)\t0.0306226943858\n",
      "  (0, 199156)\t0.0911279995445\n",
      "  (0, 198843)\t0.100900616753\n",
      "  (0, 198766)\t0.0270201961422\n",
      "  (0, 190661)\t0.0596916611611\n",
      "  (0, 187069)\t0.0299879590476\n",
      "  (0, 180836)\t0.0968389976216\n",
      "  (0, 176730)\t0.0316865123023\n",
      "  (0, 176630)\t0.0637229924694\n",
      "  (0, 175613)\t0.0543242115156\n",
      "  (0, 175480)\t0.112138936284\n",
      "  (0, 171781)\t0.0798338024262\n",
      "  :\t:\n",
      "  (12452, 51024)\t0.0471395227882\n",
      "  (12452, 49511)\t0.0523624941598\n",
      "  (12452, 47227)\t0.0938046288728\n",
      "  (12452, 39342)\t0.0198309929611\n",
      "  (12452, 38446)\t0.0428498935496\n",
      "  (12452, 36926)\t0.0342077174773\n",
      "  (12452, 35690)\t0.0402627124092\n",
      "  (12452, 33898)\t0.063963055679\n",
      "  (12452, 22045)\t0.221856562337\n",
      "  (12452, 22028)\t0.110637013986\n",
      "  (12452, 21767)\t0.0563268241309\n",
      "  (12452, 18529)\t0.0501545164628\n",
      "  (12452, 18505)\t0.0745904885567\n",
      "  (12452, 17933)\t0.0327734929389\n",
      "  (12452, 16719)\t0.0671784333884\n",
      "  (12452, 16541)\t0.0404879839675\n",
      "  (12452, 15929)\t0.059314092321\n",
      "  (12452, 15615)\t0.025171807376\n",
      "  (12452, 15520)\t0.0538450978686\n",
      "  (12452, 13416)\t0.0543064632033\n",
      "  (12452, 1439)\t0.163055672364\n",
      "  (12452, 1283)\t0.0283312901216\n",
      "  (12452, 1267)\t0.0325255517423\n",
      "  (12452, 1246)\t0.151895838882\n",
      "  (12452, 973)\t0.0452302655056\n",
      "features\n",
      "1 !\n",
      "5001 16:12\n",
      "10001 4770r-kone\n",
      "15001 aerosoli|pakkaus\n",
      "20001 ansaitsekkaan\n",
      "25001 bailu\n",
      "30001 counter\n",
      "35001 ellinooraaa\n",
      "40001 eu|ajo|kortti\n",
      "45001 grupperna\n",
      "50001 henkilö|tunniste\n",
      "55001 http://jukkawallin.puheenvuoro.uusisuomi.fi/197028-toistaako-historia-jalleen-itseaan\n",
      "60001 http://yle.fi/uutiset/kiihottavia_pornoloytoja_ei_juuri_somepaivityksissa_jaeta/7851960\n",
      "65001 hyysäämäään\n",
      "70001 inverkar\n",
      "75001 joukko|vaino\n",
      "80001 kaksois|kytkin\n",
      "85001 keksi\n",
      "90001 kissanraksua\n",
      "95001 koukero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2641: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction\n",
    "\n",
    "def tokenizer(txt):\n",
    "    \"\"\"Simple whitespace tokenizer\"\"\"\n",
    "    return txt.split()\n",
    "\n",
    "#Extract the features\n",
    "tfidf_v=sklearn.feature_extraction.text.TfidfVectorizer(tokenizer=tokenizer) #,max_df=0.9\n",
    "d=tfidf_v.fit_transform(texts)\n",
    "print \"documents x features\", d.shape\n",
    "print \"feature matrix\"\n",
    "print d\n",
    "print \"features\"\n",
    "fnames=tfidf_v.get_feature_names()\n",
    "for feature_id in range(1,100000,5000):\n",
    "    print feature_id,fnames[feature_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "* Will be explained during the lecture, Google if you couldn't attend\n",
    "* Key concepts:\n",
    "  - Separating hyperplane\n",
    "  - Margin\n",
    "  - Errors and slack variables\n",
    "  - The parameter C\n",
    "  - Regularization\n",
    "  \n",
    "<img src=\"http://docs.opencv.org/2.4/_images/sample-errors-dist.png\"/>\n",
    "\n",
    "* Multiclass classification = number of classes > 2\n",
    "* One vs all = train a classifier for each class, pick the max score\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "* Will be explained during the lecture, Google key concepts if you couldn't attend\n",
    "* Key concepts:\n",
    "  - Accuracy, Precision, Recall, F-score\n",
    "  - Train / Development / Test Data\n",
    "  - Crossvalidation\n",
    "  - Overfitting\n",
    "  - Parameter optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.010  Accuracy=37.42%\n",
      "C=0.100  Accuracy=59.82%\n",
      "C=1.000  Accuracy=66.68%\n",
      "C=10.000  Accuracy=66.62%\n",
      "C=100.000  Accuracy=65.95%\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "import sklearn.cross_validation\n",
    "X_train,X_test,Y_train,Y_test=sklearn.cross_validation.train_test_split(d, topics, test_size=0.3, random_state=0)\n",
    "\n",
    "for C in (0.01,0.1,1,10,100):\n",
    "    lin_clf = sklearn.svm.LinearSVC(C=C)\n",
    "    lin_clf.fit(X_train,Y_train)\n",
    "    print \"C=%.3f  Accuracy=%.2f%%\"%(C,lin_clf.score(X_test,Y_test)*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...66% is not bad, keeping in mind we have 23 classes to choose from.\n",
    "\n",
    "# Random baseline\n",
    "\n",
    "* That we have 23 classes doesn't mean our baseline is 1/23!\n",
    "* Class imbalance\n",
    "* Accuracy susceptible to this!\n",
    "\n",
    "How do we fare compared to making random choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier predicting most frequent class: 28.88%\n",
      "Dummy classifier predicting at random by class dist.: 13.01%\n"
     ]
    }
   ],
   "source": [
    "import sklearn.dummy\n",
    "dummy=sklearn.dummy.DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train,Y_train)\n",
    "print \"Dummy classifier predicting most frequent class: %.2f%%\"%(dummy.score(X_test,Y_test)*100.0)\n",
    "dummy=sklearn.dummy.DummyClassifier(strategy=\"stratified\")\n",
    "dummy.fit(X_train,Y_train)\n",
    "print \"Dummy classifier predicting at random by class dist.: %.2f%%\"%(dummy.score(X_test,Y_test)*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if you predict the most frequent class, you get to 28% accuracy and with the simple SVM we get 66% accuracy. I.e we can safely say the classifier is learning something. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-grams\n",
    "\n",
    "* Quite popular choice\n",
    "* Does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents x features (12453, 229764)\n",
      "feature matrix\n",
      "  (0, 327495)\t0.0279364084575\n",
      "  (0, 327494)\t0.0246940006294\n",
      "  (0, 326082)\t0.0284459123917\n",
      "  (0, 326081)\t0.0195666672323\n",
      "  (0, 325561)\t0.0570182735877\n",
      "  (0, 325556)\t0.0397059026255\n",
      "  (0, 323870)\t0.0145962063742\n",
      "  (0, 323857)\t0.0141140700881\n",
      "  (0, 323698)\t0.0329225192112\n",
      "  (0, 323691)\t0.0325440317323\n",
      "  (0, 323579)\t0.013446493561\n",
      "  (0, 323569)\t0.00752836225688\n",
      "  (0, 323164)\t0.0143551104277\n",
      "  (0, 323159)\t0.0139049754487\n",
      "  (0, 322949)\t0.0260674398704\n",
      "  (0, 322948)\t0.0252456237298\n",
      "  (0, 322845)\t0.0755417813006\n",
      "  (0, 322843)\t0.0727451050801\n",
      "  (0, 322632)\t0.0144044808613\n",
      "  (0, 322621)\t0.0124514648769\n",
      "  (0, 322401)\t0.0172161118601\n",
      "  (0, 322400)\t0.0151691640639\n",
      "  (0, 322151)\t0.0413451316873\n",
      "  (0, 322141)\t0.0229695854203\n",
      "  (0, 321922)\t0.0415038550878\n",
      "  :\t:\n",
      "  (12452, 7121)\t0.012006175183\n",
      "  (12452, 7119)\t0.0228802062083\n",
      "  (12452, 7107)\t0.0191241708685\n",
      "  (12452, 7104)\t0.00912417192638\n",
      "  (12452, 7100)\t0.0192372220089\n",
      "  (12452, 7093)\t0.0294206477113\n",
      "  (12452, 6326)\t0.0169746067645\n",
      "  (12452, 6325)\t0.0132810244705\n",
      "  (12452, 6323)\t0.0135998608957\n",
      "  (12452, 6281)\t0.0216733813397\n",
      "  (12452, 1349)\t0.0633947077443\n",
      "  (12452, 1348)\t0.0606419300251\n",
      "  (12452, 1307)\t0.0101769197325\n",
      "  (12452, 1304)\t0.0127530473273\n",
      "  (12452, 1303)\t0.0182593358082\n",
      "  (12452, 1216)\t0.0191201747253\n",
      "  (12452, 1215)\t0.0154287139419\n",
      "  (12452, 1214)\t0.0241872736056\n",
      "  (12452, 1207)\t0.0239623912573\n",
      "  (12452, 1174)\t0.0168917636929\n",
      "  (12452, 1163)\t0.0550608416555\n",
      "  (12452, 899)\t0.00946902812298\n",
      "  (12452, 893)\t0.00828576678397\n",
      "  (12452, 881)\t0.0133338344521\n",
      "  (12452, 850)\t0.0174998320168\n",
      "features\n",
      "1 \u0005 ( \n",
      "5001  70e\n",
      "10001  hah\n",
      "15001  sil\n",
      "20001  ▪ h\n",
      "25001 *k1\n",
      "30001 -31_\n",
      "35001 -zi-\n",
      "40001 .vul\n",
      "45001 /non\n",
      "50001 05/\n",
      "55001 1093\n",
      "60001 1–45\n",
      "65001 2cb\n",
      "70001 39-\n",
      "75001 4844\n",
      "80001 564b\n",
      "85001 6767\n",
      "90001 7_r\n",
      "95001 8p v\n"
     ]
    }
   ],
   "source": [
    "tfidf_v_char=sklearn.feature_extraction.text.TfidfVectorizer(analyzer='char',ngram_range=(3,4)) #,max_df=0.9\n",
    "d_char=tfidf_v_char.fit_transform(texts)\n",
    "print \"documents x features\", d.shape\n",
    "print \"feature matrix\"\n",
    "print d_char\n",
    "print \"features\"\n",
    "fnames=tfidf_v_char.get_feature_names()\n",
    "for feature_id in range(1,100000,5000):\n",
    "    print feature_id,fnames[feature_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.010  Accuracy=43.98%\n",
      "C=0.100  Accuracy=63.30%\n",
      "C=1.000  Accuracy=68.55%\n"
     ]
    }
   ],
   "source": [
    "X_train_char,X_test_char,Y_train_char,Y_test_char=\\\n",
    "    sklearn.cross_validation.train_test_split(d_char, topics, test_size=0.3, random_state=0)\n",
    "\n",
    "for C in (0.01,0.1,1):\n",
    "    lin_clf_char = sklearn.svm.LinearSVC(C=C)\n",
    "    lin_clf_char.fit(X_train_char,Y_train_char)\n",
    "    print \"C=%.3f  Accuracy=%.2f%%\"%(C,lin_clf_char.score(X_test_char,Y_test_char)*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget about words and you'll get better numbers! Cool, eh? :)\n",
    "\n",
    "Does this generalize? Let's run on Finnish tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I gathered a bunch of totally random Finnish tweets, will my model work?\n",
    "import json\n",
    "\n",
    "tweets=[]\n",
    "with open(\"fin_tweets.json\",\"r\") as f:\n",
    "    for lineno,line in enumerate(f):\n",
    "        line=line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            tweet=json.loads(line)\n",
    "        except ValueError: #some of these are broken\n",
    "            continue\n",
    "        tweets.append(tweet[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(886, 336566)\n",
      "Yhteiskunta  ---  RT @zeekends: wcw babe ; isha asli sofia rye vanessa 👅\n",
      "Yhteiskunta  ---  DaanLuyten #TilItHappensToYou #BestMovieSong #iHeartAwards\n",
      "Yhteiskunta  ---  Mulla on kangasväriä farkuissa rip 😢😢 https://t.co/67iudMBfuj\n",
      "Yhteiskunta  ---  @MaayronFerreira IJAEJIOEJIOEAJOIEAJI\n",
      "Yhteiskunta  ---  #NowPlaying BFC-radio (@BFC_radio) https://t.co/xYZVlndWyG … #Erdioo\n",
      "Yhteiskunta  ---  omfg esQUEJ MEESTOYJ\n",
      "Yhteiskunta  ---  Meikä oli jo hetken pitkäperjantaissa. Ja nyt on vasta kiiraskeskiviikko. Päivät sekoo kun on näitä pyhiä.\n",
      "Yhteiskunta  ---  [22:59:10] 118.113.52.162:4384 &gt;&gt; :1433 (TCP:SYN)\n",
      "Työ ja opiskelu  ---  Oho tukkani on ekaa kertaa vuosiin mitassa jossa se alkaa aaltoilla ellen kampaa sitä suoraksi suihkun jälkeen. Hassua.\n",
      "Yhteiskunta  ---  #vibrator adulttoys #sextoys https://t.co/dk83khR6lo\n",
      "Yhteiskunta  ---  Tänään osui Hip Hop ja Rap YouTube Video Suomessa.「Cheek」's 『Kuka Muu Muka』 https://t.co/pQQ8kJTWiL\n",
      "Yhteiskunta  ---  [23:00:26] 125.123.234.198:2980 &gt;&gt; :1433 (TCP:SYN)\n",
      "Yhteiskunta  ---  #fucking adulttoys #vibrator https://t.co/fixAjHsUTT\n",
      "Viihde ja kulttuuri  ---  #Lowongan kerja Sales Finance Business Partner - Mondelēz International Jakarta https://t.co/VBxzdYnJCN #loker lowker\n",
      "Yhteiskunta  ---  Tykkäsin @YouTube-videosta, jonka teki @pixeldan https://t.co/IIJcOiWOpu Batman v Superman Massive Mystery Box Unboxing! - Lego,\n",
      "Yhteiskunta  ---  @nicolettching @Mdcarigaba @Espanto2001 zxcvbnm kyaaaah 😍😂\n",
      "Ajanviete  ---  Uva passa https://t.co/vbn2dUmGw1\n",
      "Yhteiskunta  ---  #fucking adulttoys #sextoys https://t.co/W2O7prQfli\n",
      "Yhteiskunta  ---  RT @imazlum: Dr. Itır Toksöz presenting us on Peace Studies, Syria and Refugees #wednesdaytalks @itirtoksoz @MURCIR https://t.co/VZem98vrwo\n",
      "Yhteiskunta  ---  #sextoys adulttoys #masturbating https://t.co/DDPzcocs8P\n",
      "Yhteiskunta  ---  #fucking adulttoys #analdildo https://t.co/59lnPa0FDF\n",
      "Paikkakunnat  ---  Suomessa henkilöstöjohtajat junnaavat yhä tehokkuudessa, kun muualla sitouttaminen on tärkeää\n",
      "https://t.co/TF0OSuLaGD\n",
      "Yhteiskunta  ---  Seiyū : Kanako Mitsuhashi (1999)\n",
      "Yhteiskunta  ---  RT @Partiokuuluu: Monikulttuurisen rekrytoinnin tärkein sana on tervetuloa! https://t.co/OsDf9IhaUG #partioscout #rasisminvastainenviikko #…\n",
      "Yhteiskunta  ---  TPS:n päävalmentaja Mika Laurikainen on nimennyt kokoonpanon illan FC Honka-TPS-otteluun #FCTPS #SuomenCup #Ykkönen https://t.co/hACuGsdWh2\n",
      "Yhteiskunta  ---  RT @KiipulaAO: Ajankohtainen julkaisu, ammattitaitomaajoukkueemme juuri kisojen kynnyksellä! #Abilympics #ammatillinenkoulutus https://t.co…\n",
      "Yhteiskunta  ---  @AjatustenVanki en oo vege. Kanasalaatti on hyvää x) enkä syö muutenkaan kuin kanahamppareita xD\n",
      "Yhteiskunta  ---  RT @prillvers_kudus: @Prillverskubdg_ amiin YRA\n",
      "#CantWaitAnnivPV5th\n",
      "Yhteiskunta  ---  RT @KouvolanSanomat: Suomen ensimmäinen Youtube -opintojakso Kaakkois-Suomen ammattikorkeakouluun — opettajina maan suosituimmat… https://t…\n",
      "Yhteiskunta  ---  Mikkeli: pahoinpitely Länsi-Savo 23.3.2016 15:54 18-vuotias nainen puukotti samanikäis.. #Mikkeli https://t.co/Op9cRbdPCd\n",
      "Yhteiskunta  ---  @kenekk0317 lhmtalrtjstytyftyudrstysryfyjdsfgudtgyhimfti,yhimjnfyundrtmdrtujndrtrdtmutymdrthnrtyndrturydtydhkgfyildtysrtsyssryisrys\n",
      "Yhteiskunta  ---  Kirjoitus antoi kuvan, että miehille keskivartalolihavuus on epäterveellistä, naisille \"ei kivan näköistä\". Yllättikö neg. palaute? @ksmlfi\n",
      "Yhteiskunta  ---  LIVE-lähetys #Periscope-sovelluksessa: jooo laulua niin o https://t.co/WXgGHgrAX3\n",
      "Yhteiskunta  ---  11:11 wigetta\n",
      "Yhteiskunta  ---  @JuusoQ Ranskanbulldoggi-mittelspitz, on söpö!\n",
      "Suhteet  ---  RT @Nduweybadass: Which Rihanna? Rihanna Rihanna or Rihanna Mkhwanazi https://t.co/XghUBGt007\n",
      "Yhteiskunta  ---  @BTS_twt jin~san kakkoi desu  ^^\n",
      "Yhteiskunta  ---  RT @MitasanSharp: MX-3070N / MX-3570N / MX-4070N (Phoenix)\n",
      " \n",
      "Gelişmiş CR4 teknolojisiyle Yeni Sharp renkli ürünleri… https://t.co/XwgZz32DY1\n",
      "Yhteiskunta  ---  RT @lehtinen_esa: Suomessa HR-johto keskittyy HR-järjestelmiin, muualla yrityskulttuurin vahvistamiseen ja ihmisten sitouttamiseen.https://…\n",
      "Yhteiskunta  ---  Ruuvi säve! Karjala kadulla Alppilassa. Mitä vaa ruuvei! Sheiiiiiiiit! #studio #construction #prolevel #törkeduni https://t.co/cyXIFSthbC\n",
      "Yhteiskunta  ---  @skewnger hello joo oppa!\n",
      "Yhteiskunta  ---  @TopiYrjl Ei varmasti. Ai että sitä paskamyrskyn määrää. :D\n",
      "Yhteiskunta  ---  RT @kskokoomus: @alexstubb vastaanottaa Saarijärven #kokoomuksen terveiset Harri Lehtiseltä ja Esa Järviseltä. https://t.co/5ir4WfTxjy\n",
      "Yhteiskunta  ---  huurhun shuu https://t.co/xyQU8fmtmM\n",
      "Yhteiskunta  ---  #sextoys adulttoys #cockring https://t.co/6Cgg1eiVMd\n",
      "Yhteiskunta  ---  @kuningaskulutta Olisi ollut turhauttavaa joutua siirtämään päivittäiset raha-asiat lainan vuoksi. Jotenkin tykkään, kun ovat erillään.\n",
      "Yhteiskunta  ---  RT @OutwardBoundFIN: Satavuotiaan Mallan luonnonpuiston uusi tunnus kunnioittaa alueen ainutlaatuista luontoa https://t.co/jT5YMi5VoB https…\n",
      "Yhteiskunta  ---  RT @GreenpeaceSuomi: Myös YK:n ihmisoikeusvaltuutettu liittyi metsähallituslain arvostelijoihin! #metsähallituslaki @UNHumanRights https://…\n",
      "Yhteiskunta  ---  Kuva: Meteoriitti syöksyi Mikkeliin – tulipallo näkyi satojen kilometrien päähän {iltasanomat} https://t.co/ML75s9Uvnx\n",
      "Yhteiskunta  ---  RT @catwomaine: slvsufudjbshuygxv https://t.co/aHkyyGFpSX\n",
      "Yhteiskunta  ---  ZXCVBNM\n"
     ]
    }
   ],
   "source": [
    "d_tweet_char=tfidf_v_char.transform(tweets)\n",
    "print d_tweet_char.shape\n",
    "for counter,(tweet, cls) in enumerate(zip(tweets,lin_clf_char.predict(d_tweet_char))):\n",
    "    print cls, \" --- \", tweet\n",
    "    if counter==50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-O\n",
    "\n",
    "Oh good lord - twitter is such crap! [pulling hair 1AM the night before the lecture] Let's try to apply some of our newly acquired skills to recover. :| How about we try run the tweets through the parser and check the words against the top-most Finnish vocabulary and only keep tweets of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oho tukka olla eka kerta vuosi mitta joka se alkaa aaltoilla josei kammata se suora suihku jälkeen . hassu .\n",
      "@kuningaskulutta olla olla turhauttaa joutua siirtää päivittäinen raha-asia laina vuoksi . jotenkin tykätä , kun olla erillään .\n",
      "@maijalarmo tuoda Felix uusi korkki olla ihan ykkönen\n",
      "ei haluu liikkua , pitää mennä kauppa mut ulkona sata lumi ja mä olla ruokakooma päällä ugh\n",
      "@BornForFiNRS mä ei ärsyttää vielä koska vetää just pussi fanipaloi ja nyt sattua maha lol\n",
      "@RenneKorppila vai sellanen kaveri . mä ei toisaalta mikään ihme ettäei olla koskaan kuulla ko . tyyppi .\n",
      "@Kinukki sanoma olla selvä , että ei uskoa olla väärä kun arvella sinä kertoa tämä itse ,olethan aikuinen ..\n",
      "@Nysses ei . olla ilo huomata että minä @jysk_fi tämä tapahtua päivittäin ja asiakaspalvelu olla kunnia-asia .\n",
      "paitsi ain olla kiva nähä ämmii tappelees mut veikka tämä menoo vappun sata lumi\n",
      "RT @SaaraHuttunen : mä haluta olla terve ja onnellinen . muu prioriteetti mä ei nyt olla . toki koulu ois kiva joskus valmistua\n",
      "\n",
      "\n",
      "Oho tukkani on ekaa kertaa vuosiin mitassa jossa se alkaa aaltoilla ellen kampaa sitä suoraksi suihkun jälkeen . Hassua .\n",
      "@kuningaskulutta Olisi ollut turhauttavaa joutua siirtämään päivittäiset raha-asiat lainan vuoksi . Jotenkin tykkään , kun ovat erillään .\n",
      "@maijalarmo toi Felixin uusi korkki on ihan ykkönen\n",
      "En haluu liikkuu , pitäis mennä kauppaan mut ulkona sataa lunta ja mulla on ruokakooma päällä ugh\n",
      "@BornForFiNRS mua ei ärsytä vielä koska vedin just pussin fanipaloi ja nyt sattuu mahaan lol\n",
      "@RenneKorppila Vai sellanen kaveri . Mut eipä toisaalta mikään ihme etten ole koskaan kuullutkaan ko . tyypistä .\n",
      "@Kinukki Sanomattakin on selvää , että en usko olevani väärässä kun arvelen sinun kertovan tässä itsestäsi ,olethan aikuinen ..\n",
      "@Nysses Eikö . On ilo huomata että meillä @jysk_fi tätä tapahtuu päivittäin ja asiakaspalvelu on kunnia-asia .\n",
      "paitsi ain on kiva nähä ämmii tappelees mut veikkaan tätä menoo vappun sataa lunta\n",
      "RT @SaaraHuttunen : Mä haluan olla terve ja onnellinen . muita prioriteetteja mulla ei nyt ole . Toki koulusta ois kiva joskus valmistua\n"
     ]
    }
   ],
   "source": [
    "import lwvlib\n",
    "wv=lwvlib.load(\"pb34_lemma_200_v2.bin\",70000,70000)\n",
    "\n",
    "def read_conllu(inp):\n",
    "    tweet=[] #list of lemmas\n",
    "    tweet_words=[] #list of words\n",
    "    for line in inp:\n",
    "        line=line.strip().replace(u\"#\",u\"\")\n",
    "        if not line:\n",
    "            yield tweet, tweet_words\n",
    "            tweet=[]\n",
    "            tweet_words=[]\n",
    "        else:\n",
    "            tweet.append(line.split(u\"\\t\")[2])\n",
    "            tweet_words.append(line.split(u\"\\t\")[1])\n",
    "            \n",
    "            \n",
    "import re\n",
    "wrdre=re.compile(u\"^[a-zäöå-]+$\")\n",
    "def known_words(tweet):\n",
    "    return sum(1 for word in tweet if word in wv.words and wrdre.match(word))\n",
    "\n",
    "tweets=[]\n",
    "tweets_words=[]\n",
    "with codecs.open(\"fin_tweets.conllu\",\"r\",\"utf-8\") as f:\n",
    "    for tweet,tweet_words in read_conllu(f):\n",
    "        if float(known_words(tweet))/len(tweet)>0.7:\n",
    "            tweets.append(u\" \".join(tweet).replace(u\"#\",u\"|\"))\n",
    "            tweets_words.append(u\" \".join(tweet_words))\n",
    "            \n",
    "for t in tweets[:10]:\n",
    "    print t\n",
    "print\n",
    "print\n",
    "for t in tweets_words[:10]:\n",
    "    print t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 336566)\n",
      "Koti ja rakentaminen  ---  Oho tukkani on ekaa kertaa vuosiin mitassa jossa se alkaa aaltoilla ellen kampaa sitä suoraksi suihkun jälkeen . Hassua .\n",
      "\n",
      "Yhteiskunta  ---  @kuningaskulutta Olisi ollut turhauttavaa joutua siirtämään päivittäiset raha-asiat lainan vuoksi . Jotenkin tykkään , kun ovat erillään .\n",
      "\n",
      "Paikkakunnat  ---  @maijalarmo toi Felixin uusi korkki on ihan ykkönen\n",
      "\n",
      "Suhteet  ---  En haluu liikkuu , pitäis mennä kauppaan mut ulkona sataa lunta ja mulla on ruokakooma päällä ugh\n",
      "\n",
      "Suhteet  ---  @BornForFiNRS mua ei ärsytä vielä koska vedin just pussin fanipaloi ja nyt sattuu mahaan lol\n",
      "\n",
      "Yhteiskunta  ---  @RenneKorppila Vai sellanen kaveri . Mut eipä toisaalta mikään ihme etten ole koskaan kuullutkaan ko . tyypistä .\n",
      "\n",
      "Suhteet  ---  @Kinukki Sanomattakin on selvää , että en usko olevani väärässä kun arvelen sinun kertovan tässä itsestäsi ,olethan aikuinen ..\n",
      "\n",
      "Yhteiskunta  ---  @Nysses Eikö . On ilo huomata että meillä @jysk_fi tätä tapahtuu päivittäin ja asiakaspalvelu on kunnia-asia .\n",
      "\n",
      "Yhteiskunta  ---  paitsi ain on kiva nähä ämmii tappelees mut veikkaan tätä menoo vappun sataa lunta\n",
      "\n",
      "Nuoret  ---  RT @SaaraHuttunen : Mä haluan olla terve ja onnellinen . muita prioriteetteja mulla ei nyt ole . Toki koulusta ois kiva joskus valmistua\n",
      "\n",
      "Yhteiskunta  ---  @gynsy @MikiHoijer Tarkistin vielä , että kuudelta sen piti alkaa . Kerrankos on kyttiä enemmän kuin osallistujia .\n",
      "\n",
      "Paikkakunnat  ---  Nonii , nyt paikka vähän enemmän täynnä . Hämärästi täyttyy tää näin pienes ajas\n",
      "\n",
      "Yhteiskunta  ---  @MaipuMaire @Mallas6 Hyvä niin . Saaristossa se häijympi kaveri joka levittää häijympää tautia .\n",
      "\n",
      "Urheilu ja kuntoilu  ---  Ja niinhän se on , että kova treeni jo itsessään on stressi ja vie energiaa palautumiseen ja henkinen stressi vielä . Hyötykäyttöön siis !\n",
      "\n",
      "Lemmikit  ---  @maailmanvaltias enkkuu ! jos lykky siis käy ja pääsen ... suuntaan ehkä opettajaks mut en sulje muita vaihtoehtoi pois jos mieli muuttuu !\n",
      "\n",
      "Yhteiskunta  ---  @ManninenJoonas katsottavahko ? varmaan siinä vaiheessa kun ei enää pysy hereillä tai muuten keskittymään elokuvaan mitenkään\n",
      "\n",
      "Paikkakunnat  ---  @yleuutiset Ei ole kun merkki mikä viittaa suomalaisesta astia teollisuudesta . Tämmöistä joskus tehty suomessa\n",
      "\n",
      "Työ ja opiskelu  ---  @emohilkka mut vois myös kirjottaa kaks esseetä ennen ku syke alkaa\n",
      "\n",
      "Yhteiskunta  ---  @Mirppu @Elmatule Eiköhän se tosta aika pian lopu . Vartijat viimeistään puhaltaa pelin poikki .\n",
      "\n",
      "Suhteet  ---  psykologia on niin mielenkiintosta TYKKÄÄN\n",
      "\n",
      "Yhteiskunta  ---  @BLAKKIIIIISSSSS Jos suoraan sanon , on säälittävää jos rupeaa tubettajaksi vain rahan ja maineen takia . Tubenomithautaan\n",
      "\n",
      "Suhteet  ---  en juurikaan kuuntele david bowien musaa mut tälleen pinnallisesti voin sanoa että oli se kyllä perkeleen kaunis äijä\n",
      "\n",
      "Yhteiskunta  ---  Kuka helvetti toi tänne kämppään suklaata ? Terveisin possu .\n",
      "\n",
      "Yhteiskunta  ---  Oon melko varma , että joku mun naapuri ulkoilutti kissaa , kun tulin töistä kotiin . Hämmennyin . Ulkona on pakkasta .\n",
      "\n",
      "Paikkakunnat  ---  nonii nyt on eve aino ja elina tehny jo konseptit kai sitä itekki piakkoin vois tiedän että ne kyllä ärsyttää monia Anteeksi etukäteen : )\n",
      "\n",
      "Yhteiskunta  ---  @heikki_hakala Olen edelleen hämilläni ja kummissani Moskovan matkan tuloksista . Että mitä ne oikein ovat siis .\n",
      "\n",
      "Paikkakunnat  ---  @wwimpula vois vaan alkaa hoitaa omii asioit puhelimessa tolleen 😂? ?\n",
      "\n",
      "Paikkakunnat  ---  Se on siinä vaiheessa menetetty tapaus ku pizzeriassa osataan tilauksesi ..\n",
      "\n",
      "Suhteet  ---  @montuttaja @mitavittualehti Nyt on viety valkoinen itseinho ihan uudelle tasolle WhiteGenocideIsReal\n",
      "\n",
      "Urheilu ja kuntoilu  ---  @LottaEmpi mun salisuunnitelmat meni muiden suunnitelmien kanssa ketjussa plörinäksi , huomenna treeniä taas salilla !\n",
      "\n",
      "Paikkakunnat  ---  Tää on sellanen cannot unsee tilanne itq\n",
      "\n",
      "Yhteiskunta  ---  Onneksi on internet ja urpoja joihin purkaa oma vitutuksensa .\n",
      "\n",
      "Yhteiskunta  ---  On tuo syke vaan niin mukiinmenevä sarja näin keskiviikkoiltana 👌\n",
      "\n",
      "Suhteet  ---  @dragmenialI ÄITI JA ISKÄ MÄ ELÄN OOTTE NIIN IHANIA\n",
      "\n",
      "Suhteet  ---  Nyt kun oon täysikänen ni mut pistetään aina maistelee alkoholijuomii . Tänään testissä vadelmaskumppa - kaikki nauro ku irvistelin vaan :(\n",
      "\n",
      "Paikkakunnat  ---  Voi kyl olla et ite jännitän enemmän ku poikkis mut onhan se iso juttu esittäytyy koko suvulle ! Etenkin ku ollaan muuttamas yhteen ihan pian\n",
      "\n",
      "Yhteiskunta  ---  Tää TIsuomi olis pitäny tulla lääppijä kohun aikaan . 😃 siinä olis ollu helvetti irti\n",
      "\n",
      "Yhteiskunta  ---  Haluun ton nyt pelkästää jo esteettisistä syistä , herra varjele\n",
      "\n",
      "Yhteiskunta  ---  @maaaw Joku vanki on varmaan taas unohtunut palata lomiltansa\n",
      "\n",
      "Paikkakunnat  ---  @dragmenialI SÄ OOT JA KATO NYT TOTA TÄYDELLINEN KONSEPTI\n",
      "\n",
      "Yhteiskunta  ---  @suski_kaukinen @laurahaimila Tuo oli hölmöä jopa sinun näppäimistöltäsi . Rasismi tässä ketjussa on kaikki puoleltasi\n",
      "\n",
      "Yhteiskunta  ---  Hetken ihmettelin ettei yksikään ottelu ole mennyt vielä jatkorei 'ille , mut tän uuden formaatin mukaan voikin päättyä tasan . Njäh..\n",
      "\n",
      "Yhteiskunta  ---  VAARALLISIN\n",
      "\n",
      "Suhteet  ---  Ai vittu . Tajusin just et mä en voi jatkaa ku tajusin saatan spoilaa . Sori .\n",
      "\n",
      "Viihde ja kulttuuri  ---  Pitkä kausi takana . Upeita hetkiä ja myös pieni pettymyksen tunne . Tästä kaivetaan voimaa ja palataan vahvempana ensi kauteen ! Ketterä\n",
      "\n",
      "Suhteet  ---  tai no , ymmärrän jos siinä on kuvia jotka vievät koko aukeaman ... mutta kuitenkin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_tweet_char=tfidf_v_char.transform(tweets)\n",
    "print d_tweet_char.shape\n",
    "for counter,(tweet, tweet_words, cls) in enumerate(zip(tweets,tweets_words,lin_clf_char.predict(d_tweet_char))):\n",
    "    print cls, \" --- \", tweet_words\n",
    "    print\n",
    "    if counter==50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And how about the vectors, do they help any?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10225586,  0.08287574,  0.04128639, ...,  0.01366414,\n",
       "         0.14373324,  0.17274647],\n",
       "       [-0.05364433, -0.01276515,  0.0603284 , ...,  0.00620707,\n",
       "         0.19762445,  0.13600904],\n",
       "       [-0.06920946,  0.01818488,  0.03807921, ...,  0.02176174,\n",
       "         0.13039736,  0.17019013],\n",
       "       ..., \n",
       "       [-0.07167699,  0.06385985,  0.05177857, ..., -0.00953654,\n",
       "         0.17387475,  0.13991461],\n",
       "       [-0.06337436,  0.04784775,  0.04624651, ...,  0.03932344,\n",
       "         0.09021433,  0.12855842],\n",
       "       [-0.10096667,  0.06961406,  0.01622496, ..., -0.01524248,\n",
       "         0.14144494,  0.164709  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lwvlib\n",
    "import numpy\n",
    "wv=lwvlib.load(\"pb34_lemma_200_v2.bin\",50000,50000)\n",
    "\n",
    "def doc2vec(txt,wv,i,data_matrix):\n",
    "    \"\"\"Text with whitespace tokenization\n",
    "    wv\n",
    "    i - which row are we filling\n",
    "    data_matrix - and to where?\"\"\"\n",
    "    for w in txt.split():\n",
    "        w=w.lower()\n",
    "        dim=wv.get(w)\n",
    "        if dim==None:\n",
    "            continue\n",
    "        data_matrix[i]+=wv.vectors[dim]\n",
    "\n",
    "#topics,texts\n",
    "data_matrix=numpy.zeros((len(texts),wv.vectors.shape[1]))\n",
    "for i,txt in enumerate(texts):\n",
    "    doc2vec(txt,wv,i,data_matrix)\n",
    "sklearn.preprocessing.normalize(data_matrix,copy=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls=sklearn.svm.LinearSVC(C=1.0)\n",
    "cls.fit(data_matrix[:10000],topics[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59315124337545866"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.score(data_matrix[10000:],topics[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done.\n",
    "\n",
    "Below is a try with multilayer neural network. I won't go into this during the lecture, but leave it here for those who might be interested to check out the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 23\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.6256 - acc: 0.2446 - val_loss: 2.3121 - val_acc: 0.3460\n",
      "Epoch 2/100\n",
      "0s - loss: 2.3534 - acc: 0.3229 - val_loss: 2.1510 - val_acc: 0.3623\n",
      "Epoch 3/100\n",
      "0s - loss: 2.1948 - acc: 0.3580 - val_loss: 2.0203 - val_acc: 0.4263\n",
      "Epoch 4/100\n",
      "0s - loss: 2.0739 - acc: 0.3984 - val_loss: 1.9285 - val_acc: 0.4443\n",
      "Epoch 5/100\n",
      "0s - loss: 1.9835 - acc: 0.4297 - val_loss: 1.8644 - val_acc: 0.4783\n",
      "Epoch 6/100\n",
      "0s - loss: 1.9087 - acc: 0.4584 - val_loss: 1.8500 - val_acc: 0.4673\n",
      "Epoch 7/100\n",
      "0s - loss: 1.8540 - acc: 0.4679 - val_loss: 1.7724 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "0s - loss: 1.8137 - acc: 0.4786 - val_loss: 1.7483 - val_acc: 0.5110\n",
      "Epoch 9/100\n",
      "0s - loss: 1.7763 - acc: 0.4904 - val_loss: 1.7529 - val_acc: 0.5050\n",
      "Epoch 10/100\n",
      "0s - loss: 1.7434 - acc: 0.4894 - val_loss: 1.7137 - val_acc: 0.5137\n",
      "Epoch 11/100\n",
      "0s - loss: 1.7174 - acc: 0.5007 - val_loss: 1.6900 - val_acc: 0.5157\n",
      "Epoch 12/100\n",
      "0s - loss: 1.6938 - acc: 0.5061 - val_loss: 1.7008 - val_acc: 0.5173\n",
      "Epoch 13/100\n",
      "0s - loss: 1.6705 - acc: 0.5099 - val_loss: 1.6553 - val_acc: 0.5313\n",
      "Epoch 14/100\n",
      "0s - loss: 1.6471 - acc: 0.5193 - val_loss: 1.6372 - val_acc: 0.5360\n",
      "Epoch 15/100\n",
      "0s - loss: 1.6302 - acc: 0.5266 - val_loss: 1.6178 - val_acc: 0.5367\n",
      "Epoch 16/100\n",
      "0s - loss: 1.6196 - acc: 0.5250 - val_loss: 1.6064 - val_acc: 0.5510\n",
      "Epoch 17/100\n",
      "0s - loss: 1.5959 - acc: 0.5347 - val_loss: 1.6080 - val_acc: 0.5433\n",
      "Epoch 18/100\n",
      "0s - loss: 1.5795 - acc: 0.5420 - val_loss: 1.5812 - val_acc: 0.5580\n",
      "Epoch 19/100\n",
      "0s - loss: 1.5523 - acc: 0.5494 - val_loss: 1.6282 - val_acc: 0.5287\n",
      "Epoch 20/100\n",
      "0s - loss: 1.5447 - acc: 0.5520 - val_loss: 1.5653 - val_acc: 0.5590\n",
      "Epoch 21/100\n",
      "0s - loss: 1.5260 - acc: 0.5570 - val_loss: 1.5726 - val_acc: 0.5600\n",
      "Epoch 22/100\n",
      "0s - loss: 1.5186 - acc: 0.5589 - val_loss: 1.5600 - val_acc: 0.5627\n",
      "Epoch 23/100\n",
      "0s - loss: 1.5133 - acc: 0.5626 - val_loss: 1.5596 - val_acc: 0.5613\n",
      "Epoch 24/100\n",
      "0s - loss: 1.4998 - acc: 0.5656 - val_loss: 1.5351 - val_acc: 0.5737\n",
      "Epoch 25/100\n",
      "0s - loss: 1.5028 - acc: 0.5626 - val_loss: 1.5214 - val_acc: 0.5737\n",
      "Epoch 26/100\n",
      "0s - loss: 1.4859 - acc: 0.5670 - val_loss: 1.5369 - val_acc: 0.5730\n",
      "Epoch 27/100\n",
      "0s - loss: 1.4837 - acc: 0.5700 - val_loss: 1.5138 - val_acc: 0.5783\n",
      "Epoch 28/100\n",
      "0s - loss: 1.4720 - acc: 0.5757 - val_loss: 1.5225 - val_acc: 0.5773\n",
      "Epoch 29/100\n",
      "0s - loss: 1.4598 - acc: 0.5756 - val_loss: 1.5399 - val_acc: 0.5690\n",
      "Epoch 30/100\n",
      "0s - loss: 1.4522 - acc: 0.5726 - val_loss: 1.5094 - val_acc: 0.5803\n",
      "Epoch 31/100\n",
      "0s - loss: 1.4427 - acc: 0.5837 - val_loss: 1.5136 - val_acc: 0.5753\n",
      "Epoch 32/100\n",
      "0s - loss: 1.4389 - acc: 0.5800 - val_loss: 1.5140 - val_acc: 0.5887\n",
      "Epoch 33/100\n",
      "0s - loss: 1.4246 - acc: 0.5837 - val_loss: 1.5581 - val_acc: 0.5553\n",
      "Epoch 34/100\n",
      "0s - loss: 1.4419 - acc: 0.5771 - val_loss: 1.5260 - val_acc: 0.5783\n",
      "Epoch 35/100\n",
      "0s - loss: 1.4303 - acc: 0.5816 - val_loss: 1.5276 - val_acc: 0.5767\n",
      "Epoch 36/100\n",
      "0s - loss: 1.4193 - acc: 0.5816 - val_loss: 1.5069 - val_acc: 0.5767\n",
      "Epoch 37/100\n",
      "0s - loss: 1.4106 - acc: 0.5903 - val_loss: 1.4945 - val_acc: 0.5897\n",
      "Epoch 38/100\n",
      "0s - loss: 1.4114 - acc: 0.5821 - val_loss: 1.5115 - val_acc: 0.5810\n",
      "Epoch 39/100\n",
      "0s - loss: 1.4094 - acc: 0.5890 - val_loss: 1.5004 - val_acc: 0.5860\n",
      "Epoch 40/100\n",
      "0s - loss: 1.4117 - acc: 0.5850 - val_loss: 1.5551 - val_acc: 0.5747\n",
      "Epoch 41/100\n",
      "0s - loss: 1.4001 - acc: 0.5897 - val_loss: 1.5033 - val_acc: 0.5843\n",
      "Epoch 42/100\n",
      "0s - loss: 1.3960 - acc: 0.5857 - val_loss: 1.5185 - val_acc: 0.5813\n",
      "Epoch 43/100\n",
      "0s - loss: 1.3919 - acc: 0.5907 - val_loss: 1.5479 - val_acc: 0.5547\n",
      "Epoch 44/100\n",
      "0s - loss: 1.3833 - acc: 0.5939 - val_loss: 1.5531 - val_acc: 0.5693\n",
      "Epoch 45/100\n",
      "0s - loss: 1.3740 - acc: 0.5930 - val_loss: 1.4925 - val_acc: 0.5900\n",
      "Epoch 46/100\n",
      "0s - loss: 1.3813 - acc: 0.5960 - val_loss: 1.4962 - val_acc: 0.5873\n",
      "Epoch 47/100\n",
      "0s - loss: 1.3788 - acc: 0.5997 - val_loss: 1.5154 - val_acc: 0.5790\n",
      "Epoch 48/100\n",
      "0s - loss: 1.3751 - acc: 0.5984 - val_loss: 1.4983 - val_acc: 0.5850\n",
      "Epoch 49/100\n",
      "0s - loss: 1.3801 - acc: 0.5959 - val_loss: 1.5082 - val_acc: 0.5857\n",
      "Epoch 50/100\n",
      "0s - loss: 1.3614 - acc: 0.5966 - val_loss: 1.5036 - val_acc: 0.5820\n",
      "Epoch 51/100\n",
      "0s - loss: 1.3596 - acc: 0.6017 - val_loss: 1.4915 - val_acc: 0.5913\n",
      "Epoch 52/100\n",
      "0s - loss: 1.3609 - acc: 0.5971 - val_loss: 1.5064 - val_acc: 0.5867\n",
      "Epoch 53/100\n",
      "0s - loss: 1.3573 - acc: 0.5999 - val_loss: 1.5090 - val_acc: 0.5770\n",
      "Epoch 54/100\n",
      "0s - loss: 1.3509 - acc: 0.6011 - val_loss: 1.4949 - val_acc: 0.5927\n",
      "Epoch 55/100\n",
      "0s - loss: 1.3537 - acc: 0.6047 - val_loss: 1.4949 - val_acc: 0.5887\n",
      "Epoch 56/100\n",
      "0s - loss: 1.3396 - acc: 0.6091 - val_loss: 1.4953 - val_acc: 0.5843\n",
      "Epoch 57/100\n",
      "0s - loss: 1.3388 - acc: 0.6077 - val_loss: 1.5272 - val_acc: 0.5733\n",
      "Epoch 58/100\n",
      "0s - loss: 1.3475 - acc: 0.6036 - val_loss: 1.5136 - val_acc: 0.5813\n",
      "Epoch 59/100\n",
      "0s - loss: 1.3439 - acc: 0.6094 - val_loss: 1.4985 - val_acc: 0.5877\n",
      "Epoch 60/100\n",
      "0s - loss: 1.3462 - acc: 0.6064 - val_loss: 1.5260 - val_acc: 0.5757\n",
      "Epoch 61/100\n",
      "0s - loss: 1.3311 - acc: 0.6051 - val_loss: 1.5134 - val_acc: 0.5780\n",
      "Epoch 62/100\n",
      "0s - loss: 1.3251 - acc: 0.6080 - val_loss: 1.5172 - val_acc: 0.5777\n",
      "Epoch 63/100\n",
      "0s - loss: 1.3261 - acc: 0.6096 - val_loss: 1.5419 - val_acc: 0.5750\n",
      "Epoch 64/100\n",
      "0s - loss: 1.3260 - acc: 0.6133 - val_loss: 1.5002 - val_acc: 0.5850\n",
      "Epoch 65/100\n",
      "0s - loss: 1.3256 - acc: 0.6034 - val_loss: 1.5071 - val_acc: 0.5813\n",
      "Epoch 66/100\n",
      "0s - loss: 1.3245 - acc: 0.6114 - val_loss: 1.5165 - val_acc: 0.5830\n",
      "Epoch 67/100\n",
      "0s - loss: 1.3220 - acc: 0.6114 - val_loss: 1.5059 - val_acc: 0.5803\n",
      "Epoch 68/100\n",
      "0s - loss: 1.3140 - acc: 0.6071 - val_loss: 1.5203 - val_acc: 0.5837\n",
      "Epoch 69/100\n",
      "0s - loss: 1.3061 - acc: 0.6161 - val_loss: 1.4995 - val_acc: 0.5877\n",
      "Epoch 70/100\n",
      "0s - loss: 1.3021 - acc: 0.6146 - val_loss: 1.5347 - val_acc: 0.5750\n",
      "Epoch 71/100\n",
      "0s - loss: 1.3144 - acc: 0.6101 - val_loss: 1.4958 - val_acc: 0.5957\n",
      "Epoch 72/100\n",
      "0s - loss: 1.3095 - acc: 0.6107 - val_loss: 1.5053 - val_acc: 0.5843\n",
      "Epoch 73/100\n",
      "0s - loss: 1.3104 - acc: 0.6166 - val_loss: 1.5255 - val_acc: 0.5780\n",
      "Epoch 74/100\n",
      "0s - loss: 1.3130 - acc: 0.6147 - val_loss: 1.5061 - val_acc: 0.5910\n",
      "Epoch 75/100\n",
      "0s - loss: 1.2970 - acc: 0.6219 - val_loss: 1.5097 - val_acc: 0.5837\n",
      "Epoch 76/100\n",
      "0s - loss: 1.2990 - acc: 0.6134 - val_loss: 1.5172 - val_acc: 0.5773\n",
      "Epoch 77/100\n",
      "0s - loss: 1.3083 - acc: 0.6119 - val_loss: 1.5035 - val_acc: 0.5873\n",
      "Epoch 78/100\n",
      "0s - loss: 1.3085 - acc: 0.6150 - val_loss: 1.5205 - val_acc: 0.5720\n",
      "Epoch 79/100\n",
      "0s - loss: 1.2990 - acc: 0.6140 - val_loss: 1.5246 - val_acc: 0.5797\n",
      "Epoch 80/100\n",
      "0s - loss: 1.2943 - acc: 0.6191 - val_loss: 1.5022 - val_acc: 0.5883\n",
      "Epoch 81/100\n",
      "0s - loss: 1.3001 - acc: 0.6121 - val_loss: 1.5018 - val_acc: 0.5873\n",
      "Epoch 82/100\n",
      "0s - loss: 1.2965 - acc: 0.6160 - val_loss: 1.5085 - val_acc: 0.5883\n",
      "Epoch 83/100\n",
      "0s - loss: 1.2973 - acc: 0.6133 - val_loss: 1.5124 - val_acc: 0.5907\n",
      "Epoch 84/100\n",
      "0s - loss: 1.3013 - acc: 0.6111 - val_loss: 1.5084 - val_acc: 0.5930\n",
      "Epoch 85/100\n",
      "0s - loss: 1.2881 - acc: 0.6253 - val_loss: 1.5353 - val_acc: 0.5817\n",
      "Epoch 86/100\n",
      "0s - loss: 1.2960 - acc: 0.6163 - val_loss: 1.5156 - val_acc: 0.5863\n",
      "Epoch 87/100\n",
      "0s - loss: 1.2851 - acc: 0.6206 - val_loss: 1.5178 - val_acc: 0.5890\n",
      "Epoch 88/100\n",
      "0s - loss: 1.2791 - acc: 0.6204 - val_loss: 1.5148 - val_acc: 0.5850\n",
      "Epoch 89/100\n",
      "0s - loss: 1.2832 - acc: 0.6189 - val_loss: 1.5549 - val_acc: 0.5640\n",
      "Epoch 90/100\n",
      "0s - loss: 1.2866 - acc: 0.6204 - val_loss: 1.5199 - val_acc: 0.5870\n",
      "Epoch 91/100\n",
      "0s - loss: 1.2853 - acc: 0.6216 - val_loss: 1.5179 - val_acc: 0.5790\n",
      "Epoch 92/100\n",
      "0s - loss: 1.2812 - acc: 0.6173 - val_loss: 1.5298 - val_acc: 0.5817\n",
      "Epoch 93/100\n",
      "0s - loss: 1.2878 - acc: 0.6180 - val_loss: 1.5179 - val_acc: 0.5823\n",
      "Epoch 94/100\n",
      "0s - loss: 1.2836 - acc: 0.6109 - val_loss: 1.5149 - val_acc: 0.5870\n",
      "Epoch 95/100\n",
      "0s - loss: 1.2744 - acc: 0.6217 - val_loss: 1.5460 - val_acc: 0.5743\n",
      "Epoch 96/100\n",
      "0s - loss: 1.2722 - acc: 0.6193 - val_loss: 1.5140 - val_acc: 0.5887\n",
      "Epoch 97/100\n",
      "0s - loss: 1.2828 - acc: 0.6164 - val_loss: 1.4998 - val_acc: 0.5927\n",
      "Epoch 98/100\n",
      "0s - loss: 1.2652 - acc: 0.6200 - val_loss: 1.5213 - val_acc: 0.5897\n",
      "Epoch 99/100\n",
      "0s - loss: 1.2837 - acc: 0.6197 - val_loss: 1.5165 - val_acc: 0.5853\n",
      "Epoch 100/100\n",
      "0s - loss: 1.2677 - acc: 0.6239 - val_loss: 1.5176 - val_acc: 0.5873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ce9d10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe we could try with some nonlinear stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import keras.optimizers\n",
    "import keras.utils.np_utils\n",
    "\n",
    "def class2id(topics):\n",
    "    d={}\n",
    "    nums=[]\n",
    "    for t in topics:\n",
    "        nums.append(d.setdefault(t,len(d)))\n",
    "    return nums,d\n",
    "\n",
    "topic_numbers,class_dict=class2id(topics)\n",
    "topic_numbers_matrix=keras.utils.np_utils.to_categorical(topic_numbers)\n",
    "dim_in,dim_internal,dim_out=data_matrix.shape[1],200,len(class_dict)\n",
    "\n",
    "print dim_in, dim_internal,dim_out\n",
    "\n",
    "#Neural network:\n",
    "model = Sequential()\n",
    "#Non-linear layer #1\n",
    "model.add(Dense(dim_internal, input_dim=dim_in))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(dim_internal))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "#Linear projection at the end\n",
    "model.add(Dense(dim_out))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd,class_mode='categorical')\n",
    "#Learn!\n",
    "model.fit(data_matrix[:10000],topic_numbers_matrix[:10000],verbose=2,batch_size=200,show_accuracy=True,validation_split=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2453/2453 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58499796167957607"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \n",
    "import sklearn.metrics\n",
    "\n",
    "sklearn.metrics.accuracy_score(topic_numbers[10000:],model.predict_classes(data_matrix[10000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...no luck. :) This time the good old bag of character n-grams beats us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
