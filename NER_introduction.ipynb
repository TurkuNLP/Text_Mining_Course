{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "NER-introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9S6DpILi_rL"
      },
      "source": [
        "# Information extraction\n",
        "\n",
        "**Previously**:\n",
        "\n",
        "* Text sources\n",
        "* Information retrieval (text search)\n",
        "\n",
        "Both information retrieval and information extraction aim to respond to _information needs_ on the basis of a (typically large) collection of texts. However, there are notable differences between the approaches (prototypical case):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vUw9Thrml8x"
      },
      "source": [
        "| <font size=\"4\">Information retrieval</font> | <font size=\"4\">Information extraction</font> |\n",
        "| ----------- | ----------- |\n",
        "| <font size=\"3\">Ad-hoc information needs</font> | <font size=\"3\">Persistent information needs</font> |\n",
        "| <font size=\"3\">Response using \"live\" database query</font> | <font size=\"3\">Response using dedicated IE system</font> |\n",
        "| <font size=\"3\">Unstructured output (documents)</font> | <font size=\"3\">Structured output (e.g. relations)</font> |\n",
        "| <font size=\"3\">Results discarded after query</font> | <font size=\"3\">Results stored in database after extraction</font> |\n",
        "| <font size=\"3\">Example: <a href=\"https://google.com\">Google</a></font> | <font size=\"3\">Example: <a href=\"https://string-db.org/\">STRING DB</a></font> |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OA82zW2pjcc"
      },
      "source": [
        "---\n",
        "\n",
        "<center>(<b>NOTE: the following is a recap of material presented in the Introduction to Language Technology course.</b> If you've recently taken this course or are otherwise familiar with NER concepts, feel free to skip this and move to the material on NER using BERT.)</center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqwdDLkMi_rb"
      },
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "* Recognize mentions of *named entities* (people, places, companies, etc.) and their types (`PER[SON]`, `LOC[ATION]`, etc.) in text\n",
        "    * Commonly expanded to include also mentions of e.g. dates\n",
        "    * In specialized domains, recognize names of e.g. genes, proteins, and chemicals\n",
        "* Basic task in information extraction: need to know what things are talked about in order to extract e.g. relations between them\n",
        "* Reliable for well-studied text domains in resource-rich languages (e.g. English news), often well over 90% precision and recall\n",
        "    * Much worse for e.g. Finnish Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzmnqE6qi_rc"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/TurkuNLP/Text_Mining_Course/master/figs/english_ner_example.png\">\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/TurkuNLP/Text_Mining_Course/master/figs/finnish_ner_example.png\" width=\"90%\" align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1qC0Caoi_rc"
      },
      "source": [
        "### Approaches\n",
        "\n",
        "* **Dictionary lookup**: simple and fast to implement and apply, but no list is complete, no answer to ambiguity\n",
        "    * e.g. \"Nokia\" as `ORG` vs `LOC`, \"Bush\" as `PER` vs common noun \n",
        "* **Rule-based systems**: able to take context into account, but require manual tuning, can be brittle\n",
        "* **Machine learning**: state-of-the-art results, can be retrained, but supervised ML requires manual annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caiEupDZi_re"
      },
      "source": [
        "# Sequence labeling\n",
        "\n",
        "* Fundamental machine learning task: assign a class (label) to each item in a sequence\n",
        "    * Given observed input sequence ***x*** = { *x<sub>1</sub>*, *x<sub>2</sub>*, ... *x<sub>n</sub>* }, predict outputs ***y*** = { *y<sub>1</sub>*, *y<sub>2</sub>*, ... *y<sub>n</sub>* }\n",
        "* Examples in NLP: part-of-speech tagging, chunking, **named entity recognition**, speech recognition\n",
        "    * e.g. for part-of-speech tagging, the input ***x*** is a sequence of words and the output ***y*** a sequence of POS tags\n",
        "\n",
        "<hr>\n",
        "    \n",
        "```\n",
        "x:    The  quick  brown  fox   jumps  over  the  lazy  dog\n",
        "       ↓     ↓      ↓     ↓     ↓      ↓     ↓    ↓     ↓\n",
        "y:    DET   ADJ    ADJ   NOUN  VERB   ADP   DET  ADJ   NOUN\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "* Simply predicting ***x*** ↦ *y<sub>i</sub>* separately for each *y<sub>i</sub>* misses dependencies between outputs \n",
        "    * For example, in English, `ADJ` and `DET` are likely before `NOUN` but unlikely after"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSEnqBVwi_re"
      },
      "source": [
        "## NER as sequence labeling\n",
        "\n",
        "* Given a sequence of words, determine for each whether it is (a part of) a name\n",
        "* *In-Out-Begin* (IOB) encoding (or BIO):\n",
        "    * A token can <b>B</b>egin an entity mention, be <b>I</b>nside one, or be <b>O</b>utside of mentions\n",
        "    * Labels also encode entity type: **B-PER**, **I-PER**, **B-ORG**, **I-ORG**\n",
        "* NER as multiclass classification: assign each word to one class\n",
        "\n",
        "<hr>\n",
        "\n",
        "```\n",
        "x:    Adams  will  miss  England's   opening  World   Cup  qualifier\n",
        "        ↓     ↓     ↓       ↓           ↓       ↓      ↓      ↓\n",
        "y:    B-PER   O     O     B-LOC         O     B-MISC I-MISC   O\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "* Simple representation applicable with most machine learning methods\n",
        "* Limitation: discontinuous and overlapping (e.g. nested) spans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BgUp1Zgi_rf"
      },
      "source": [
        "# Learning sequence labeling\n",
        "\n",
        "* Supervised machine learning: requires annotated data for training\n",
        "    * We assume examples of inputs ***x*** with correct outputs ***y***\n",
        "    * Train ML method to predict outputs for new, unseen inputs → Need to *generalize*, not simply memorize (x,y) pairs from training data\n",
        "* Key question: how to represent the words to the ML method as *features*\n",
        "    * Examples: words (``Finland``) prefixes/suffixes (`Fin-`, `-and`), POS tags (`NOUN`), word shape (`STARTS-WITH-CAPITAL`), ...\n",
        "    * **Deep learning approach**: feature learning, often combining unsupervised pre-training with task-specific fine-tuning\n",
        "\n",
        "\n",
        "## Naive Bayes\n",
        "\n",
        "* Simple probabilistic classification method popularized by success in spam filtering\n",
        "* Predict probability each class <i>y</i> given input <i>x</i> using Bayes' rule $P(y|x) = \\frac{P(y)P(x|y)}{P(x)}$, pick most likely \n",
        "* Estimate probabilities based on (x,y) counts in training data (+smoothing), e.g.\n",
        "    * Prior: `P(B-PER) = count(*, B-PER) / total-examples`\n",
        "    * Conditional: `P(Adam|B-PER) = count(Adam, B-PER) / count(*, B-PER)`\n",
        "* Simple and fast, but limited: assumes independence of features (the \"naive\" part -- almost always false!) and independence of labels (no notion of sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cCgvcSTi_rj"
      },
      "source": [
        "## Hidden Markov Models (HMM)\n",
        "\n",
        "* \"Sequence version\" of Naive Bayes: in addition to output probabilities, model also *transition probabilities* $P(y_i|y_{i-1})$\n",
        "    * Markov assumption: probability of state depends only on previous state (not whole history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIwO-OSni_rj"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/TurkuNLP/Text_Mining_Course/master/figs/bayes_hmm.png\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtzBoWkSi_rk"
      },
      "source": [
        "<div style=\"text-align:center; color:gray; font-size:80%\">(Figure modified from <a href=\"https://arxiv.org/pdf/1011.4088v1.pdf\">Sutton and McCallum (2011)</a>)</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd_Iewqqi_rl"
      },
      "source": [
        "* Assume an underlying \"hidden\" sequence of labels, which generates the observed data\n",
        "\n",
        "<hr>\n",
        "\n",
        "```\n",
        "y:    DET →  ADJ  →  ADJ →  NOUN → VERB → ADP → DET → ADJ → NOUN\n",
        "       ↓      ↓       ↓      ↓      ↓      ↓     ↓     ↓     ↓\n",
        "x:    The   quick   brown   fox   jumps   over  the  lazy   dog\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "* Transition probabilities can also be straightforwardly estimated from data, e.g.\n",
        "    * `P(I-PER|B-PER) = count-seq(I-PER, B-PER) / count-seq(*, B-PER)`\n",
        "* Decoding: [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) - efficient polynomial algorithm to find the best hidden sequence of labels for the observed data\n",
        "* Incorporates sequence information, but otherwise retains the independence assumptions from Naive Bayes\n",
        "\n",
        "\n",
        "## Conditional Random Fields (CRF)\n",
        "\n",
        "(Specifically, linear-chain CRFs)\n",
        "\n",
        "* *Discriminative* sequence classifier: estimate conditional probability $P(\\textbf{y}|\\textbf{x})$ (this is what we're interested in)\n",
        "    * We know the observed data (and don't care about it's probability) and want to predict the output (i.e. discriminate between possible outputs)\n",
        "    * (By contrast, *generative* models such as NB and HMM estimate the joint distribution $P(\\textbf{x},\\textbf{y})$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKNgkZ2hi_rl"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/TurkuNLP/Text_Mining_Course/master/figs/sutton_mccallum_models.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm-KmzOzi_rl"
      },
      "source": [
        "<div style=\"text-align:center; color:gray; font-size:80%\">(Figure modified from <a href=\"https://arxiv.org/pdf/1011.4088v1.pdf\">Sutton and McCallum (2011)</a>)</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q50eCCGfi_rl"
      },
      "source": [
        "* Avoid the independence assumptions made by HMMs (HMM is a special case of CRF)\n",
        "* The go-to ML method for many sequence classification tasks since its introduction in 2001 ([paper](https://people.cs.umass.edu/~mccallum/papers/crf-icml01.ps))\n",
        "* Trained iteratively using gradient descent; can get stuck in a local optimum and can be slow\n",
        "* Decoded in much the same way as HMMs - efficient polynomial algorithm to find the best sequence of labels\n",
        "\n",
        "We won't go into any real details here, you can check out one of the many tutorials out there if you want to know more about the inner workings of CRFs and the way they're trained (e.g. [this one](http://www.cs.upc.edu/~aquattoni/AllMyPapers/crf_tutorial_talk.pdf)). For a comprehensive introduction, look <a href=\"https://arxiv.org/pdf/1011.4088v1.pdf\">here</a>.\n",
        "\n",
        "---\n",
        "\n",
        "### Implementations\n",
        "\n",
        "* [CRFsuite](http://www.chokkan.org/software/crfsuite/) is a good general CRF training software\n",
        "* [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io/en/latest/) CRFsuite Python wrapper\n",
        "* [NERSuite](http://nersuite.nlplab.org/) a driver script for *CRFsuite* with predefined features tuned for the NER task\n",
        "* [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) also has a NER annotator (remember we played with it on the first lectures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40QNQpdRu-Km"
      },
      "source": [
        "## Deep learning approaches\n",
        "\n",
        "* State-of-the-art in NER as in many other machine learning tasks\n",
        "* Recurrent neural networks (RNNs) with e.g. LSTM cells previously a popular choice\n",
        "* Recently increasing focus on Transformer-based models such as BERT\n",
        "* (Previous work not obsolete: e.g. CRF losses still popular with Transformers for NER)\n",
        "\n",
        "We'll be studying a BERT-based approach to NER in detail shortly.\n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/TurkuNLP/Text_Mining_Course/master/figs/bert-for-token-classification.png\"><img src=\"https://raw.githubusercontent.com/TurkuNLP/Text_Mining_Course/master/figs/bert-for-token-classification.png\" width=\"400px\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TlOIsrSwTCR"
      },
      "source": [
        "<div style=\"color:gray; font-size:80%\">(Figure adapted from <a href=\"http://jalammar.github.io/illustrated-bert/\">The Illustrated BERT, ELMo, and co.</a> CC BY-NC-SA)</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRHYlvCPi_rm"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Sequence tagging methods are typically evaluated using the following metrics:\n",
        "\n",
        "* precision: fraction of predicted items that were correct\n",
        "* recall: fraction of correct items that were predicted\n",
        "* F1-score: harmonic mean of precision and recall\n",
        "\n",
        "These metrics are normally calculated for _mentions_ rather than words: instead of looking at whether individual words are tagged correctly or incorrectly, the evaluation determines whether named entity mentions, potentially consisting of multiple words, are correctly recognized.\n",
        "\n",
        "This means that a correctly tagged multi-word mention such as `Time Warner Cable` counts as one correct name, not three correct tags. (No partial credit is given for e.g. `Time Warner`.) For datasets with many multi-word mentions, this is a considerably more demanding metric.\n",
        "\n",
        "(The `pip`-installable library [`seqeval`](https://pypi.org/project/seqeval/) is one Python implementation of standard NER metrics.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHxzhWCri_rm"
      },
      "source": [
        "#  Training data for NER\n",
        "\n",
        "Supervised machine learning methods require annotated data for training. Datasets available for various languages and domains, e.g.:\n",
        "\n",
        "* https://www.clips.uantwerpen.be/conll2002/ner/: Spanish, Dutch\n",
        "* https://www.clips.uantwerpen.be/conll2003/ner/: English, German\n",
        "* https://github.com/turkunlp/turku-ner-corpus: Finnish\n",
        "* http://biocreative.org: English (biomedical)\n",
        "* https://catalog.ldc.upenn.edu/LDC2013T19: English, Mandarin Chinese, Arabic, Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LMSlMc--m4F"
      },
      "source": [
        "**Next**: named entity recognition with BERT"
      ]
    }
  ]
}