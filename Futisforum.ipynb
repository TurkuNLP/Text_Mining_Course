{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Focused web crawl of a discussion forum\n",
    "\n",
    "## Open-domain crawling\n",
    "\n",
    "* Naive open-domain crawling: start from index page, follow each link and go to see if there is something useful\n",
    "* Then just seed it with few URLs\n",
    "* You can use ready made crawlers, e.g. [SpiderLing](http://corpus.tools/wiki/SpiderLing), [Heritrix](https://webarchive.jira.com/wiki/display/Heritrix/Heritrix)\n",
    "\n",
    "## Case study: Futisforum http://futisforum2.org/\n",
    "\n",
    "* When you target the crawling to a specific site, it makes sense to first eyeball the site\n",
    "* [robots.txt](http://futisforum2.org/robots.txt)\n",
    "* **IMPORTANT: crawl headers (state who you are) and crawl delay (do not choke a server)**\n",
    "* In Python [Requests-library](http://docs.python-requests.org/en/master/) is easy for downloading the pages, [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) is handy for processing html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sys.exit()\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as et\n",
    "import time\n",
    "import sys\n",
    "\n",
    "crawl_headers={'User-Agent':'Research Group from University of Turku, Educational purposes', 'From':'jmnybl@utu.fi'}\n",
    "\n",
    "\n",
    "## Download and processs urls from sitemap-boards.php\n",
    "print >> sys.stderr, \"Downloading http://futisforum2.org/sitemap-boards.php\"\n",
    "wp=requests.get(u\"http://futisforum2.org/sitemap-boards.php\", headers=crawl_headers) # this contains the page structure from robots.txt, wp is a response object\n",
    "time.sleep(20) # wait for 20sec (crawl delay)\n",
    "tree = et.fromstring(wp.text)\n",
    "\n",
    "boards=[] # boards are those main partitions of the forum (suomen maajoukkue, veikkausliiga...) # http://futisforum2.org/index.php?board=1.0\n",
    "\n",
    "for elem in tree.iter():\n",
    "    if elem.tag.endswith(u\"loc\"):\n",
    "        url=elem.text.strip().rsplit(u\".\",1)[0] # strip the .0 part from url\n",
    "        boards.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sys.exit()\n",
    "\n",
    "visited_topics=set() # here we collect all urls pointing to topics (message chains) we have visited\n",
    "# http://futisforum2.org/index.php?topic=187475.0\n",
    "\n",
    "counter=1\n",
    "board_regex=re.compile(u\"board=([0-9]+)\\.([0-9]+)\")\n",
    "topic_regex=re.compile(u\"topic=([0-9]+)\\.([0-9]+)\")\n",
    "\n",
    "## now go through all boards (e.g. http://futisforum2.org/index.php?board=1.0)\n",
    "for board in boards:\n",
    "    i=0 #(http://futisforum2.org/index.php?board=1.i)\n",
    "    max_board=None\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        if max_board!=None and i>max_board: # we are ready here\n",
    "            break\n",
    "            \n",
    "        board_url=u\".\".join(s for s in (board,unicode(i)))\n",
    "        print >> sys.stderr, \"Downloading\",board_url\n",
    "        wp=requests.get(board_url,headers=crawl_headers) # download the page\n",
    "        time.sleep(20) # wait for 20sec (crawl delay)\n",
    "\n",
    "        parsed_html=BeautifulSoup(wp.text, 'html.parser')\n",
    "\n",
    "        if max_board==None:\n",
    "            try:\n",
    "                max_board=get_max_page(board_regex,parsed_html) # this function returns the number of pages this board have so thst we can iterate over those\n",
    "            except:\n",
    "                print >> sys.stderr, \"Something went wrong:\",board # always keep a log file\n",
    "                break\n",
    "\n",
    "                \n",
    "        # now iterate through all topic chain links in this page\n",
    "        for link in parsed_html.find_all(u\"a\"): # thanks to beautiful soup its really easy\n",
    "\n",
    "            if u\"nofollow\" in link.get(u\"rel\",[]): # this is not suppposed to be used when crawling (user profiles, shortcuts to newest messages etc.)\n",
    "                continue\n",
    "            url=link.get(u\"href\")\n",
    "            if not url or u\"#\" in url: # take only clean urls ('#' ones points to a specific message etc.)\n",
    "                continue\n",
    "\n",
    "            if u\"topic=\" in url: # these are message chains\n",
    "                try:\n",
    "                    topic=url.rsplit(u\".\",1)[0]\n",
    "                    if topic in visited_topics: # make sure that we don't download same pages twice\n",
    "                        continue\n",
    "                    visited_topics.add(topic)\n",
    "                except:\n",
    "                    print \"Something went wrong:\",url # always keep a log file\n",
    "                    continue\n",
    "                    \n",
    "                j=0 #(http://futisforum2.org/index.php?topic=187475.j)\n",
    "                max_topic=None\n",
    "                while True:\n",
    "                    if max_topic!=None and j>max_topic: # we are ready here\n",
    "                        break\n",
    "                    topic_url=u\".\".join(s for s in (topic,unicode(j)))\n",
    "                    print >> sys.stderr, \"Downloading\",topic_url # always keep a log file\n",
    "                    tp=requests.get(topic_url,headers=crawl_headers) # download the page\n",
    "                    time.sleep(20) # wait for 20sec (crawl delay)\n",
    "\n",
    "                    \n",
    "                    if max_topic==None: # again we have to know how many pages there are so that we can download all\n",
    "                        try:\n",
    "                            max_topic=get_max_page(topic_regex,tp.text)\n",
    "                        except:\n",
    "                            print >> sys.stderr, \"Something went wrong:\",topic_url\n",
    "                            break\n",
    "                            \n",
    "\n",
    "                    # SAVE THE PAGE\n",
    "                    print >> sys.stderr, \"Saving\", topic_url\n",
    "                    \n",
    "                    f=codecs.open(u\"page_\"+str(counter)+\".html\",u\"wt\",u\"utf-8\")\n",
    "                    counter+=1\n",
    "                    for line in tp.text:\n",
    "                        f.write(line)\n",
    "                    f.close()        \n",
    "                    \n",
    "                    j+=25 # increase the topic counter\n",
    "                    \n",
    "        i+=30 # increase the board counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## When working with big data\n",
    "\n",
    "* Use try--except statements. It does not really matter if you miss a page or two.\n",
    "* Keep sufficient progress and error logs, so that you know how you are progressing, and that you can restart in case something unexcepted happens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
