{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6ad9c9",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "**Previously**:\n",
    "\n",
    "* Introduction to Information Extraction\n",
    "* Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) methods recognize mentions of target entities in text and (typically) assign each a type (e.g. `PERSON`, `LOCATION`)\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"90%\" src=\"https://raw.githubusercontent.com/TurkuNLP/turku-ner-corpus/master/docs/example.png\">\n",
    "\n",
    "---\n",
    "\n",
    "While NER is important as a starting point for structured information extraction, it doesn't identify the real-word entities referred to in text, i.e. make connections such as these:\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/entity-linking-marin.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Issues associating mentions in text to the things they refer to include:\n",
    "\n",
    "* **Ambiguity** of names: e.g. _George Bush_ can refer either to the 41st or 43rd US president (among others)\n",
    "    * common names like _Emma Korhonen_ have dozens of potential referents even in comparatively small Finland\n",
    "* **Variability** of mentions: e.g. _George Bush_, _George Walker Bush_, _Bush Jr._, _GWB_ and _Dubya_ referring to the same person\n",
    "    * Morphological variability: _Turku_, _Turun_, _Turkua_, _Turkuun_, _Turkuhan_, _Turkukaan_, _Turkukin_, _Turkuunkin_, _Turkuunkaan_, _Turkuakin_, ...\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/george-bush.png\"><img src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/george-bush.png\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Tasks related these challenges are variously termed _(named entity) normalization_, _grounding_, _entity linking_, and [_wikification_](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.cikm07.pdf). We can separate two closely related tasks, here termed for clarity\n",
    "\n",
    "* **Mention normalization**: restoring strings in text to a standardized surface form (e.g. _Turkuunkaan_ → _Turku_)\n",
    "* **Entity linking**: associating mentions in text with entries representing them in a knowledge base (e.g. Wikipedia/Wikidata)\n",
    "\n",
    "We'll introduce these in detail in the following.\n",
    "\n",
    "Overall, we'll be sketching a normalization approach applying the following steps for each entity mention in text:\n",
    "\n",
    "1. Normalize mention to a standardized (\"dictionary\") form\n",
    "2. Find candidate entities in a knowledge base whose names or aliases match that form\n",
    "3. Disambiguate between candidate entities based on e.g. mention context\n",
    "\n",
    "Note that in an NLP pipeline, these steps would follow NER, and we assume NE tagging as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f51231",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Mention normalization\n",
    "\n",
    "Restoring text strings to standardized surface forms\n",
    "\n",
    "Challenges include\n",
    "\n",
    "* Lemmatization: _Turkuunkaan_ → _Turku_\n",
    "* Truecasing: _TURKU_, _turku_ → _Turku_\n",
    "* Multi-word names: _Turun yliopistonkin_, _Turunkin yliopisto_ → _Turun yliopisto_ (not _Turku yliopisto_!) \n",
    "\n",
    "## Lemmatization\n",
    "\n",
    "Mapping inflected forms of words to their dictionary forms:\n",
    "\n",
    "* _voi olla niin_ → _voida olla niin_\n",
    "* _voi on pilalla_ → _voi olla pilalla_\n",
    "\n",
    "Lemmatization as a task has been covered previously; here we'll simply use the [Turku neural parser](http://turkunlp.org/Turku-neural-parser-pipeline/) for lemmatization as a remotely set up service. You can easily set up similar services by following the [installation instructions](http://turkunlp.org/Turku-neural-parser-pipeline/docker.html).\n",
    "\n",
    "If you're interested in the technical aspects of the lemmatization method, these are presented in detail by [Kanerva et al. 2020](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9341ECA9B562DAF55E2F3F966554A667/S1351324920000224a.pdf/div-class-title-universal-lemmatizer-a-sequence-to-sequence-model-for-lemmatizing-universal-dependencies-treebanks-div.pdf). [PDF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7ad663",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet requests conllu\n",
    "\n",
    "import requests\n",
    "import conllu\n",
    "\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    SERVER_URL = 'http://86.50.253.19:8002/parser/parse'\n",
    "    response = requests.post(SERVER_URL, data={ 'text': sentence })\n",
    "    return conllu.parse(response.text)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea67f1",
   "metadata": {},
   "source": [
    "(**NOTE**: if you're running this after the 2021 spring course, this service is likely no longer available at the above URL.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb53682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['voi', 'olla', 'niin'] → ['voida', 'olla', 'niin']\n",
      "['voi', 'on', 'pilalla'] → ['voi', 'olla', 'pilalla']\n"
     ]
    }
   ],
   "source": [
    "for sentence in ('voi olla niin', 'voi on pilalla'):\n",
    "    tokens = parse_sentence(sentence)\n",
    "    print(sentence.split(), '→', [t['lemma'] for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a441c0",
   "metadata": {},
   "source": [
    "We can use a lemmatizer to identify the dictionary forms of simple names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbab1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turun → Turku\n",
      "Turkua → Turku\n",
      "Turkuun → Turku\n",
      "Turusta → Turku\n",
      "TURKUUNKOHAN → turkuunko\n"
     ]
    }
   ],
   "source": [
    "for form in ('Turun', 'Turkua', 'Turkuun', 'Turusta', 'TURKUUNKOHAN'):\n",
    "    print(form, '→', parse_sentence(form)[0]['lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3437e",
   "metadata": {},
   "source": [
    "Note that statistical and machine learning-based lemmatizers may require context to work well, and may fail in particular for rare and nonstandard forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dc1c5",
   "metadata": {},
   "source": [
    "## Truecasing\n",
    "\n",
    "Restoring \"normal\" case to text with non-standard case (e.g. _ALL UPPERCASE_)\n",
    "\n",
    "* _GEORGE BUSH WENT TO WASHINGTON_ → _George Bush went to Washington_\n",
    "* _the bush that george planted_ → _The bush that George planted_\n",
    "\n",
    "Can be part of lemmatization or considered as a separate task: lemmatization methods do not necessarily perform well on input text with non-standard case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d43058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TÄSSÄ', 'ON', 'PELKÄSTÄÄN', 'ISOJA', 'KIRJAIMIA'] → ['tämä', 'olla', 'pelkästään', 'iso', 'kirjain']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'TÄSSÄ ON PELKÄSTÄÄN ISOJA KIRJAIMIA' \n",
    "tokens = parse_sentence(sentence)\n",
    "print(sentence.split(), '→', [t['lemma'] for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62179d7e",
   "metadata": {},
   "source": [
    "Given a sufficient amount of correctly cased text, can be performed highly reliably using a language modeling approach (See [Lita et al. 2018](https://www.aclweb.org/anthology/P03-1020.pdf) [PDF]).\n",
    "\n",
    "Truecasing is rarely considered as a separate step in recent lemmatization methods, but may be of value in particular when working with irregularly cased documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c443e06",
   "metadata": {},
   "source": [
    "## Multi-word names\n",
    "\n",
    "Multi-word names can inflect in unpredictable ways, and lemmatizing words separately is not always correct:\n",
    "\n",
    "* _Turun Energian_ → _Turku Energia_ ✅ \n",
    "* _Turun yliopiston_ → _Turku yliopisto_ ❌ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fac7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turun', 'Energian'] → ['Turku', 'energia']\n",
      "['Turun', 'yliopiston'] → ['Turku', 'yli#opisto']\n"
     ]
    }
   ],
   "source": [
    "for sentence in ('Turun Energian', 'Turun yliopiston'):\n",
    "    tokens = parse_sentence(sentence)\n",
    "    print(sentence.split(), '→', [t['lemma'] for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c211a",
   "metadata": {},
   "source": [
    "Note that lemmatizing _Turun_ → _Turku_ in _Turun yliopiston_ is not a mistake by the lemmatizer: this is the correct dictionary form. The issue is rather that the multi-word name does not use the dictionary form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500671f",
   "metadata": {},
   "source": [
    "Only lemmatizing the head word will likewise fail in some cases:\n",
    "\n",
    "* _Turun yliopiston_ → _Turun yliopisto_ ✅\n",
    "* _Turun yliopistonkin_ → _Turun yliopisto_ ✅\n",
    "* _Turunkin yliopiston_ → _Turunkin yliopisto_ ❌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdcfbec",
   "metadata": {},
   "source": [
    "There is no \"standard\" NLP task setting for this particular challenge (and some of the issues noted here are somewhat specific to Finnish). However, we can consider some options:\n",
    "\n",
    "* Knowledge-based approach: gather standard forms of names from resources such as Wikipedia or Wikidata (see below)\n",
    "* Statistical approach: identify most common forms of names in large corpora of automatically tagged text\n",
    "\n",
    "We'll below briefly sketch the latter approach using summary data from the [Finnish internet parsebank](https://turkunlp.org/finnish_nlp.html#finnish-internet-parsebank-) tagged using the [Turku NER tagger](https://turkunlp.org/fin-ner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8533f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘parsebank-freq-type-form.tsv’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://a3s.fi/TKO_8964_2021/parsebank-freq-type-form.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332844db",
   "metadata": {},
   "source": [
    "The above file contains a frequency-sorted list of tagged strings and the most common types assigned to each by the tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29cefee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244115\tCARDINAL\tyksi\n",
      "3024280\tCARDINAL\tkaksi\n",
      "2842207\tGPE\tSuomen\n",
      "2733604\tGPE\tSuomessa\n",
      "2414878\tCARDINAL\t2\n"
     ]
    }
   ],
   "source": [
    "with open('parsebank-freq-type-form.tsv') as f:\n",
    "    for i in range(5):\n",
    "        print(next(f).rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b183602",
   "metadata": {},
   "source": [
    "The idea is to find the most frequent form of the words other than the head word (here, heuristically, the last word) to determine the most likely form.\n",
    "\n",
    "We'll here use stemming to minimize the computational cost. A serious implementation would lemmatize the mentions in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc2b15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19642 Turun yliopiston → ['Turun']\n",
      "10805 Turun yliopisto → ['Turun']\n",
      "7597 Turun yliopistossa → ['Turun']\n",
      "4757 Turun yliopistosta → ['Turun']\n"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import stemmer\n",
    "\n",
    "\n",
    "mention = 'Turunkin yliopiston'\n",
    "\n",
    "finnish_stemmer = stemmer('finnish')\n",
    "\n",
    "stems = finnish_stemmer.stemWords(mention.split())\n",
    "\n",
    "with open('parsebank-freq-type-form.tsv') as f:\n",
    "    for line in f:\n",
    "        freq, type_, form = line.rstrip('\\n').split('\\t')\n",
    "        if finnish_stemmer.stemWords(form.split()) == stems:\n",
    "            print(f'{freq} {form} → {form.split()[:-1]}')\n",
    "        if int(freq) < 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2560f26",
   "metadata": {},
   "source": [
    "This concludes our brief look into mention (string) normalization. Let's next look at entity linking and how these techniques relate to that task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e53eb6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c084253",
   "metadata": {},
   "source": [
    "Associating mentions in text with identifiers that represent the real-world entities that they refer to\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/entity-linking-marin.png\">\n",
    "\n",
    "---\n",
    "\n",
    "In the following, we'll assume that the mentions have been normalized to standardized forms (covered above).\n",
    "\n",
    "By linking entities in text to representations of entities it is possible to create fully structured representations of statements, such no part of the representation requires human interepretion.\n",
    "\n",
    "We can illustrate the difference between unstructured (textual) and structured resources by comparing the [Wikipedia](https://en.wikipedia.org/wiki/Douglas_Adams) and [Wikidata](https://www.wikidata.org/wiki/Q42) entries for the same person:\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/douglas-adams.png\"><img src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/douglas-adams.png\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "The information that is stored in structured form is conveyed on the Wikipedia page in natural language, e.g.\n",
    "\n",
    "* _Douglas Noel Adams [...] was an English author_ ⇒ `instance_of(Douglas_Adams, human)`\n",
    "* _Douglas Noel Adams (11 March 1952 – 11 May 2001)_ ⇒ `date_of_birth(Douglas_Adams, 11 March 1952)`\n",
    "\n",
    "This mapping is an information extraction task, specifically relation extraction. However, we won't consider this task in detail yet, and will instead focus on the use of these resources for entity linking.\n",
    "\n",
    "Resources such as Wikipedia and Wikidata provide two key pieces of information for entity linking:\n",
    "\n",
    "1. Unique identifiers associating mentions in text with real-world entities\n",
    "2. The names and synonyms of those real-world entitites\n",
    "\n",
    "As en example, let's again have a look at the Wikidata page for Douglas Adams, <https://www.wikidata.org/wiki/Q42>:\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/douglas-adams-wikidata.png\">\n",
    "\n",
    "---\n",
    "\n",
    "We find among many other pieces of information an ID, here `Q42`, and various names and aliases, including in other languages; via Wikipedia links, we can infer that Douglas Adams can be referred to (among many others) as e.g.\n",
    "\n",
    "* Дуглас Адамс (Russian)\n",
    "* ダグラス・アダムズ (Japanese)\n",
    "* 더글러스 애덤스 (Korean)\n",
    "* Ντάγκλας Άνταμς (Greek)\n",
    "* דאגלס אדמס (Hebrew)\n",
    "\n",
    "(Note that while these transliterations could mostly be generated straightforwardly from _Douglas Adams_, the same doesn't apply to all aliases, such as _Douglas Noel Adams_.)\n",
    "\n",
    "With these pieces of information -- the ID and names -- we can characterize the stages of a common approach to entity linking:\n",
    "\n",
    "* **Candidate generation**: for each mention in text, identify which entities (IDs) it may refer to\n",
    "* **Entity mention disambiguation**: given a mention in text and a set of candidate entities, identify the mentioned entity (ID).\n",
    "\n",
    "While these two subproblems can be addressed jointly in a single system, we'll treat them separately here for simplicity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aacce3f",
   "metadata": {},
   "source": [
    "## Candidate generation for entity normalization\n",
    "\n",
    "Candidate generation primarily seeks to address the _variability_ of entity names, i.e. the many possible ways in which people, places, etc. can be referred do.\n",
    "\n",
    "We can formalize the candidate generation problem e.g. as follows:\n",
    "\n",
    "- Given a knowledge base $K$ containing representations of real-world entities, and\n",
    "- Given an entity mention $m$ occurring in a document $d$\n",
    "- Return a subset ${ k_1, k_2, \\ldots k_n } \\subset K$ of representations that includes the representation of the entity referred to by $m$\n",
    "\n",
    "Note that there is a trivial solution: always return the full knowledge base $K$. While this would never omit the referred entity if it is included in the knowledge base, this is not normally viable in practice as knowledge bases can be very large (e.g. Wikidata contains nearly 100 million entries) and disambiguation is costly. Candidate generation should thus strike a balance between recall and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "Practically, consider the following example, where the knowledge base $K$ is Wikidata, the document $d$ = \n",
    "```\n",
    "Former President George Bush on Sunday congratulated President-elect Joe Biden and Vice President-elect Kamala Harris on their election. The 43rd president of the United States said ...\n",
    "```\n",
    "\n",
    "and the typed mention $m$ = (`George Bush`, `PERSON`). That is, we need to determine which entries in Wikidata the person name _George Bush_ could refer to in this context.\n",
    "\n",
    "For simplicity, let's ignore the document context and try to find people named _George Bush_ in Wikidata. We'll consider two options: querying the knowledge base directly, and reading a database dump."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285f2df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Querying Wikidata\n",
    "\n",
    "With more than 100 million entries, Wikidata is the most extensive freely available broad-coverage resource with information on real-world entities. While the details of using and querying this resource fall out of our scope here (and _you do not nead to know_ any RDF/SPARQL syntax), it's good to know some of the basic ideas.\n",
    "\n",
    "Wikidata is represented in [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework), and (simplifying) we can think of its entries as triples of the format\n",
    "\n",
    "```\n",
    "ENTITY-1    RELATION    ENTITY-2\n",
    "```\n",
    "\n",
    "So e.g. the information that `Douglas Adams` is a `Person` could be written in the abstract as\n",
    "\n",
    "```\n",
    "Douglas Adams    instance_of    Person\n",
    "```\n",
    "\n",
    "However, instead of human-readable (and potentially ambiguous!) strings, Wikidata uses unique IDs. The ID for Douglas Adams, the English author, happens to be [`Q42`](https://www.wikidata.org/entity/Q42), the ID for the `instance_of` relation is [`P31`](https://www.wikidata.org/wiki/Property:P31), and the ID for humans (the species _Homo sapiens_) is [`Q5`](https://www.wikidata.org/entity/Q5). So, we would more specifically write\n",
    "\n",
    "```\n",
    "Q42    P31    Q5\n",
    "```\n",
    "\n",
    "to assert that Douglas Adams is a person (a member of the species _Homo sapiens_).\n",
    "\n",
    "Wikidata can be queried using the [SPARQL](https://en.wikipedia.org/wiki/SPARQL) query language, where simple queries can be though of in the abstract as taking forms such as\n",
    "\n",
    "```\n",
    "ENTITY-1     RELATION    ?\n",
    "```\n",
    "\n",
    "to ask what entities `ENTITY-1` is related via `RELATION`. In practice, we can query e.g.\n",
    "\n",
    "```\n",
    "SELECT ?country WHERE {\n",
    "    wd:Q42 wdt:P27 ?country.\n",
    "}\n",
    "```\n",
    "\n",
    "To ask what was the country of citizenship ([`P27`](https://www.wikidata.org/wiki/Property:P27)) of Douglas Adams ([`Q42`](https://www.wikidata.org/entity/Q42)) (<a href=\"https://query.wikidata.org/#SELECT %3Fcountry %0AWHERE { wd%3AQ42 wdt%3AP27 %3Fcountry. }\">Try this query!</a>). This gives as response _United Kingdom_ ([`Q145`](https://www.wikidata.org/wiki/Q145)).\n",
    "\n",
    "We can similarly query e.g. for the names of cities in Finland (<a href=\"https://query.wikidata.org/#SELECT%20%3Fcity%20%3Fname%20WHERE%20%7B%0A%20%20%3Fcity%20wdt%3AP31%20wd%3AQ515.%0A%20%20%3Fcity%20wdt%3AP17%20wd%3AQ33.%0A%20%20%3Fcity%20wdt%3AP1705%20%3Fname%20%20%0A%7D%0ALIMIT%20100\">Try this query!</a>):\n",
    "\n",
    "```\n",
    "SELECT ?city ?name WHERE {\n",
    "  ?city wdt:P31 wd:Q515.        # explanation: ?city instance_of City.\n",
    "  ?city wdt:P17 wd:Q33.         # explanation: ?city country Finland.\n",
    "  ?city wdt:P1705 ?name         # explanation: ?city native_label ?name\n",
    "}\n",
    "```\n",
    "\n",
    "(Queries such as these could be used to generate dictionaries for entity normalization.)\n",
    "\n",
    "Coming back to our motivating example above, we can query Wikidata for people who have the name (`rdfs:label`) or alias (`skos:altLabel`) \"George Bush\" as follows (<a href=\"https://query.wikidata.org/#SELECT%20DISTINCT%20%3Fperson%20%3Fdescription%0AWHERE%0A%7B%0A%20%20%3Fperson%20wdt%3AP31%20wd%3AQ5.%0A%20%20%7B%20%3Fperson%20rdfs%3Alabel%20%22George%20Bush%22%40en.%20%7D%20UNION%0A%20%20%7B%20%3Fperson%20skos%3AaltLabel%20%22George%20Bush%22%40en.%20%20%7D%0A%20%20%3Fperson%20schema%3Adescription%20%3Fdescription.%0A%20%20FILTER%28LANG%28%3Fdescription%29%20%3D%20%22en%22%29%0A%7D%0A\">Try this query!</a>):\n",
    "\n",
    "```\n",
    "SELECT DISTINCT ?p ?d\n",
    "WHERE\n",
    "{\n",
    "  ?p wdt:P31 wd:Q5.                            # explanation: ?p instance-of Human\n",
    "  { ?p rdfs:label \"George Bush\"@en. } UNION    # explanation: ?p has-name \"George Bush\" (in English) or\n",
    "  { ?p skos:altLabel \"George Bush\"@en.  }      # explanation: ?p has-alias \"George Bush\" (in English)\n",
    "  ?p schema:description ?d.                    # explanation: ?p has-description ?d\n",
    "  FILTER(LANG(?d) = \"en\")                      # explanation: the language of ?d is English\n",
    "}\n",
    "```\n",
    "\n",
    "Producing the following result:\n",
    "\n",
    "\t\n",
    "| person        | description\n",
    "|:--------------|:-----------\n",
    "| wd:Q5537484\t| racing driver\n",
    "| wd:Q100766406 | college basketball player (1950–1950) Toledo\n",
    "| wd:Q5537488\t| American biblical scholar and pastor\n",
    "| wd:Q28445429  | association football player (1883-1936)|\n",
    "| wd:Q207       | 43rd president of the United States\n",
    "| wd:Q23505     | 41st president of the United States (1924-2018)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b0d1",
   "metadata": {},
   "source": [
    "### Wikidata dumps\n",
    "\n",
    "In practical systems, we likely don't want to make a SPARQL query every time we want to look up candidate names. Further, RDF triples are (arguably) not the most approachable of representations. Fortunately, the entire Wikidata knowledgebase is [available for download](https://dumps.wikimedia.org/wikidatawiki/entities/), also in JSON format: https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2 .\n",
    "\n",
    "This data is over 60G packed, so we won't be demonstrating the use of the entire knowledge base here. Instead, let's first look at the JSON structure of the Douglas Adams entry: https://www.wikidata.org/wiki/Special:EntityData/Q42.json\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"35%\" src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/wikidata-q42.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Note here the keys `id`, `labels`, and `aliases`. These are all we need here: the unique ID, and the names and aliases associated with that ID. Let's look at this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cfadf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity Q42 has label 'Douglas Adams' and aliases ['Douglas Noel Adams', 'Douglas Noël Adams', 'Douglas N. Adams']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "response = requests.get('https://www.wikidata.org/wiki/Special:EntityData/Q42.json')\n",
    "data = response.json()\n",
    "entity = data['entities']['Q42']\n",
    "\n",
    "id_ = entity['id']\n",
    "label = entity['labels']['en']['value']\n",
    "aliases = [a['value'] for a in entity['aliases']['en']]\n",
    "\n",
    "print(f\"entity {id_} has label '{label}' and aliases {aliases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980b6ba",
   "metadata": {},
   "source": [
    "We've prepared the subset of Wikidata people (entities that have `instance_of human`) filtered just to these pieces of information in a JSON lines format ([conversion script](https://github.com/TurkuNLP/Text_Mining_Course/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5d74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘wikidata-people.jsonl’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://a3s.fi/TKO_8964_2021/wikidata-people.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07822a2e",
   "metadata": {},
   "source": [
    "We can use this data as above to (relative quickly) access information about all representations of people in Wikidata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62235c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q23 \t George Washington\n",
      "Q42 \t Douglas Adams\n",
      "Q1868 \t Paul Otlet\n",
      "Q207 \t George W. Bush\n",
      "Q297 \t Diego Velázquez\n",
      "Q368 \t Augusto Pinochet\n",
      "Q501 \t Charles Baudelaire\n",
      "Q619 \t Nicolaus Copernicus\n",
      "Q633 \t Neil Young\n",
      "Q640 \t Harald Krichel\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('wikidata-people.jsonl') as f:\n",
    "    for i in range(10):\n",
    "        entity = json.loads(f.readline())\n",
    "        print(entity['id'], '\\t', entity['labels']['en']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c8316",
   "metadata": {},
   "source": [
    "(There are also a wealth of off-the-shelf tools for working with Wikidata available at https://www.wikidata.org/wiki/Wikidata:Tools/For_programmers)\n",
    "\n",
    "### Approximate string matching\n",
    "\n",
    "The combination of mention normalization methods and access to names and aliases in a knowledgebase offers one possible solution to candidate generation:\n",
    "\n",
    "* Normalize the given mention $m$ (potentially using context $d$) to a standard (\"dictionary\") form $s$\n",
    "* Return all entries in the knowledge base that contain an name or alias matching $s$\n",
    "\n",
    "However, no knowledge base is ever absolutely complete, and even if one were, mentions can have typos or irregularities. If our knowledge base contains e.g. the name _George W. Bush_ but not the form _George W Bush_ (without the dot), we would still like match the latter.\n",
    "\n",
    "To solve minor deviations in string forms, we can apply _approximate_ (or _fuzzy_) _string matching_ methods. A core set of algorithms for this task involves the notion of _edit distance_, i.e. how many insertions, deletions or substitutions need to be made to edit one string into another.\n",
    "\n",
    "For example, the [Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings can be  calculated using tools available for `pip install`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb8c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance(\"George W. Bush\", \"George W Bush\") = 1\n",
      "distance(\"George W. Bush\", \"George Bush\") = 3\n",
      "distance(\"Levenshtein\", \"Lehvenstien\") = 4\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet python-Levenshtein\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "for s1, s2 in [('George W. Bush', 'George W Bush'),\n",
    "               ('George W. Bush', 'George Bush'),\n",
    "               ('Levenshtein', 'Lehvenstien')]:\n",
    "    print(f'distance(\"{s1}\", \"{s2}\") = {distance(s1, s2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eff06a",
   "metadata": {},
   "source": [
    "For large sets of strings, the computational cost of calculating pairwise edit distances for all strings can be too high. To allow approximate candidate generation from very large knowledge bases, we can use e.g. [Locality-sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) methods for strings, which aim to map similar strings to the same hash value, or dedicated methods such as [simstring](http://www.chokkan.org/software/simstring/).\n",
    "\n",
    "We'll here demonstrate approximate matching using a Python implementation of simstring, [`simstring-pure`](https://pypi.org/project/simstring-pure/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f2e198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet simstring-pure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b330d59",
   "metadata": {},
   "source": [
    "We'll first build a simstring database with character N-gram features (this takes a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7cc5e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3153159 entries to DB\n"
     ]
    }
   ],
   "source": [
    "from simstring.feature_extractor.character_ngram import CharacterNgramFeatureExtractor\n",
    "from simstring.database.dict import DictDatabase\n",
    "\n",
    "\n",
    "db = DictDatabase(CharacterNgramFeatureExtractor(2))\n",
    "\n",
    "\n",
    "with open('wikidata-people.jsonl') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        entity = json.loads(line)\n",
    "        try:\n",
    "            db.add(entity['labels']['en']['value'])\n",
    "            count += 1\n",
    "        except:\n",
    "            pass    # skip people without an English name\n",
    "\n",
    "print(f'Added {count} entries to DB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f2753",
   "metadata": {},
   "source": [
    "We can then query that database for approximate string matches using e.g. n-gram cosine similarity very quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643b8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George W. Bush → ['George Bush', 'George W. Buck', 'George W. Bush', 'George W. Brush']\n",
      "George W Bush → ['George Bush', 'George W. Bush', 'George W. Brush']\n"
     ]
    }
   ],
   "source": [
    "from simstring.measure.cosine import CosineMeasure\n",
    "from simstring.searcher import Searcher\n",
    "\n",
    "\n",
    "THRESHOLD = 0.8    # minimum similarity for retrieved strings\n",
    "\n",
    "searcher = Searcher(db, CosineMeasure())\n",
    "for mention in ['George W. Bush', 'George W Bush']:\n",
    "    print(mention, '→', searcher.search(mention, THRESHOLD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12766d",
   "metadata": {},
   "source": [
    "With mention normalization providing us with standardized forms of strings appearing in text and approximate matching finding minor variations, we're practically done with candidate generation (right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5922c",
   "metadata": {},
   "source": [
    "### Alternative forms of reference\n",
    "\n",
    "We have so far focused on cases where a mention can be approximately matched to a full known name alias. However, entities are not always referenced by their full names. Common exceptions include\n",
    "\n",
    "* Only first or last name after first mention in a document (e.g. _George Bush_ → _Bush_)\n",
    "    * In particular in infomal writing, first or last names can be used exclusively (e.g. _Obama_)\n",
    "* Short \"local\" abbreviations for repeatedly mentioned names (e.g. _Emma Korhonen_ → _EK_)\n",
    "* Reference by title or position (e.g. _The president_, _the Member for Cambridge_)\n",
    "\n",
    "Some of these issues are best resolved by [coreference resolution](https://nlp.stanford.edu/projects/coref.shtml), i.e. using the same or similar methods as for deciding the referent of _he_ or _she_. However, it should be noted that some forms of reference not typically found in knowledge bases can be identified using resources such as Wikipedia where any referring string of text can potentially be linked to an entry associated with the full name of the referenced entity:\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"75%\" src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/paaministeri-marin.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Linked texts such as these can be used to augment the aliases found in knowledge bases for candidate generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded23e4c",
   "metadata": {},
   "source": [
    "## Entity mention disambiguation\n",
    "\n",
    "Where candidate generation aimed to capture the _variability_ of names, their _ambiguity_ is addressed by disambiguation.\n",
    "\n",
    "The final step of our normalization approach is to disambiguate between the options provided by the candidate generation step:\n",
    "\n",
    "- Given an entity mention $m$ occurring in a document $d$ and\n",
    "- Given a subset ${ k_1, k_2, \\ldots k_n } \\subset K$ of entity representations from a knowledge base $K$\n",
    "- Select the representation most likely referenced by the mention $m$, or `NIL` if the reference is not included\n",
    "\n",
    "The option to select an \"empty\" value (`NIL`) is included to cover the common case where an entity referenced in a text is not found in a knowledge base. \n",
    "\n",
    "A wealth of methods have been proposed to this task, with various combinations or rule-based approaches, machine learning with explicitly engineered features, and recently feature learning-based methods (See e.g. [Shen et al. 2015](http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/TKDE14-entitylinking.pdf) for a survey of the field, and [Sevgili et al. 2021](https://arxiv.org/pdf/2006.00575.pdf)).\n",
    "\n",
    "We will here discuss string similarity, one effective heuristic (popularity), and ranking-based machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779bc1d0",
   "metadata": {},
   "source": [
    "### String similarity\n",
    "\n",
    "Assuming our candidate generation includes some form of approximate matching, the string we queried with may not exactly match any of the names and aliases in our knowledge base. Consider our example from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "860d2d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['George Bush', 'George W. Bush', 'George W. Brush']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.search('George W Bush', THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f81e9",
   "metadata": {},
   "source": [
    "It seems intuitively clear that it's more likely that our mention just lacks a dot (i.e. should match _George W. Bush_) than that it lacks a dot _and_ mispells _Brush_ as _Bush_.\n",
    "\n",
    "Here we can again look at edit distance to identify likely candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0e9dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance(\"George W Bush\", \"George Bush\") = 2\n",
      "distance(\"George W Bush\", \"George W. Bush\") = 1\n",
      "distance(\"George W Bush\", \"George W. Brush\") = 2\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "s1 = 'George W Bush'\n",
    "\n",
    "for s2 in searcher.search(mention, THRESHOLD):\n",
    "    print(f'distance(\"{s1}\", \"{s2}\") = {distance(s1, s2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012298c",
   "metadata": {},
   "source": [
    "We can also further refine this approach by assigning different costs to different edit operations: for example, removing or inserting a dot could have a lower cost than removing or inserting an alphabetic character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bd2ff",
   "metadata": {},
   "source": [
    "### Entity popularity\n",
    "\n",
    "Although the concept is hard to define objectively, various measures of entity \"popularity\" provide both a strong baseline method for disambiguation as well as a valuable feature for machine learning approaches. We could measure the popularity of an entity e.g. by\n",
    "\n",
    "* The number of words on the Wikipedia page for the entity\n",
    "* The number of facts (relations) recorded on the Wikidata entry of the entity\n",
    "* The number of incoming links to the Wikipedia/Wikidata pages (either internal, or in a web crawl)\n",
    "\n",
    "As an example, consider our previous query for <a href=\"https://query.wikidata.org/#SELECT%20DISTINCT%20%3Fperson%20%3Fdescription%0AWHERE%0A%7B%0A%20%20%3Fperson%20wdt%3AP31%20wd%3AQ5.%0A%20%20%7B%20%3Fperson%20rdfs%3Alabel%20%22George%20Bush%22%40en.%20%7D%20UNION%0A%20%20%7B%20%3Fperson%20skos%3AaltLabel%20%22George%20Bush%22%40en.%20%20%7D%0A%20%20%3Fperson%20schema%3Adescription%20%3Fdescription.%0A%20%20FILTER%28LANG%28%3Fdescription%29%20%3D%20%22en%22%29%0A%7D%0A\">people named George Bush</a> in Wikidata: the candidates include e.g. a [professional racing driver](https://en.wikipedia.org/wiki/George_Bush_(racing_driver)) of that name. While some references of the name do undoubtedly refer to this person, _in the absence of other evidence_ most people would likely assume that the name references a former US president.\n",
    "\n",
    "We can implement something along these intuitive lines by ranking the entries by the number of [Wikidata sitelinks](https://www.wikidata.org/wiki/Help:Sitelinks), i.e. the number of links to an entry from any Wiki resource (<a href=\"https://query.wikidata.org/#SELECT%20DISTINCT%20%3Fperson%20%3Fdescription%20%3Flinkcount%0AWHERE%0A%7B%0A%20%20%3Fperson%20wdt%3AP31%20wd%3AQ5.%0A%20%20%7B%20%3Fperson%20rdfs%3Alabel%20%22George%20Bush%22%40en.%20%7D%20UNION%0A%20%20%7B%20%3Fperson%20skos%3AaltLabel%20%22George%20Bush%22%40en.%20%20%7D%0A%20%20%3Fperson%20schema%3Adescription%20%3Fdescription.%0A%20%20%3Fperson%20wikibase%3Asitelinks%20%3Flinkcount%0A%20%20FILTER%28LANG%28%3Fdescription%29%20%3D%20%22en%22%29%0A%7D%0AORDER%20BY%20DESC%28%3Flinkcount%29\">try this query!</a>):\n",
    "\n",
    "```\n",
    "SELECT DISTINCT ?person ?description ?linkcount\n",
    "WHERE\n",
    "{\n",
    "  ?person wdt:P31 wd:Q5.\n",
    "  { ?person rdfs:label \"George Bush\"@en. } UNION\n",
    "  { ?person skos:altLabel \"George Bush\"@en.  }\n",
    "  ?person schema:description ?description.\n",
    "  ?person wikibase:sitelinks ?linkcount               # this line is new\n",
    "  FILTER(LANG(?description) = \"en\")\n",
    "}\n",
    "ORDER BY DESC(?linkcount)                             # this line is new\n",
    "```\n",
    "\n",
    "Running this updated query, we get the following result:\n",
    "\n",
    "| person        | description                                     | linkcount\n",
    "|:--------------|:------------------------------------------------|----\n",
    "| wd:Q207\t    | 43rd president of the United States\t          | 266\n",
    "| wd:Q23505     | 41st president of the United States (1924-2018) | 177\n",
    "| wd:Q5537488\t| American biblical scholar and pastor            | 6\n",
    "| wd:Q5537484\t| racing driver                                   | 2\n",
    "| wd:Q28445429\t| association football player (1883-1936)         | 2\n",
    "| wd:Q100766406\t| college basketball player (1950–1950) Toledo    | 0\n",
    "\n",
    "We see the expected result with the two presidents ranked higher than the scholar, racing driver, and the football and basketball players named _George Bush_.\n",
    "\n",
    "While imperfect and biased in various ways, popularity heuristics such as these can provide a strong baseline method: nevermind the context, just always return the most popular candidate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c12979",
   "metadata": {},
   "source": [
    "### Machine learning for entity linking\n",
    "\n",
    "In our discussion of entity linking, we have so far ignored the document context $d$ in which a mention $m$ occurs in. However, in some cases context is absolutely required to disambiguate: consider again our previous example with the typed mention typed = (`George Bush`, `PERSON`). If our document is $d_1$ = \n",
    "\n",
    "```\n",
    "Former President George Bush on Sunday congratulated President-elect [...] The 43rd\n",
    "president of the United States said ...\n",
    "```\n",
    "\n",
    "we can infer that the correct entity ID in Wikidata is [Q207](https://www.wikidata.org/wiki/Q207), _George W. Bush, 43rd president of the United States_. However, if our context were instead $d_2$ =\n",
    "\n",
    "```\n",
    "Former President George Bush on Sunday congratulated President-elect [...] The 41st\n",
    "president of the United States said ...\n",
    "```\n",
    "\n",
    "we should link to [Q23505](https://www.wikidata.org/wiki/Q23505), _George H. W. Bush, 41st president of the United States_.\n",
    "\n",
    "While heuristics can be written for specific cases (e.g. search for \"41\" or \"43\" to disambiguate the Bushes), it is very difficult to write general rules to answer the question _which of these entities does this mention refer to in this context_. As is common for cases where we cannot code a solution to a problem, we can apply machine learning to approximate a solution.\n",
    "\n",
    "Machine learning for entity linking is an active area or research with many proposed methods (see e.g. [Sevgili et al. 2021](https://arxiv.org/pdf/2006.00575.pdf)) and few off-the-shelf tools. However, many state-of-the-art approaches can be broadly characterized as follows:\n",
    "\n",
    "* Pre-train a _neural language model_ (LM) on a large corpus of unannotated data (e.g. BERT)\n",
    "* Train an _entity encoder_ to create representations of knowledge-base entries\n",
    "* Calculate the similarity of a mention-in-context vector encoded by the LM with the entity vector created by the entity encoder for each candidate entity\n",
    "* (Optionally add in prior information such as entity popularity)\n",
    "* Fine-tune with labelled data (mention-entity pairs)\n",
    "* Use similarity of mention-in-context and entity representations to rank candidates\n",
    "* Return top candidate if similarity higher than threshold, or `NIL` otherwise\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"75%\" src=\"https://github.com/TurkuNLP/Text_Mining_Course/raw/master/figs/entity-ranking-sevgili-et-al-2021.png\">\n",
    "<div style=\"text-align:center; color:gray; font-size:80%\">(Figure from <a href=\"https://arxiv.org/pdf/2006.00575.pdf\">Sevgili et al. 2021</a>)</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02edfc",
   "metadata": {},
   "source": [
    "To the best of my knowledge, no system of this type currently exists for Finnish, but perhaps one will have been created by the next time we give this course!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
