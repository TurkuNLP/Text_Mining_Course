{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " train-bert-for-ner-exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LPd4TaxwbMR"
      },
      "source": [
        "# Train BERT text for Named Entity Recognition (exercise)\n",
        "\n",
        "In this exercise, we'll create a Finnish named entity recognition model by fine-tuning BERT for the task.\n",
        "\n",
        "You should already be familiar with most of this code from the exercise on [sentiment classification with BERT](https://moodle.utu.fi/mod/url/view.php?id=1095675) and the course material on Named Entity Recognition. However, some settings are missing or far from optimal. The goals of the exercise are\n",
        "\n",
        "1. Find a Finnish BERT model in the Hugging Face [model repository](https://huggingface.co/models) and the Turku NER corpus in the [dataset repository](https://huggingface.co/datasets)\n",
        "2. Modify the code to use the Finnish BERT model and fine-tune on the Turku NER corpus\n",
        "3. Modify the hyperparameters to optimize the performance of your Finnish NER tagger. What performance (F-score) can you achieve, and what are the best settings you found?\n",
        "4. Try to find at least three example sentences (e.g. from Wikipedia or news) where the tagger makes a mistake. Can you explain the errors of the system?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3uu9vMewmM6"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1XGi09l-KxP"
      },
      "source": [
        "!pip --quiet install transformers\n",
        "!pip --quiet install datasets\n",
        "!pip --quiet install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfE2d5Ryisz"
      },
      "source": [
        "Imports and global settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q21P5qTsyyzO"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "\n",
        "MODEL_NAME = ''\n",
        "DATASET = ''\n",
        "MAX_LENGTH=128\n",
        "DUMMY_LABEL_ID = -100    # Don't change this!\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    'output_dir',\n",
        "    save_strategy='no',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB9k8bU_6ZFK"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUO0grs56oDF"
      },
      "source": [
        "dataset = load_dataset(DATASET)\n",
        "label_list = dataset[\"train\"].features['ner_tags'].feature.names\n",
        "num_labels = len(label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2XiKHwAJtoC"
      },
      "source": [
        "Downsample training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-zoQFxy8xq6"
      },
      "source": [
        "dataset['train'] = dataset['train'].filter(lambda example, idx: idx % 10 == 0, with_indices=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf_R3aYC38bM"
      },
      "source": [
        "Load tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjPCRpjl7ymV"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3KF07xP7zFl"
      },
      "source": [
        "Encode data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-9TlSVDBx0E"
      },
      "source": [
        "def encode_dataset(data):\n",
        "    tokenized = tokenizer(\n",
        "        data['tokens'],\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "    labels = []\n",
        "    prev_word_idx = None\n",
        "    for word_idx in tokenized.word_ids():\n",
        "        if word_idx is None or word_idx == prev_word_idx:\n",
        "            labels.append(DUMMY_LABEL_ID)\n",
        "        else:\n",
        "            labels.append(data['ner_tags'][word_idx])\n",
        "        prev_word_idx = word_idx\n",
        "    tokenized['labels'] = labels\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "encoded_dataset = dataset.map(encode_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCcf-EZF9x0j"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wd3_VaQ_94D"
      },
      "source": [
        "seq_eval_metrics = load_metric('seqeval')\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    true_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions.argmax(axis=2)\n",
        "    true_word_tags = [\n",
        "        [label_list[l] for (p, l) in zip(sent_pred, sent_true) if l != DUMMY_LABEL_ID]\n",
        "        for sent_pred, sent_true in zip(pred_ids, true_ids)\n",
        "    ]\n",
        "    pred_word_tags = [\n",
        "        [label_list[p] for (p, l) in zip(sent_pred, sent_true) if l != DUMMY_LABEL_ID]\n",
        "        for sent_pred, sent_true in zip(pred_ids, true_ids)\n",
        "    ]\n",
        "    results = seq_eval_metrics.compute(predictions=pred_word_tags, references=true_word_tags)\n",
        "    return {\n",
        "        'precision': results['overall_precision'],\n",
        "        'recall': results['overall_recall'],\n",
        "        'f1': results['overall_f1'],\n",
        "        'accuracy': results['overall_accuracy'],\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyRihTPA_SOU"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uwIGX6Y_fP8"
      },
      "source": [
        "data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer,\n",
        "    label_pad_token_id=DUMMY_LABEL_ID,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    train_args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPvLNK9EcN8"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f-XXjvkE0aL"
      },
      "source": [
        "results = trainer.evaluate()\n",
        "print(f'Accuracy : {results[\"eval_accuracy\"]:.1%}\\n'\n",
        "      f'Precision: {results[\"eval_precision\"]:.1%}\\n'\n",
        "      f'Recall   : {results[\"eval_recall\"]:.1%}\\n'\n",
        "      f'F1-score : {results[\"eval_f1\"]:.1%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGAlzsCKf7vc"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocPeVRnPWF_m"
      },
      "source": [
        "model = trainer.model\n",
        "model.eval()    # switch to evaluation mode\n",
        "model.to('cpu')    # switch to CPU\n",
        "\n",
        "\n",
        "def word_start_tokens(tokenized):\n",
        "    \"\"\"Return list of bool identifying which tokens start words.\"\"\"\n",
        "    prev_word_idx = None\n",
        "    is_word_start = []\n",
        "    for word_idx in tokenized.word_ids():\n",
        "        if word_idx is None or word_idx == prev_word_idx:\n",
        "            is_word_start.append(False)\n",
        "        else:\n",
        "            is_word_start.append(True)\n",
        "        prev_word_idx = word_idx\n",
        "    return is_word_start\n",
        "\n",
        "\n",
        "def predict_ner(words):\n",
        "    tokenized = tokenizer(words, is_split_into_words=True, return_tensors='pt')\n",
        "    pred = model(**tokenized)\n",
        "    pred_idx = pred.logits.detach().numpy().argmax(axis=2)\n",
        "    token_labels = [label_list[i] for s in pred_idx for i in s]\n",
        "    word_labels = []\n",
        "    for label, is_word_start in zip(token_labels, word_start_tokens(tokenized)):\n",
        "        if is_word_start:\n",
        "            word_labels.append(label)\n",
        "    return word_labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABRu7CAjVhbz"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a1TNTUyM2N8"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "\n",
        "example_sentences = [\n",
        "    'Turku on Suomen vanhin kaupunki',\n",
        "    'Larry Page ja Sergey Brin ovat Googlen perustajat',\n",
        "]\n",
        "\n",
        "# Mapping of CoNLL'03 types for displacy\n",
        "type_map = {\n",
        "    'PER': 'PERSON',\n",
        "}\n",
        "\n",
        "def render_with_displacy(words, tags):\n",
        "    tagged, offset, start, label = [], 0, None, None\n",
        "    for word, tag in zip(words, tags):\n",
        "        if tag[0] in 'OB' and start is not None:    # current ends\n",
        "            tagged.append({\n",
        "                'start': start,\n",
        "                'end': offset,\n",
        "                'label': type_map.get(label, label)\n",
        "            })\n",
        "            start, label = None, None\n",
        "        if tag[0] == 'B':\n",
        "            start, label = offset, tag[2:]\n",
        "        elif tag[0] == 'I':\n",
        "            if start is None:    # I without B, but nevermind\n",
        "                start, label = offset, tag[2:]\n",
        "        else:\n",
        "            assert tag == 'O', 'unexpected tag {}'.format(tag)\n",
        "        offset += len(word) + 1    # +1 for space\n",
        "    if start:    # span open at sentence end\n",
        "        tagged.append({\n",
        "                'start': start,\n",
        "                'end': offset,\n",
        "                'label': type_map.get(label, label)\n",
        "        })\n",
        "    doc = {\n",
        "        'text': ' '.join(words),\n",
        "        'ents': tagged\n",
        "    }\n",
        "    displacy.render(doc, style='ent', jupyter=True, manual=True)\n",
        "\n",
        "\n",
        "for e in example_sentences:\n",
        "    words = e.split()    # Note: assumes white-space tokenization is OK\n",
        "    ner_tags = predict_ner(words)\n",
        "    render_with_displacy(words, ner_tags)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}