{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation\n",
    "\n",
    "(Large topic, we'll only be scratching the surface here!)\n",
    "\n",
    "* Manually annotated data is a key resource for natural language processing\n",
    "    * Evaluation (regardless of approach)\n",
    "    * Training supervised machine learning methods\n",
    "* Wealth of annotated text corpora available, e.g.\n",
    "    * Finnish: [Kielipankki](https://www.kielipankki.fi/)\n",
    "    * English: [Linguistic Data Consortium](https://www.ldc.upenn.edu/), [CoNLL shared tasks](http://www.conll.org/previous-tasks)\n",
    "* To address a new task, language, (sub)domain, etc. it may be necessary to create new annotation\n",
    "\n",
    "An annotated corpus can have a very large impact: for example, the Penn Treebank was the main focus of research into automatic syntactic analysis research for more than a decade after its release in 1990."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/finnish_ner_example_large.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:80%\">Example: Finnish named entity annotation</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text selection\n",
    "\n",
    "* Corpus texts must be *representative* (domain, genre, register, style) and *balanced*\n",
    "    * Methods developed only on news text will perform poorly on tweets (and vice versa)\n",
    "* (To share corpora, its texts must be have a license that allows it!)\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/ptb_sources.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:80%\">Composition of Penn Treebank 1 (table from <a href=\"https://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&context=cis_reports\">Marcus et al. (1993)</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/tdt_sources.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:80%\">Composition of Turku Dependency Treebank (table from <a href=\"https://link.springer.com/article/10.1007%2Fs10579-013-9244-1\">Haverinen et al. (2013)</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation guidelines\n",
    "\n",
    "Detailed documentation of the annotation guidelines -- what is annotated and how -- is necessary to assure the quality and consistency of annotation.\n",
    "\n",
    "* Classes/categories/labels/types, for example\n",
    "    * Sentiment analysis: `negative`, `neutral`, `positive`\n",
    "    * Named entity recognition: `PERSON`, `LOCATION`, `ORGANIZATION`,\n",
    "    * Part-of-speech annotation: `NOUN`, `VERB`, `ADJ`, ...\n",
    "    * Dependency syntax: `subj`, `obj`, `nmod`, ...\n",
    "* Annotation formalism and representation (e.g. continuous spans of characters)\n",
    "* Scope of annotation, e.g. all words for POS/syntax, proper nouns for NER\n",
    "* Examples, edge cases and exceptions\n",
    "\n",
    "Guidelines should ideally be complete before annotation starts, but are frequently updated and extended thoughout annotation projects.\n",
    "\n",
    "Examples: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/ace_entity_example.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:80%\">Extract from ACE entity annotation guidelines (<a href=\"https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v6.6.pdf\">LDC 2008</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/ud_cop_example.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:80%\">Extract from <a href=\"http://universaldependencies.org/u/dep/cop.html\">Universal Dependencies annotation guidelines</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **English entity mention (named entity) annotation**: ACE (Automatic Content Extraction) entity guidelines ([LDC 2008](https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v6.6.pdf)): 72 pages\n",
    "* **English relation annotation**: ACE (Automatic Content Extraction) relation guidelines ([LDC 2008](https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-relations-guidelines-v6.2.pdf)): 33 pages\n",
    "* **English constituency syntax**: Penn Treebank II bracketing guidelines ([Bies et al. 1995](http://cs.jhu.edu/~jason/465/hw-parse/treebank-manual.pdf)): 318 pages\n",
    "* **Multilingual dependency syntax**: [Universal Dependencies](http://universaldependencies.org/) documentation: 15710 HTML pages (a **lot** of redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Annotation is never perfect, and it is important to know the quality and consistency of an annotated corpus. One typical strategy:\n",
    "\n",
    "* Two or more annotators are trained to perform the task\n",
    "* A part of the data is annotated independently by each annotator, without communicating with others\n",
    "* The redundant annotations are compared to identify differences\n",
    "\n",
    "In classification-type tasks, *inter-annotator agreement* is frequently measured using [Cohen's kappa statistic](https://en.wikipedia.org/wiki/Cohen%27s_kappa), which accounts for the possibility of chance agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Many text annotations tools exist, ranging from custom tools created for single annotation projects to commercial systems. One category of interest are free, browser-based tools, including (nb: incomplete listing):\n",
    "\n",
    "* Annotator.js: http://annotatorjs.org/\n",
    "* brat: http://brat.nlplab.org/\n",
    "* hypothes.is: https://web.hypothes.is/\n",
    "* TextAE: http://textae.pubannotation.org/\n",
    "* WebAnno: https://webanno.github.io/webanno/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This will only work with a tunnel to the VM)\n",
    "\n",
    "http://localhost:8001/index.xhtml#/ud-fi-ne/train/b603"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
