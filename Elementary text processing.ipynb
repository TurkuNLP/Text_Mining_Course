{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Off-the-shelf pipelines\n",
    "\n",
    "* There are a number of ready-made pipelines for processing raw text into something more useful for us\n",
    "* Use those unless you have special requirements and domain\n",
    "* In this course, we focus on mastering these ready-made tools rather than building new ones (Intro to NLP, and NLP courses are for that)\n",
    "\n",
    "## English: Stanford CoreNLP\n",
    "\n",
    "* Set of tools for processing basic well-formed English\n",
    "* The go-to solution for you unless you are in a very specific domain\n",
    "* http://nlp.stanford.edu:8080/corenlp/ (online demo)\n",
    "* http://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "## Finnish: Finnish dependency parser\n",
    "\n",
    "* A full Finnish parsing pipeline developed here in Turku\n",
    "* http://bionlp-www.utu.fi/parser_demo (online demo)\n",
    "* http://turkunlp.github.io/Finnish-dep-parser/\n",
    "\n",
    "## Others\n",
    "\n",
    "There's a bunch of other similar pipelines / frameworks:\n",
    "\n",
    "* Apache OpenNLP https://opennlp.apache.org/\n",
    "* GATE https://gate.ac.uk/\n",
    "* UIMA http://uima.apache.org/\n",
    "* NLTK http://www.nltk.org/\n",
    "* (...)\n",
    "* ...but let's focus on the two above for our work with English and Finnish\n",
    "\n",
    "# Elementary text analysis\n",
    "\n",
    "* Raw text in, analyzed text out\n",
    "* A typical text analysis pipeline will progress through a series of steps:\n",
    "\n",
    "1. Split text into sentences\n",
    "2. Split sentences into words (often steps 1 and 2 are reversed)\n",
    "3. Get the base forms for the words and assign them linguistic categories\n",
    "4. Analyze the syntactic structure of the sentence\n",
    "5. Relate text segments to each other across sentences, recognize named entities,...\n",
    "\n",
    "Note: These steps depend on each other in non-trivial ways: http://stanfordnlp.github.io/CoreNLP/dependencies.html\n",
    "\n",
    "## Sentence splitting\n",
    "\n",
    "* Not as easy as it sounds\n",
    "* Split a sentence after a dot and space and capital letter and call it a day?\n",
    "  * *F. Ginter is holding this course*\n",
    "* Gets even more exciting on Internet text - capitalization not necessarily consistent, no space after dot, etc.\n",
    "  * *hi.i am a dude on the internet.really i have not yet discovered the shift key!!!!!!!! LOL!!!!*\n",
    "* Existing tools typically expect plain text input - extraction of text from whatever source you have must have happened already\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "* Divide running text into *tokens*\n",
    "* Tokens: words, numbers, dates, punctuation\n",
    "* Lots of room for interpretation on what should constitute a token\n",
    "    * March 2, 2016  vs.  2.3.2016\n",
    "    * ettei -> että ei\n",
    "    * can't -> can not\n",
    "    * dog's -> dog 's\n",
    "    * NACA 2415 profile - is *NACA 2415* a single token?\n",
    "    * F.G. - two tokens or not?\n",
    "    * e.g. - two tokens or not?\n",
    "* In the end, you are restricted to what the tool you use produces\n",
    "* ...and that tool is restricted by the data it was built on\n",
    "\n",
    "## Lemmatization\n",
    "\n",
    "* Assign words to their base form\n",
    "  * Dogs -> dog\n",
    "  * sinullahan -> sinä\n",
    "  * voi -> voi or voida\n",
    "* Especially tough for inflective languages like Finnish\n",
    "* Sadly, also especially useful for inflective languages like Finnish\n",
    "  * Why?\n",
    "\n",
    "Let's try:\n",
    "\n",
    "```\n",
    "git clone https://github.com/TurkuNLP/Finnish-dep-parser.git\n",
    "cd Finnish-dep-parser\n",
    "./install.sh\n",
    "python omorfi_pos.py -i -o\n",
    "```\n",
    "\n",
    "Observations:\n",
    "\n",
    "* Lots of ambiguity\n",
    "  * Correct baseform must be decided based on the context\n",
    "* When a word is not known -> no luck\n",
    "* Heavy colloquial language -> no luck\n",
    "* Spelling errors -> no luck\n",
    "\n",
    "## POS tagging\n",
    "\n",
    "* Assign words to their linguistic categories (POS - part of speech): noun, verb, etc.\n",
    "* In practice: give each word a tag from a predefined set (of which there are many)\n",
    "* Not an easy task for most languages:\n",
    "  * Ambiguity wherever you look\n",
    "  * English: *Time flies like an arrow*\n",
    "  * Finnish: *Haetaan lakkaa satamasta, kun lakkaa satamasta*\n",
    "* Let's try:\n",
    "  * OpenNLP demo for English\n",
    "  * Finnish parser demo\n",
    "* English tags: [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "* Finnish tags: [here](http://universaldependencies.org/fi/pos/all.html)\n",
    "* Very useful if you want e.g. \n",
    "  * Only grab the content-bearing words in a text\n",
    "  * Distinguish between *lead* the metal and *to lead* the verb\n",
    "* An important step for downstream tasks like syntactic parsing (densifies data)\n",
    "  \n",
    "## Syntactic parsing\n",
    "\n",
    "* Many languages have a relatively free word order\n",
    "* Dealing with text as a linear sequence of words leads to sparsity problems\n",
    "  * So many ways to order the words...\n",
    "  * Related words can be far apart in the sentence:\n",
    "    * *Ford, as you may well know, is a car maker*\n",
    "* It is often more useful to know how the words in a sentence relate to each other\n",
    "* ...which makes it easier to mine relations from text (company - company mergers, and the like)\n",
    "* Every sentence can be given a *syntactic tree*\n",
    "\n",
    "English:\n",
    "<img src=\"figs/ptree_eng_1.png\"/>\n",
    "\n",
    "Finnish:\n",
    "<img src=\"figs/ptree_fin_1.png\"/>\n",
    "\n",
    "* The graphs above are trees\n",
    "  * Every word depends on one other word (*head*), and exactly one word in the sentence is the root\n",
    "  * The edges (called *dependencies*) have labels (called *dependency types*)\n",
    "  * Finnish types: http://universaldependencies.org/fi/dep/all.html\n",
    "  * English types: http://universaldependencies.org/en/dep/all.html\n",
    "* These trees tell us a lot about the structure but also the meaning of the sentence\n",
    "  * Oftentimes the same sentence can have a number of different meanings\n",
    "  * These correspond to different syntactic trees\n",
    "  * The *parser* must distinguish these\n",
    "* Note: a single edge connects *Ford* and *maker* in the tree -> perfect for us!\n",
    "\n",
    "## Named Entity recognition\n",
    "\n",
    "* Often, we are after *named entities* and their relations\n",
    "* Names of companies, places, people, dates, drug names,...\n",
    "* *Named Entity Recognition* is the task of identifying these\n",
    "* The inventory of possible entity types entirely depends on the NER tool you use and the data it was built on / built for\n",
    "* Typical off-the-shelf NER tools will recognize a small number of quite generic entity categories\n",
    "  * Organiation, Location, Person, etc.\n",
    "  * Specialized tools exist for different domains: a recognizer built for news text will not do good on medical research articles (and vice versa)\n",
    "* Let us try on English:\n",
    "  * CoreNLP demo\n",
    "\n",
    "## Coreference\n",
    "\n",
    "* Two text segments refering to the same object in the real world\n",
    "* Form a chain of backreferences\n",
    "\n",
    "<img src=\"figs/coref_eng_1.png\"/>\n",
    "\n",
    "* Unlike the syntactic trees, coreferences span across sentences\n",
    "* Allow us to assemble information about entities, even without these being explicitly mentioned\n",
    "\n",
    "# But how are all these tools built?\n",
    "\n",
    "Short answer: **machine learning** from manually prepared (annotated) *training data*\n",
    "\n",
    "* Build a syntactic parser -> you need thousands of example sentences for which the correct tree was given by a human\n",
    "* To build an NER recognizer -> you need thousands of examples of entities in context\n",
    "* Also sentence splitters and tokenizers are built via machine learning nowadays\n",
    "* ...\n",
    "* No free lunches here - someone needs to prepare the training data\n",
    "    * http://www.universaldependencies.org is a good collection of training datasets for 30+ languages\n",
    "\n",
    "But this all has practical implications for us:\n",
    "\n",
    "* The tools work best on data which is similar in type to the data on which they were trained\n",
    "  * A POS tagger trained on news will perform much worse on twitter\n",
    "  * From twitter: *\"What do we want?!!\" \"PSYCHIC POWERS! NOW!\" \"When do we want it?!!\"*\n",
    "* Often we don't really have choice - no special tools for processing Finnish Twitter, for example\n",
    "* Be aware that the tools are not perfect! Nowhere near so. (as we have seen)\n",
    "\n",
    "\n",
    "# How to run these tools in practice\n",
    "\n",
    "* CoreNLP is really simple to run\n",
    "  * Download and unpack, make sure you have Java v8\n",
    "  * Pick which annotation you want, pick the output format, feed in text\n",
    "  * http://stanfordnlp.github.io/CoreNLP/annotators.html\n",
    "  \n",
    "\n",
    "```\n",
    "./corenlp.sh -annotators tokenize,ssplit,pos,lemma,depparse,ner -outputFormat conll -file ../wiki.txt\n",
    "less wiki.txt.conll\n",
    "column -t wiki.txt.conll\n",
    "```\n",
    "\n",
    "* Running the Finnish parsing pipeline is not harder:\n",
    "  * Download and install using the instructions http://turkunlp.github.io/Finnish-dep-parser/\n",
    "\n",
    "```\n",
    "cat ../wiki-fin.txt | ./parser-wrapper.sh > wiki-fin.conllu\n",
    "less wiki-fin.conllu\n",
    "column -t wiki-fin.conllu\n",
    "```\n",
    "\n",
    "\n",
    "## CoNLL format\n",
    "\n",
    "* **The** format in which analyzed text is passed around through the various tools\n",
    "* Various versions of the format CoNLL-X, CoNLL-09, CoNLL-U (the last will pretty much replace the others in the near future) differ by which columns are present\n",
    "* Sentences delimited by empty lines\n",
    "* One line per token (word or punctuation)\n",
    "* TAB-delimited columns containing per-token analysis\n",
    "\n",
    "```\n",
    "1\tThe\tthe\tDT\tO\t3\tdet\n",
    "2\tPIK-15\tpik-15\tNN\tMISC\t3\tcompound\n",
    "3\tHinu\thinu\tNN\tMISC\t7\tnsubj\n",
    "4\twas\tbe\tVBD\tO\t7\tcop\n",
    "5\ta\ta\tDT\tO\t7\tdet\n",
    "6\tlight\tlight\tJJ\tO\t7\tamod\n",
    "7\taircraft\taircraft\tNN\tO\t0\tROOT\n",
    "8\tdeveloped\tdevelop\tVBN\tO\t7\tacl\n",
    "9\tin\tin\tIN\tO\t10\tcase\n",
    "10\tFinland\tFinland\tNNP\tLOCATION\t8\tnmod\n",
    "11\tin\tin\tIN\tO\t13\tcase\n",
    "12\tthe\tthe\tDT\tDATE\t13\tdet\n",
    "13\t1960s\t1960\tNNS\tDATE\t8\tnmod\n",
    "14\tfor\tfor\tIN\tO\t15\tcase\n",
    "15\tuse\tuse\tNN\tO\t13\tnmod\n",
    "16\tas\tas\tIN\tO\t19\tcase\n",
    "17\ta\ta\tDT\tO\t19\tdet\n",
    "18\tglider\tglider\tNN\tO\t19\tcompound\n",
    "19\ttug\ttug\tNN\tO\t8\tnmod\n",
    "20\t.\t.\t.\tO\t_\t_\n",
    "```\n",
    "\n",
    "```\n",
    "1       Kurt    Kurt    PROPN   _       Case=Nom|Number=Sing    2       name    _       _\n",
    "2       Hedström        Hedström        PROPN   _       Case=Nom|Number=Sing    6       nsubj   _       _\n",
    "3       ja      ja      CONJ    _       _       2       cc      _       _\n",
    "4       Tuomo   Tuomo   PROPN   _       Case=Nom|Number=Sing    5       name    _       _\n",
    "5       Tervo   Tervo   PROPN   _       Case=Nom|Number=Sing    2       conj    _       _\n",
    "6       aloittivat      aloittaa        VERB    _       Mood=Ind|Number=Plur|Person=3|Tense=Past|VerbForm=Fin|Voice=Act 0       root    _       _\n",
    "7       suunnittelun    suunnittelu     NOUN    _       Case=Gen|Number=Sing    6       dobj    _       _\n",
    "8       vuonna  vuosi   NOUN    _       Case=Ess|Number=Sing    6       nmod    _       _\n",
    "9       1959    1959    NUM     _       NumType=Card    8       nummod  _       _\n",
    "10      .       .       PUNCT   _       _       6       punct   _       _\n",
    "```\n",
    "\n",
    "### CoNLL-U format columns\n",
    "\n",
    "Copied from http://universaldependencies.org/format.html\n",
    "\n",
    "1. ID: Word index, integer starting at 1 for each new sentence; may be a range for tokens with multiple words.\n",
    "2. FORM: Word form or punctuation symbol.\n",
    "3. LEMMA: Lemma or stem of word form.\n",
    "4. UPOSTAG: [Universal part-of-speech tag](http://universaldependencies.org/u/pos/index.html)\n",
    "5. XPOSTAG: Language-specific part-of-speech tag; underscore if not available.\n",
    "6. FEATS: List of morphological features from the [universal feature inventory](http://universaldependencies.org/u/feat/index.html) or from a defined [language-specific extension](http://universaldependencies.org/ext-feat-index.html); underscore if not available.\n",
    "7. HEAD: Head of the current token, which is either a value of ID or zero (0).\n",
    "8. DEPREL: [Universal Stanford dependency relation](http://universaldependencies.org/u/dep/index.html) to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "9. DEPS: List of secondary dependencies (head-deprel pairs).\n",
    "10. MISC: Any other annotation.\n",
    "\n",
    "\n",
    "# Recap\n",
    "\n",
    "* Surprisingly easy to obtain pretty detailed analysis of input text\n",
    "* Looking good also for Finnish, not just English\n",
    "* Much of the analysis strives to normalize the variance present in natural language\n",
    "\n",
    "* Tools are trained from example data and are sensitive to the type of text being processed\n",
    "* There is a certain error rate in the output and that is something to live with\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
